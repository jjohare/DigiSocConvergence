 






- ## Chapter 7 Collaborative mixed reality
- ### 7.1 Toward an open metaverse
The Openstand principles are a great starting place for what an open metaverse might mean. [They are](https://open-stand.org/about-us/principles/):\
    Cooperation: Respectful cooperation between standards organizations,     whereby each respects the autonomy, integrity, processes, and     intellectual property rules of the others.     Adherence to Principles: Adherence to the five fundamental     principles of standards development:
    -   [[--] ]          Due process. Decisions are made with equity and fairness among         participants. No one party dominates or guides standards         development. Standards processes are transparent and         opportunities exist to appeal decisions. Processes for periodic         standards review and updating are well defined.     -   [[--] ]          Broad consensus. Processes allow for all views to be considered         and addressed, such that agreement can be found across a range         of interests.     -   [[--] ]          Transparency. Standards organizations provide advance public         notice of proposed standards development activities, the scope         of work to be undertaken, and conditions for participation.         Easily accessible records of decisions and the materials used in         reaching those decisions are provided. Public comment periods         are provided before final standards approval and adoption.     -   [[--] ]          Balance. Standards activities are not exclusively dominated by         any particular person, company or interest group.     -   [[--] ]          Openness. Standards processes are open to all interested and         informed parties.     Collective Empowerment: Commitment by affirming standards     organizations and their participants to collective empowerment by     striving for standards that:
    -   [[--] ]          are chosen and defined based on technical merit, as judged by         the contributed expertise of each participant;     -   [[--] ]          provide global interoperability, scalability, stability, and         resiliency;     -   [[--] ]          enable global competition;     -   [[--] ]          serve as building blocks for further innovation;     -   [[--] ]          contribute to the creation of global communities, benefiting         humanity.     Availability: Standards specifications are made accessible to all     for implementation and deployment. Affirming standards organizations     have defined procedures to develop specifications that can be     implemented under fair terms. Given market diversity, fair terms may     vary from royalty-free to fair, reasonable, and non-discriminatory     terms (FRAND).     Voluntary Adoption: Standards are voluntarily adopted and success is     determined by the market.
The push toward open standards is being joined (somewhat late) by credible and established bodies [like the IEEE](https://spectrum.ieee.org/laying-foundation-for-extended-reality). It's such a fast moving and under explored set of problems that this movement toward standards will take a long time to even find it's feet. Hopefully it's clear to the reader that this kind of development guides the work here. In the wider "real-time social VR" various companies have attempted to build closed ecosystems, for years. These now look more like attempts at digital society, but are closer to isolated metaverses, or more usefully isolated digital ecosystems. This is still happening. There's every chance that when Apple make their augmented reality play this year or next they will keep their system closed off as this tends to be their business model. Theo Priestly, CEO at Metanomics [points out](https://www.linkedin.com/feed/update/urn:li:activity:6977366421034967040/) that Chinese Giant Tencent are doing similar, and he cited Figure [7.1](https://arxiv.org/html/2207.09460v11/#Ch7.F1 "Figure 7.1 ‣ 7.1 Toward an open metaverse ‣ Chapter 7 Collaborative mixed reality ‣ Part I State of the art"); building a closed but tightly linked suite of businesses into something that looks like a metaverse. The levels of investment which are being hung under the metaverse moniker [are mind blowing](https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse), but that is not what we want to discuss as an end point for this book.
![Figure 7.1: [McCormick attempts to guess the Tencent metaverse](https://www.notboring.co/p/tencents-dreams)](../assets/tencent.png) 
For our purposes in this product design the interface between the previous chapter (NFTs) and this metaverse chapter is crucial. Punk6529 is a pseudonymous twitter account and thought leader in the "crypto" space. The text below encapsulates much of the reasoning that led to this book and product exploration, and is paraphrased [from this thread](https://twitter.com/punk6529/status/1536046831045685248) for our purposes.
[Bit by bit, the visualization layer of the internet will get better until it is unrecognisably better (+/- 10 years). As the visualization layer of the internet gets better, digital objects will become more useful and more important. Avatars (2D and 3D), art, schoolwork, work work, 3D virtual spaces and hundreds of other things. Not only will the objects themselves become more important, they will lead to different emergent behaviours. We see this already with avatars and mixed eponymous/pseudonymous/anonymous communities. Yes, it is the internet plumbing underneath, but just like social media changed human behaviour on the internet, metaverse type experiences will further change it. NFT Twitter + Discord + various virtual worlds is a form of early metaverse. I feel like I am entering a different world here, not just some websites. The most important question for the health of the internet/metaverse/human society in the 2030s will be decided now. And that question is: \"who stores the definitive ownership records of those digital objects\". There are two answers: a company's database OR a blockchain. If we end up with \"a company's database\" we will end up with all the web dysfunctions, but worse. SMTP is an open protocol that anyone can use so we don't have societal level fights on \"who is allowed to use email\". Short messaging online ended up becoming Twitter. So we end up having the most absurd, surreal discussions on the topic of \"who is allowed to use short-messaging\" being dependant on \"who is the CEO of Twitter\". There is no way this is the correct architecture for our progressively more digital economy.... If this is your first time around here, we are fighting for an open metaverse."] 
It seems that industry shares much of this opinion regarding an open metaverse. The proposal of a persistent interactive digital universe online is [so]  vast that major players recognise that they will not be able to monopolise this space, though Facebook/Meta are clearly attempting to. The [Metaverse Standards Forum](https://metaverse-standards.org/news/press-releases/leading-standards-organizations-and-companies-unite-to-drive-open-metaverse-interoperability/) is clearly an attempt by the other industry players to catch up and then get out ahead of Meta in this regard. It's also possible to view this as just another land grab, but through the vehicle of a standards body. Time will tell. They say:
["Announced today, The Metaverse Standards Forum brings together leading standards organizations and companies for industry-wide cooperation on interoperability standards needed to build the open metaverse. The Forum will explore where the lack of interoperability is holding back metaverse deployment and how the work of Standards Developing Organizations (SDOs) defining and evolving needed standards may be coordinated and accelerated. Open to any organization at no cost, the Forum will focus on pragmatic, action-based projects such as implementation prototyping, hackathons, plugfests, and open-source tooling to accelerate the testing and adoption of metaverse standards, while also developing consistent terminology and deployment guidelines."] 
This looks like it will be a useful project and community for the purposes outlined in this book, but the technology is young enough (in that it doesn't really exist) for multiple approaches to be trailed.
- #### 7.1.1 Primitives
OpenAI identified the following 5 points about metaverse, in response to the query "What are 5 key points I should know when studying metaverse?"
    Metaverse is a virtual reality platform that allows users to     interact with each other and with digital objects in a virtual     space.     Metaverse is a decentralized platform, meaning that there is no     central authority or server that controls the platform.     Metaverse is an open platform, meaning that anyone can develop     applications for the platform.     Metaverse is a secure platform, meaning that all data and     transactions are encrypted and secure.     Metaverse is a scalable platform, meaning that it can support a     large number of users and a large number of transactions.
This is an unexpectedly great answer, probably the cleanest we have found. The [Metaverse Standard Forum](https://metaverse-standards.org/) highlights the following, which reads like the output from a brainstorm between academia and industry stakeholders.
    collaborative spatial computing     augmented and virtual reality     photorealistic content authoring     new levels of scale and immersiveness.
It's not a useless list by any means, but it lacks the kind of product focus we need for detailed exploration of value and trust transfer.
Mystakidis identifies the following \[[155](https://arxiv.org/html/2207.09460v11/#bib.bibx155)\]:
 This is quite an academic list. A lot of these words will be explored in the next section which is more of an academic literature review.
Nevelsteen attempted to identify key elements for a 'virtual work' in 2018 and these are relevant now, and described rigorously in the appendix of his paper \[[156](https://arxiv.org/html/2207.09460v11/#bib.bibx156)\]:
    Shared Temporality, meaning that the distributed users of the     virtual world share the same frame of time.     Real time which he defines as "not turn based".     Shared Spatiality, which he says can include an 'allegory' of a     space, as in text adventures. It seems this might extend to a spoken     interface to a mixed reality metaverse.     ONE Shard is a description of the WLAN network architecture, and     conforms to servers in a connected open metaverse.     Many human agents simply means that more than one person can be     represented in the virtual world and corresponds to 'social' in our     description.     Many Software Agents corresponds to AI actors in our descriptions.     Non playing characters would be the gaming equivalent.     Virtual Interaction pertains to any ability of a user to interact     actively with the persistent virtual scene, and is pretty much a     given these days.     Nonpausable isn't even a word, but is pretty self explanatory.     Persistence means that if human participants leave then the data of     the virtual world continues. This applies to the scenes, the data     representing actions, and objects and actors in the worlds.     Avatar is interesting as it might seem that having avatar     representations of connected human participants is a given. In fact     the shared spaces employed by Nvidia for digital engineering do not.
Turning to industry; John Riccitiello, CEO of Unity Technologies says that metaverse is ["The next generation of the internet that is:] 
 Expanding this slightly we will us the following primitives of what we think are important for a metaverse:
    Fusing of digital and real life     Real time interactive 3d graphics first     Supports user generated content     \[[157](https://arxiv.org/html/2207.09460v11/#bib.bibx157)\]     Low friction economic actors and actions     Convergence of film and games     Blurring of IP boundaries     Blurring of narrative flow     Multimodal and hardware agnostic     Safeguarding, and governance
There is a [lot]  of work for the creative and technical industries to do to integrate human narrative creativity this nascent metaverse, and it's not even completely clear that this is possible, or even what people want.
- ### 7.2 History
The word metaverse was coined by the author Neal Stephenson in his 1992 novel Snowcrash. It started popping up soon after in [news articles](https://www.newscientist.com/article/mg14819994-000-how-to-build-a-metaverse/) and research papers \[[158](https://arxiv.org/html/2207.09460v11/#bib.bibx158)\], but in the last five years it has been finding a new life within a silicon valley narrative. Perhaps in response to this Stephenson is now working with a company called [Lamina1](https://www.lamina1.com/) which actually looks a lot like the rest of this book, so perhaps we have been on the right track.
There were clear precursors to modern social VR, such as [VRML in the 1990's](https://www.howtogeek.com/778554/remembering-vrml-the-metaverse-of-1995/) which laid much of the groundwork for 3D content over networked computers.
It might seem that there would be a clear path from there to now, in terms of a metaverse increasingly meaning connected social virtual spaces, but this has not happened. Instead interest in metaverse as a concept waned, MMORG (described later) filled in the utility, and then recently an entirely new definition emerged. Park and Kim surveyed dozens of different historical interpretations of the word, and the generational reboot they describe makes it even less clear \[[159](https://arxiv.org/html/2207.09460v11/#bib.bibx159)\]. The concept of the Metaverse is extremely plastic at this time (Figure [7.2](https://arxiv.org/html/2207.09460v11/#Ch7.F2 "Figure 7.2 ‣ 7.2 History ‣ Chapter 7 Collaborative mixed reality ‣ Part I State of the art")).
It's arguable that what will be expanding in this chapter is more appropriately 'Cyberspace' as described by William Gibson in Neuromancer \[[160](https://arxiv.org/html/2207.09460v11/#bib.bibx160)\] ["A global domain within the information environment consisting of the interdependent network of information systems infrastructures including the Internet, telecommunications networks, computer systems, and embedded processors and controllers."] 
Park and Kim identify the generational inflection point which has led to the resurgence of the concept of Metaverse \[[159](https://arxiv.org/html/2207.09460v11/#bib.bibx159)\]: ["Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offine selves are not different."] 
Brett Leonard, writer director of Lawnmower Man talks about the pressing need to get out in front of moral questions in the development of metaverse applications. He stressed that wellbeing will be a crucial underpinning of the technology because of the inherent intimacy of immersion in virtual spaces. He suggests that emotional engagement with storied characters is needed to satisfy the human need for narrative, and that this should be utopian by design to stave off the worst of dystopian emergent characteristics of the technology.
The book will aim to build toward an understanding of metaverse as a useful social mixed reality, that allows low friction communication and economic activity, within groups, at a global scale. Cryptography and distributed software can assist us with globally 'true' persistence of digital data, so we will look to integrate this with our social XR. This focus on persistence, value, and trust means it's most appropriate to focus on business uses as there is more opportunity for value creation which will be important to bootstrap this technology.
Elsewhere in the book we state that metaverse is the worst of the tele-collaboration tool-kits, and in general we 'believe' this to be true at this time. With that said Hennig-Thurau says the following in a [LinkedIn post](https://www.linkedin.com/feed/update/urn:li:activity:7020679507141361664/): [Our research finds that the performance of social interactions in the VR metaverse varies for different outcomes and settings, with productivity and creativity being on par with Zoom (not higher, but also not lower) for the two experimental settings in which we studied these constructs. Thus, as of today, meeting in VR does not overcome all the limitations that we are facing when using Zoom or Teams. But most importantly (to us), we find clear evidence that when people get together in the metaverse via VR, it creates SUBSTANTIALLY higher levels of social presence among group members across ALL FIVE STUDY CONTEXTS, from idea generation to joint movie going. This is the main insight from our study and the stuff we believe future uses of social virtual reality can (and should) build on. We also explain that the effectiveness of VR meetings can be further increased, and also how this can be done (by selecting the most appropriate settings, people, avatars, hardware, environments etc.).]  \[[161](https://arxiv.org/html/2207.09460v11/#bib.bibx161)\]
We agree that with sufficiently informed guiding constraints in place, and smaller group sizes (ie, not a large scale social metaverse), that there is a path forward.
This chapter will first attempt to frame the context for telepresence (the academic term for communicating through technology), and then explain the increasingly polarised options for metaverse. It's useful to precisely identify the primitives of the product we would like to see here, so this chapter is far more a review of academic literature in the field, culminating in a proposed framework.
![Figure 7.2: Elon Musk agrees with this on Twitter. It's notable that Musk is now Twitters' [biggest shareholder](https://twitter.com/paraga/status/1511320953598357505), and has been vocal about web censorship on the platform.](../assets/muskWeb3.png) 
- ### 7.3 Video conferencing, the status quo
This section has been adapted and updated for open source release, from the authors PhD thesis, with the permission of the University of Salford.
Video-conferencing has become more popular as technology improves, as it gets better integrated with ubiquitous cloud business support suites, and as a function of the global pandemic and changing work patterns. There is obviously increasing demands for real-time communication across greater distances.
The full effects of video-conferencing on human communication are still being explored, as seen in the experimental ["Together Mode"](https://news.microsoft.com/innovation-stories/microsoft-teams-together-mode/) within Microsoft Teams. Video-conferencing is presumed to be a somewhat richer form of communication than email and telephone, but not quite as informative as face-to-face communication.
In this section we look at the influence of eye contact on communication and how video-conferencing mediates both verbal and non-verbal interactions. Facilitation of eye contact is a challenge that must be addressed so that video-conferencing can approach the rich interactions of face-to-face communication. This is an even bigger problem in the emerging metaverse systems, so it's important that we examine the history and trajectory.
There is a tension emerging for companies who do not necessarily need to employ remote meeting technology, but also cannot afford to ignore the competitive advantages that such systems bring. In an experiment preformed well before the 2020 global pandemic at CTrip, Bloom et al describe how home working led to a 13% performance increase, of which about 9% was from working more minutes per shift (fewer breaks and sick-days) and 4% from more calls per minute (attributed to a quieter working environment) \[[162](https://arxiv.org/html/2207.09460v11/#bib.bibx162)\]. Home workers also reported improved work satisfaction and experienced less turnover, but their promotion rate conditional on performance fell. This speaks to a lack of management capability with such systemic change. It's clearly a complex and still barely understood change within business and management.
Due to the success of the experiment, CTrip rolled-out the option to work from home to the whole company, and allowed the experimental employees to re-select between the home or office. Interestingly, over half of them switched, which led to the gains almost doubling to 22%. This highlights the benefits of learning and selection effects when adopting modern management practices like working from home. Increasingly this is becoming a choice issue for prospective employees, and an advantage for hiring managers to be able to offer it.
More recent research by Barrero, Bloom and Davies found that working from home is likely to be "sticky" \[[163](https://arxiv.org/html/2207.09460v11/#bib.bibx163)\]. They found:
    better-than-expected WFH experiences,     new investments in physical and human capital that enable WFH,     greatly diminished stigma associated with WFH,     lingering concerns about crowds and contagion risks,     a pandemic-driven surge in technological innovations that support     WFH.
More recently Enterprise Collaboration Systems (ECS) provide rich document management, sharing, and collaboration functionality across an organisation. The enterprise ECS system may integrate collaborative video \[[164](https://arxiv.org/html/2207.09460v11/#bib.bibx164)\]. This is for instance the case with Microsoft Teams / Sharepoint. This integration of ECS should be considered when thinking about social VR systems which wish to support business, value, and trust. It is very much the case that large technology providers are attempting to integrate their 'business back end' systems into their emerging metaverse systems. Open source equivalents are currently lacking.
- #### 7.3.1 Pandemic drives adoption
The ongoing global COVID-19 pandemic is [changing how people work](https://blog.yelp.com/news/the-future-of-work-is-remote/), toward a new global 'normal'. Some ways of working are overdue transformation, and will be naturally disrupted. In the UK at least it seems that there may be real appetite to shift away from old practises. This upheaval will inevitably present both challenges and opportunities.
Highly technical workforces, especially, can [operate from anywhere](https://globalworkplaceanalytics.com/telecommuting-statistics). The post pandemic world seems to have stronger national border controls, with a resultant shortage of highly technical staff. This has forced the hand of global business toward [internationally distributed teams](https://www.lifeatspotify.com/being-here/work-from-anywhere).
If only a small percentage of companies allow the option of remote working, then they gain a structural advantage, enjoying benefits of reduced travel, lower workplace infection risk across all disease, and global agility for the personnel. Building and estate costs will certainly be reduced. More diversity may be possible. Issues such as sexual harassment and bullying may be reduced. With reduced overheads product quality may increase. If customers are happier with their services, then over time this 'push' may mean an enormous shift away from centralised working practises toward distributed working.
Technologies which support this working style were still in their infancy at the beginning of the pandemic. The rush to 'Zoom', a previously relatively unknown and insecure \[[165](https://arxiv.org/html/2207.09460v11/#bib.bibx165)\] web meeting product, shows how naive businesses were in this space.
Connection of multiple users is now far better supported, with Zoom and [Mircosoft Teams](https://www.microsoft.com/en-us/Investor/earnings/FY-2021-Q1/press-release-webcast) alone supporting hundreds of millions of chats a day. This is a 20x increase on market leader Skype's 2013 figure of [280 million](https://www.microsoft.com/en-us/Investor/earnings/FY-2013-Q1/press-release-webcast) connections per month. Such technologies extend traditional telephony to provide important multi sensory cues. However, these technologies demonstrate shortfalls compared to a live face-to-face meeting, which is generally agreed to be optimal for human-human interaction \[[166](https://arxiv.org/html/2207.09460v11/#bib.bibx166)\].
While the research community and business are learning how to adapt working practises to web based telepresence \[[167](https://arxiv.org/html/2207.09460v11/#bib.bibx167)\], there remains little technology support for ad-hoc serendipitous meetings between small groups. It's possible that Metaverse applications can help to fill this gap, by gamification of social spaces, but the under discussed problems with video conferencing are likely to be even worse in such systems.
Chris Herd of "FirstBase" (who admittedly have a bias) provides some fascinating speculations:
["I've spoken to 2,000+ companies with 40M+ employees about remote work in the last 12 months A few predictions of what will happen before 2030:] 
    [Rural Living: World-class people will move to smaller cities, have     a lower cost of living & higher quality of     life.]      [These regions must innovate quickly to attract that wealth. Better     schools, faster internet connections are a     must.]      [Async Work: Offices are instantaneous gratification distraction     factories where synchronous work makes it impossible to get stuff     done.]      [Tools that enable asynchronous work are the most important thing     globally remote teams need. A lot of startups will try to tackle     this.]      [Hobbie Renaissance: Remote working will lead to a rise in people     participating in hobbies and activities which link them to people in     their local community.]      [This will lead to deeper, more meaningful relationships which     overcome societal issues of loneliness and     isolation.]      [Diversity & Inclusion: The most diverse and inclusive teams in     history will emerge rapidly Companies who embrace it have a     first-mover advantage to attract great talent globally. Companies     who don't will lose their best people to their biggest     competitors.]      [Output Focus: Time will be replaced as the main KPI for judging     performance by productivity and output.]      [Great workers will be the ones who deliver what they promise     consistently]      [Advancement decisions will be decided by capability rather than who     you drink beer with after work.]      [Private Equity: The hottest trend of the next decade for private     equity will see them purchase companies, make them remote-first The     cost saving in real-estate at scale will be eye-watering. The     productivity gains will be the final nail in the coffin for the     office Working Too Much: Companies worry that the workers won't work     enough when operating remotely.]      [The opposite will be true and become a big     problem.]      [Remote workers burning out because they work too much will have to     be addressed.]      [Remote Retreats: Purpose-built destinations that allow for entire     companies to fly into a campus for a synchronous     week.]      [Likely staffed with facilitators and educators who train staff on     how to maximize effectiveness.]      [Life-Work Balance: The rise of remote will lead to people     re-prioritizing what is important to them.]      [Organizing your work around your life will be the first noticeable     switch. People realizing they are more than their job will lead to     deeper purpose in other areas.]      [Bullshit Tasks: The need to pad out your 8 hour day will evaporate,     replaced by clear tasks and     responsibilities.]      [Workers will do what needs to be done rather than wasting their     trying to look busy with the rest of the     office] 
["] 
- #### 7.3.2 Point to Point Video Conferencing
O'Malley et al. showed that face-to-face and video mediated employed visual cues for mutual understanding, and that addition of video to the audio channel aided confidence and mutual understanding. However, video mediated did not provide the clear cues of being co-located \[[168](https://arxiv.org/html/2207.09460v11/#bib.bibx168)\].
Dourish et al. make a case for not using face-to-face as a baseline for comparison, but rather that analysis of the efficacy of remote tele-collaboration tools should be made in a wider context of connected multimedia tools and 'emergent communicative practises' \[[169](https://arxiv.org/html/2207.09460v11/#bib.bibx169)\]. While this is an interesting viewpoint it does not necessarily map well to a recreation of the ad-hoc meeting.
There is established literature on human sensitivity to eye contact in both 2D and 3D VC \[[170](https://arxiv.org/html/2207.09460v11/#bib.bibx170), [171](https://arxiv.org/html/2207.09460v11/#bib.bibx171)\], with an accepted minimum of 5-10 degrees before observers can reliably sense they are not being looked at \[[172](https://arxiv.org/html/2207.09460v11/#bib.bibx172)\]. Roberts et al. suggested that at the limit of social gaze distance ( 4m) the maximum angular separation between people standing shoulder to shoulder in the real world would be around 4 degrees\[[173](https://arxiv.org/html/2207.09460v11/#bib.bibx173)\].
Sellen found limited impact on turn passing when adding a visual channel to audio between two people when using Hydra, an early system which provided multiple video conference displays in an intuitive spatial distribution\[[174](https://arxiv.org/html/2207.09460v11/#bib.bibx174)\]. She did however, find that the design of the video system affected the ability to hold multi-party conversations \[[175](https://arxiv.org/html/2207.09460v11/#bib.bibx175)\].
Monk and Gale describe in detail experiments which they used for examining gaze awareness in communication which is mediated and unmediated by technology. They found that gaze awareness increased message understanding \[[176](https://arxiv.org/html/2207.09460v11/#bib.bibx176)\].
Both Kuster et al. and Gemmel et al. have successfuly demonstrated software systems which can adjust eye gaze to correct for off axis capture in real time video systems\[[177](https://arxiv.org/html/2207.09460v11/#bib.bibx177), [178](https://arxiv.org/html/2207.09460v11/#bib.bibx178)\].
Shahid et al. conducted a study on pairs of children playing games with and without video mediation and concluded that the availability of mutual gaze affordance enriched social presence and fun, while its absence dramatically affects the quality of the interaction. They used the 'Networked Minds', a social presence questionnaire.
- #### 7.3.3 Triadic and Small Group
Early enthusiasm in the 1970's for video conferencing, as a medium for small group interaction quickly turned to disillusionment. It was agreed after a flurry of initial research that the systems at the time offered no particular advantage over audio only communication, and at considerable cost \[[179](https://arxiv.org/html/2207.09460v11/#bib.bibx179)\].
Something in the breakdown of normal visual cues seems to impact the ability of the technology to support flowing group interaction. Nonetheless, some non-verbal communication is supported in VC with limited success.
Additional screens and cameras can partially overcome the limitation of no multi-party support (that of addressing a room full of people on a single screen) by making available more bidirectional channels. For instance, every remote user can be a head on a screen with a corresponding camera. The positioning of the screens must then necessarily match the physical organization of the remote room.
Egido provides an early review of the failure of VC for group activity, with the "misrepresentation of the technology as a substitute for face-to-face\" still being valid today \[[180](https://arxiv.org/html/2207.09460v11/#bib.bibx180)\].
Commercial systems such as Cisco Telepresence Rooms cluster their cameras above the centre screen of three for meetings using their telecollaboration product, while admitting that this only works well for the central seat of the three screens. They also group multiple people on a single screen in what Workhoven et al. dub a "non-isotropic\" configuration \[[181](https://arxiv.org/html/2207.09460v11/#bib.bibx181)\]. They maintain that this is a suitable trade off as the focus of the meeting is more generally toward the important contributor in the central seat. This does not necessarily follow for less formal meeting paradigms.
In small groups, it is more difficult to align non-verbal cues between all parties, and at the same time, it is more important because the hand-offs between parties are more numerous and important in groups. A breakdown in conversational flow in such circumstances is harder to solve. A perception of the next person to talk must be resolved for all parties and agreed upon to some extent.
However, most of the conventional single camera, and expensive multi camera VC systems, suffer a fundamental limitation in that the offset between the camera sight lines and the lines of actual sight introduce incongruities that the brain must compensate for \[[166](https://arxiv.org/html/2207.09460v11/#bib.bibx166)\].
- #### 7.3.4 Other Systems to Support Business
There have been many attempts to support group working and rich data sharing between dispersed groups in a business setting. So called 'smart spaces' allow interaction with different displays for different activities and add in some ability to communicate with remote or even mobile collaborators on shared documents \[[182](https://arxiv.org/html/2207.09460v11/#bib.bibx182)\], with additional challenges for multi-disciplinary groups who are perhaps less familiar with one or more of the technology barriers involved \[[183](https://arxiv.org/html/2207.09460v11/#bib.bibx183)\].
Early systems like clearboard \[[184](https://arxiv.org/html/2207.09460v11/#bib.bibx184)\] demonstrated the potential for smart whiteboards with a webcam component for peer-to-peer collaborative working. Indeed it is possible to support this modality with Skype and a smartboard system (and up to deployments such as Accessgrid). They remain relatively unpopular however.
- #### 7.3.5 Mona Lisa Type Effects
Almost all traditional group video meeting tools suffer from the so-called Mona Lisa effect which describes the phenomenon where the apparent gaze of a portrait or 2 dimensional image always appears to look at the observer regardless of the observer's position \[[185](https://arxiv.org/html/2207.09460v11/#bib.bibx185), [186](https://arxiv.org/html/2207.09460v11/#bib.bibx186), [187](https://arxiv.org/html/2207.09460v11/#bib.bibx187)\]. This situation manifests when the painted or imaged subject is looking into the camera or at the eyes of the painter \[[188](https://arxiv.org/html/2207.09460v11/#bib.bibx188), [189](https://arxiv.org/html/2207.09460v11/#bib.bibx189)\].
Single user-to-user systems based around bidirectional video implicitly align the user's gaze by constraining the camera to roughly the same location as the display. When viewed away from this ideal axis, it creates the feeling of being looked at regardless of where this observer is \[[190](https://arxiv.org/html/2207.09460v11/#bib.bibx190), [185](https://arxiv.org/html/2207.09460v11/#bib.bibx185), [186](https://arxiv.org/html/2207.09460v11/#bib.bibx186), [187](https://arxiv.org/html/2207.09460v11/#bib.bibx187)\], or the "collapsed view effect" \[[191](https://arxiv.org/html/2207.09460v11/#bib.bibx191)\] where perception of gaze transmitted from a 2 dimensional image or video is dependent on the incidence of originating gaze to the transmission medium.
Multiple individuals using one such channel can feel as if they are being looked at simultaneously, leading to a breakdown in the normal non-verbal communication which mediates turn passing \[[192](https://arxiv.org/html/2207.09460v11/#bib.bibx192)\]. There is research investigating this sensitivity when the gaze is mediated by a technology, finding that "disparity between the optical axis of the camera and the looking direction of a looker should be at most 1.2 degrees in the horizontal direction, and 1.7 degrees in vertical direction to support eye contact\" \[[171](https://arxiv.org/html/2207.09460v11/#bib.bibx171), [193](https://arxiv.org/html/2207.09460v11/#bib.bibx193)\]. It seems that humans assume that they are being looked at unless they are sure that they are not \[[172](https://arxiv.org/html/2207.09460v11/#bib.bibx172)\].
To be clear, there are technological solutions to this problem, but it's useful in the context of discussing metaverse to know that this problem exists. It's known that there are cognitive dissonances around panes of video conference images, but it seems that the effect is truely limited to 2D surfaces. A 3D projection surface (a physical model of a human) designed to address this problem completely removed the Mona Lisa effect \[[190](https://arxiv.org/html/2207.09460v11/#bib.bibx190)\].
Metaverse then perhaps offers the promise of solving this, making more natural interaction possible, but it's clearly a long way from delivering on those promises right now. We need to understand what's important and try to map these into a metaverse product.
- ### 7.4 What's important for human communication
- #### 7.4.1 Vocal
The ubiquitous technology to mediate conversation is, of course, the telephone. The [2021 Ericsson mobility report](https://www.ericsson.com/en/reports-and-papers/mobility-report/reports/november-2021) states that there are around 8 billion mobile subscriptions globally. More people have access to mobile phones than to working toilets [according to UNICEF](https://www.unicef.org/innovation/stories/more-cellphones-toilets).
Joupii and Pan designed a system which focused attention on spatially correct high definition audio. They found "significant improvement over traditional audio conferencing technology, primarily due to the increased dynamic range and directionality. \[[194](https://arxiv.org/html/2207.09460v11/#bib.bibx194)\]. Aoki et al. also describe an audio only system with support for spatial cues \[[195](https://arxiv.org/html/2207.09460v11/#bib.bibx195)\].
In the following sections we will attempt to rigorously identify just what is important for our proposed application of business centric communication, supportive of trust, and thereby value transfer.
In his book 'Bodily Communication' \[[196](https://arxiv.org/html/2207.09460v11/#bib.bibx196)\] Michael Argyle divides vocal signals into the following categories:
1.  [1.]  2.  [2.]      Non-Verbal Vocalisations
    1.  [(a)]          Linked to Speech
        1.  [i.]          2.  [ii.]          3.  [iii.]      2.  [(b)]          Independent of Speech
        1.  [i.]          2.  [ii.]              Paralinguistic (emotion and interpersonal attitudes)         3.  [iii.]              Personal voice and quality of accent
Additional to the semantic content of verbal communication there is a rich layer of meaning in pauses, gaps, and overlaps \[[197](https://arxiv.org/html/2207.09460v11/#bib.bibx197)\] which help to mediate who is speaking and who is listening in multi-party conversation. This mediation of turn passing, to facilitate flow, is by no means a given and is highly dependent on context and other factors \[[198](https://arxiv.org/html/2207.09460v11/#bib.bibx198)\]. Interruptions are also a major factor in turn passing.
This extra-verbal content \[[199](https://arxiv.org/html/2207.09460v11/#bib.bibx199)\] extends into physical cues, so-called 'nonverbal' cues, and there are utterances which link the verbal and non-verbal \[[200](https://arxiv.org/html/2207.09460v11/#bib.bibx200)\]. This will be discussed later, but to an extent, it is impossible to discuss verbal communication without regard to the implicit support which exists around the words themselves.
In the context of all technology-mediated conversation the extra-verbal is easily compromised if technology used to support communication over a distance does not convey the information, or conveys it badly. This can introduce additional complexity \[[200](https://arxiv.org/html/2207.09460v11/#bib.bibx200)\].
These support structures are pretty much lacking in metaverse XR systems. The goal then here perhaps is to examine the state-of-the-art, and remove as many of the known barriers as possible. Such a process might better support trust, which might better support the kind of economic and activity we seek to engineer.
When examining just verbal / audio communication technology it can be assumed that the physical non-verbal cues are lost, though not necessarily unused. In the absence of non-verbal cues it falls to timely vocal signals to take up the slack when framing and organising the turn passing. For the synchronising of vocal signals between the parties to be effective the systemic delays must remain small. System latency, the inherent delays added by the communication technology, can allow slips or a complete breakdown of 'flow' \[katagiri2007aiduti\]. This problem can be felt in current social VR platforms, though people don't necessarily identify the cause of the breakdown correctly. In the main they feel to the users like a bad "audio-only" teleconference.
With that said, the transmission of verbal / audio remains the most critical element for interpersonal communication as the most essential meaning is encoded semantically. There is a debate about ratios of how much information is conveyed through the various human channels \[[201](https://arxiv.org/html/2207.09460v11/#bib.bibx201)\], but it is reasonable to infer from its ubiquity that support for audio is essential for meaningful communication over a distance. We have seen that it must be timely, to prevent a breakdown of framing, and preferably have sufficient fidelity to convey sub-vocal utterances.
For social immersive VR for business users, a real-time network such as websockets, RTP, or UDP seems essential, much better microphones are important, and the system should support both angular spatialisation, and respond to distance between interlocutors.
- #### 7.4.2 Nonverbal
We have already seen that verbal exchanges take place in a wider context of sub vocal and physical cues. In addition, the spatial relationship between the parties, their focus of attention, their gestures and actions, and the wider context of their environment all play a part in communication \[[202](https://arxiv.org/html/2207.09460v11/#bib.bibx202)\]. These are identified as follows by Gillies and Slater \[[203](https://arxiv.org/html/2207.09460v11/#bib.bibx203)\] in their paper on virtual agents.
    Head position and orientation
This is clearly important for our proposed collaborative mixed reality application. Below we will examine these six areas by looking across the wider available research.
- ##### Gaze
Of particular importance is judgement of eye gaze which is normally fast, accurate and automatic, operating at multiple levels of cognition through multiple cues \[[196](https://arxiv.org/html/2207.09460v11/#bib.bibx196), [204](https://arxiv.org/html/2207.09460v11/#bib.bibx204), [205](https://arxiv.org/html/2207.09460v11/#bib.bibx205), [204](https://arxiv.org/html/2207.09460v11/#bib.bibx204), [206](https://arxiv.org/html/2207.09460v11/#bib.bibx206), [207](https://arxiv.org/html/2207.09460v11/#bib.bibx207), [176](https://arxiv.org/html/2207.09460v11/#bib.bibx176)\].
Gaze in particular aids smooth turn passing \[[208](https://arxiv.org/html/2207.09460v11/#bib.bibx208)\] \[[209](https://arxiv.org/html/2207.09460v11/#bib.bibx209)\] and lack of support for eye gaze has been found to decrease the efficiency of turn passing by 25% \[[210](https://arxiv.org/html/2207.09460v11/#bib.bibx210)\].
There are clear patterns to eye gaze in groups, with the person talking, or being talked to, probably also being looked at \[[211](https://arxiv.org/html/2207.09460v11/#bib.bibx211)\] \[[212](https://arxiv.org/html/2207.09460v11/#bib.bibx212)\]. To facilitate this groups will tend to position themselves to maximally enable observation of the gaze of the other parties \[[207](https://arxiv.org/html/2207.09460v11/#bib.bibx207)\]. This intersects with proxemics which will be discussed shortly. In general people look most when they are listening, with short glances of 3-10 seconds \[[205](https://arxiv.org/html/2207.09460v11/#bib.bibx205)\]. Colburn et al. suggest that gaze direction and the perception of the gaze of others directly impacts social cognition \[[213](https://arxiv.org/html/2207.09460v11/#bib.bibx213)\] and this has been supported in a follow up study \[[214](https://arxiv.org/html/2207.09460v11/#bib.bibx214)\].
The importance of gaze is clearly so significant in evolutionary terms that human acuity for eye direction is considered high at  30 sec arc \[[215](https://arxiv.org/html/2207.09460v11/#bib.bibx215)\] with straight binocular gaze judged more accurately than straight monocular gaze \[[216](https://arxiv.org/html/2207.09460v11/#bib.bibx216)\], when using stereo vision.
Regarding the judgement of the gaze of others, Symons et al. suggested that "people are remarkably sensitive to shifts in a person's eye gaze" in triadic conversation \[[215](https://arxiv.org/html/2207.09460v11/#bib.bibx215)\]. This perception of the gaze of others operates at a low level and is automatic. Langton et al. cite research stating that the gaze of others is "able to trigger reflexive shifts of an observer's visual attention" and further discuss the deep biological underpinnings of gaze processing \[[212](https://arxiv.org/html/2207.09460v11/#bib.bibx212)\].
When discussing technology-mediated systems, Vertegaal & Ding suggested that understanding the effects of gaze on triadic conversation is "crucial for the design of teleconferencing systems and collaborative virtual environments" \[[192](https://arxiv.org/html/2207.09460v11/#bib.bibx192)\], and further found correlation between the amount of gaze, and amount of speech. Vertegaal & Slagter suggest that "gaze function(s) as an indicator of conversational attention in multiparty conversations" \[[211](https://arxiv.org/html/2207.09460v11/#bib.bibx211)\]. It seems like is we are to have useful markets within social immersive environments then support for natural gaze effects should be a priority.
Wilson et al. found that subjects can "discriminate gaze focused on adjacent faces up to \[3.5m\]" \[[217](https://arxiv.org/html/2207.09460v11/#bib.bibx217)\]. This perhaps gives us a testable benchmark within a metaverse application which is eye gaze enabled. In this regard Schrammel et al. investigated to what extent embodied agents can elicit the same responses in eye gaze detection \[[218](https://arxiv.org/html/2207.09460v11/#bib.bibx218)\].
Vertegaal et al. found that task performace was 46% better when gaze was synchronised in their telepresence scenario. As they point out, gaze synchonisation (temporal and spatial) is 'commendable' in all such group situations, but the precise utility will depend upon the task \[[192](https://arxiv.org/html/2207.09460v11/#bib.bibx192)\].
There has been some success in the automatic detection of the focus of attention of participants in multi party meetings \[[219](https://arxiv.org/html/2207.09460v11/#bib.bibx219), [220](https://arxiv.org/html/2207.09460v11/#bib.bibx220)\]. More recently, eye tracking technologies allow the recording and replaying of accurate eye gaze information \[[221](https://arxiv.org/html/2207.09460v11/#bib.bibx221)\] alongside information about pupil dilation toward determination of honesty and social presence \[[222](https://arxiv.org/html/2207.09460v11/#bib.bibx222)\]. It seems there are trust and honesty issues conflated with how collaborants in a virtual space are represented.
In summary, gaze awareness does not just mediate verbal communication but rather is a complex channel of communication in its own right. Importantly, gaze has a controlling impact on those who are involved in the communication at any one time, including and excluding even beyond the current participants. Perhaps the systems we propose in this book need to demand eye gaze support, but it is clear that it should be recommended, and that the software selected should support the technology integration in principle.
- ##### Mutual Gaze
Aygyle and Cook established early work around gaze and mutual gaze, with their seminal book of the same title \[[204](https://arxiv.org/html/2207.09460v11/#bib.bibx204)\], additionally detailing confounding factors around limitations and inaccuracies in observance of gaze and how this varies with distance \[[206](https://arxiv.org/html/2207.09460v11/#bib.bibx206), [196](https://arxiv.org/html/2207.09460v11/#bib.bibx196), [223](https://arxiv.org/html/2207.09460v11/#bib.bibx223)\].
Mutual gaze is considered to be the most sophisticated form of gaze awareness with significant impact on dyadic conversation especially \[[223](https://arxiv.org/html/2207.09460v11/#bib.bibx223), [198](https://arxiv.org/html/2207.09460v11/#bib.bibx198), [224](https://arxiv.org/html/2207.09460v11/#bib.bibx224)\]. The effects seem more profound than just helping to mediate flow and attention, with mutual eye gaze aiding in memory recall and the formation of impressions \[[225](https://arxiv.org/html/2207.09460v11/#bib.bibx225)\].
While reconnection of mutual eye gaze through a technology boundary does not seem completely necessary it is potentially important, with impact on subtle elements of one-to-one communication, and therefore discrimination of eye gaze direction should be bi-directional if possible, and if possible have sufficient accuracy to judge direct eye contact. In their review Bohannon et al. said that the issue of rejoining eye contact must be addressed in order to fully realise the richness of simulating face-to-face encounters \[[225](https://arxiv.org/html/2207.09460v11/#bib.bibx225)\].
Mutual gaze is a challenging affordance as bi-directional connection of gaze is not a trivial problem. It's perhaps best to view this as at the 'edge' of our requirements for a metaverse.
- ##### Mutual Gaze in Telepresence
We have seen that transmission of attention can broadly impact communication in subtle ways, impacting empathy, trust, cognition, and co-working patterns. Mutual gaze (looking into one another's eyes), is currently the high water mark for technology-mediated conversation.
Many attempts have been made to re-unite mutual eye gaze when using tele-conferencing systems. In their 2015 review of approaches Regenbrecht and Langlotz found that none of the methods they examined were completely ideal \[[226](https://arxiv.org/html/2207.09460v11/#bib.bibx226)\]. They found most promise in 2D and 3D interpolation techniques, which will be discussed in detail later, but they opined that such systems were very much ongoing research and lacked sufficient optimisation.
A popular approach uses the so called 'Peppers Ghost' phenomenon \[[227](https://arxiv.org/html/2207.09460v11/#bib.bibx227)\], where a semi silvered mirror presents an image to the eye of the observer, but allows a camera to view through from behind the angled mirror surface. The earliest example of this is Rosental's two way television system in 1947 \[[228](https://arxiv.org/html/2207.09460v11/#bib.bibx228)\], though Buxton et al. 'Reciprocal Video Tunnel' from 1992 is more often cited \[[229](https://arxiv.org/html/2207.09460v11/#bib.bibx229)\]. This optical characteristic isn't supported by retroreflective projection technology, and besides requires careful control of light levels either side of the semi-silvered surface.
The early GAZE-2 system (which makes use of Pepper's ghost) is novel in that it uses an eye tracker to select the correct camera from several trained on the remote user. This ensures that the correct returned gaze (within the ability of the system) is returned to the correct user on the other end of the network \[[230](https://arxiv.org/html/2207.09460v11/#bib.bibx230)\]. Mutual gaze capability is later highlighted as an affordance supported or unsupported by key research and commercial systems.
- ##### Head Orientation
Orientation of the head (judged by the breaking of bilateral symmetry and alignment of nose) is a key factor when judging attention. Perception of head orientation can be judged to within a couple of degrees \[[217](https://arxiv.org/html/2207.09460v11/#bib.bibx217)\].
It has been established that head gaze can be detected all the way out to the extremis of peripheral vision, with accurate eye gaze assessment only achievable in central vision \[[188](https://arxiv.org/html/2207.09460v11/#bib.bibx188)\]. This is less of use for our metaverses at this time, because user field of view is almost always restricted in such systems. More usefully, features of illumination can alter the apparent orientation of the head \[[231](https://arxiv.org/html/2207.09460v11/#bib.bibx231)\].
Head motion over head orientation is a more nuanced propostion and can be considered a micro gesture \[[232](https://arxiv.org/html/2207.09460v11/#bib.bibx232)\]. Head tracking systems within head mounted displays can certainly detect these tiny movements, but it's clear that not all of this resolution is passed into shared virtual settings through avatars. It would be beneficial to be able to fine tune this feature within any software selected.
It is possible that 3D displays are better suited to perception of head gaze since it is suggested that they are more suitable for "shape understanding tasks" \[[233](https://arxiv.org/html/2207.09460v11/#bib.bibx233)\]
Bailenson, Baell, and Blascovich found that giving avatars rendered head movements in a shared virtual environment decreased the amount of talking, possibly as the extra channel of head gaze was opened up. They also reported that subjectively, communication was enhanced \[[234](https://arxiv.org/html/2207.09460v11/#bib.bibx234)\].
Clearly head orientation is an important indicator of the direction of attention of members of a group and can be discerned even in peripheral vision. This allows the focus of several parties to be followed simultaneously and is an important affordance to replicate on any multi-party communication system.
- ##### Combined Head and Eye Gaze
Rienks et al. found that head orientation alone does not provide a reliable cue for identification of the speaker in a multiparty setting \[[235](https://arxiv.org/html/2207.09460v11/#bib.bibx235)\]. Stiefelhagen & Zhu found "that head orientation contributes 68.9% to the overall gaze direction on average" \[[220](https://arxiv.org/html/2207.09460v11/#bib.bibx220)\], though head and eye gaze seem to be judged interdependently \[[216](https://arxiv.org/html/2207.09460v11/#bib.bibx216)\]. Langton noted that head and eye gaze are "mutually influential in the analysis of social attention" \[[212](https://arxiv.org/html/2207.09460v11/#bib.bibx212)\], and it is clear that transmission of 'head gaze' by any mediating system, enhances rather than replaces timely detection of subtle cues. Combined head and eye gaze give the best of both worlds and extend the lateral field of view in which attention can be reliably conveyed to others \[[188](https://arxiv.org/html/2207.09460v11/#bib.bibx188)\].
- ##### Other Upper Body: Overview
While it is well evidenced that there are advantages to accurate connection of the gaze between conversational partners \[[206](https://arxiv.org/html/2207.09460v11/#bib.bibx206), [198](https://arxiv.org/html/2207.09460v11/#bib.bibx198)\], there is also a body of evidence that physical communication channels extend beyond the face \[[198](https://arxiv.org/html/2207.09460v11/#bib.bibx198), [236](https://arxiv.org/html/2207.09460v11/#bib.bibx236)\] and include both micro (shrugs, hands and arms), and macro movement of the upper body \[[237](https://arxiv.org/html/2207.09460v11/#bib.bibx237)\]. Goldin-Meadow suggests that gesturing aids conversational flow by resolving mismatches and aiding cognition \[[238](https://arxiv.org/html/2207.09460v11/#bib.bibx238)\].
In their technology-mediated experiment which compared face to upper body and face on a flat screen, Nguyen and Canny found that "upper-body framing improves empathy measures and gives results not significantly different from face-to-face under several empathy measures" \[[236](https://arxiv.org/html/2207.09460v11/#bib.bibx236)\].
The upper body can be broken up as follows:
[Facial\ ] Much emotional context can be described by facial expression (display) alone \[[237](https://arxiv.org/html/2207.09460v11/#bib.bibx237), [239](https://arxiv.org/html/2207.09460v11/#bib.bibx239)\], with smooth transition between expressions seemingly important \[[240](https://arxiv.org/html/2207.09460v11/#bib.bibx240)\]. This suggests that mediating technologies should support high temporal resolution, or at least that there is a minimum resolution between which transitions between expressions become too 'categorical'. Some aspects of conversational flow appear to be mediated in part by facial expression \[[241](https://arxiv.org/html/2207.09460v11/#bib.bibx241)\]. There are gender differences in the perception of facial affect \[[242](https://arxiv.org/html/2207.09460v11/#bib.bibx242)\].
[Gesturing] \ (such as pointing at objects) paves the way for more complex channels of human communication and is a basic and ubiquitous channel \[[243](https://arxiv.org/html/2207.09460v11/#bib.bibx243)\]. Conversational hand gestures provide a powerful additional augmentation to verbal content \[[244](https://arxiv.org/html/2207.09460v11/#bib.bibx244)\].
[Posture] \ Some emotions can be conveyed through upper body configurations alone. Argyle details some of these \[[196](https://arxiv.org/html/2207.09460v11/#bib.bibx196)\] and makes reference to the posture of the body and the arrangement of the arms (i.e. folded across the chest). These are clearly important cues. Kleinsmith and Bianchi-Berthouze assert that \"some affective expressions may be better communicated by the body than the face\" \[[245](https://arxiv.org/html/2207.09460v11/#bib.bibx245)\].
[Body Torque] \ In multi-party conversation, body torque, that is the rotation of the trunk from front facing, can convey aspects of attention and focus \[[246](https://arxiv.org/html/2207.09460v11/#bib.bibx246)\].
In summary, visual cues which manifest on the upper body and face can convey meaning, mediate conversation, direct attention, and augment verbal utterances.
- ##### Effect of Shared Objects on Gaze
Ou et al. detail shared task eye gaze behaviour "in which helpers seek visual evidence for workers' understanding when they lack confidence of that understanding, either from a shared, or common vocabulary" \[[247](https://arxiv.org/html/2207.09460v11/#bib.bibx247)\].
Murray et al. found that in virtual environments, eye gaze is crucial for discerning what a subject is looking at \[[248](https://arxiv.org/html/2207.09460v11/#bib.bibx248)\]. This work is shown in Figure [7.3](https://arxiv.org/html/2207.09460v11/#Ch7.F3 "Figure 7.3 ‣ Effect of Shared Objects on Gaze ‣ 7.4.2 Nonverbal ‣ 7.4 What’s important for human communication ‣ Chapter 7 Collaborative mixed reality ‣ Part I State of the art").
It is established that conversation around a shared object or task, especially a complex one, mitigates gaze between parties \[[204](https://arxiv.org/html/2207.09460v11/#bib.bibx204)\] and this suggests that in some situations around shared tasks in metaverses it may be appropriate to reduce fidelity of representation of the avatars.
![Figure 7.3: Eye tracked eye gaze awareness in VR. Murray et al. used immersive and semi immersive systems alongside eye trackers to examine the ability of two avatars to detect the gaze awareness of a similarly immersed collaborator.](../assets/x3.jpg) 
- ##### Tabletop and Shared Task
In early telepresence research Buxton and William argued through examples that "effective telepresence depends on quality sharing of both person and task space \[[229](https://arxiv.org/html/2207.09460v11/#bib.bibx229)\].
In their triadic shared virtual workspace Tang et al. found difficulty in reading shared text using a 'round the table' configuration, a marked preference for working collaboratively on the same side of the table. They also found additional confusion as to the identity of remote participants \[[249](https://arxiv.org/html/2207.09460v11/#bib.bibx249)\]. Tse et al. found that pairs can work well over a shared digital tabletop, successfully overcoming a single user interface to interleave tasks \[[250](https://arxiv.org/html/2207.09460v11/#bib.bibx250)\].
Tang et al. demonstrate that collaborators engage and disengage around a group activity through several distinct, recognizable mechanisms with unique characteristics \[[251](https://arxiv.org/html/2207.09460v11/#bib.bibx251)\]. They state that tabletop interfaces should offer a variety of tools to facilitate this fluidity.
Camblend is a shared workspace with panoramic high resolution video. It maintains some spatial cues between locations by keeping a shared object in the video feeds \[[252](https://arxiv.org/html/2207.09460v11/#bib.bibx252), [253](https://arxiv.org/html/2207.09460v11/#bib.bibx253)\]. Participants successfully resolved co-orientation within the system.
The t-room system implemented by Luff et al. surrounds co-located participants standing at a shared digital table with life sized body and head video representations of remote collaborators \[[254](https://arxiv.org/html/2207.09460v11/#bib.bibx254)\] but found that there were incongruities in the spatial and temporal matching between the collaborators which broke the flow of conversation. Tuddenham et al. found that co-located collaborators naturally devolved 'territory' of working when sharing a task space, and that this did not happen the same way with a tele-present collaborator \[[255](https://arxiv.org/html/2207.09460v11/#bib.bibx255)\]. Instead remote collaboration adapted to use a patchwork of ownership of a shared task. It seems obvious to say that task ownership is a function of working space, but it is interesting that the research found no measurable difference in performance when the patchwork coping strategy was employed.
The nature of a shared collaborative task and/or interface directly impacts the style of interaction between collaborators. This will have a bearing on the choice of task for experimentation \[[256](https://arxiv.org/html/2207.09460v11/#bib.bibx256), [257](https://arxiv.org/html/2207.09460v11/#bib.bibx257)\].
- ### 7.5 Psychology of Technology-Mediated Interaction
- #### 7.5.1 Proxemics
Proxemics is the formal study of the regions of interpersonal space begun in the late 50's by Hall and Sommers and building toward The Hidden Dimension \[[258](https://arxiv.org/html/2207.09460v11/#bib.bibx258)\], which details bands of space (Figure [7.4](https://arxiv.org/html/2207.09460v11/#Ch7.F4 "Figure 7.4 ‣ 7.5.1 Proxemics ‣ 7.5 Psychology of Technology-Mediated Interaction ‣ Chapter 7 Collaborative mixed reality ‣ Part I State of the art")) that are implicitly and instinctively created by humans and which have a direct bearing on communication.
![Figure 7.4: Bands of social space around a person Image CC0 [from wikipedia](https://en.wikipedia.org/wiki/Proxemics).](../assets/proxemics.png) 
Distance between conversational partners, and affiliation, also have a bearing on the level of eye contact \[[205](https://arxiv.org/html/2207.09460v11/#bib.bibx205)\] with a natural distance equilibrium being established and developed throughout, through both eye contact and a variety of subtle factors. Argyle & Ingham provide levels of expected gaze and mutual gaze against distance \[[206](https://arxiv.org/html/2207.09460v11/#bib.bibx206)\]. These boundaries are altered by ethnicity \[[259](https://arxiv.org/html/2207.09460v11/#bib.bibx259), [196](https://arxiv.org/html/2207.09460v11/#bib.bibx196)\] and somewhat by gender \[[260](https://arxiv.org/html/2207.09460v11/#bib.bibx260)\], and age \[[261](https://arxiv.org/html/2207.09460v11/#bib.bibx261), [242](https://arxiv.org/html/2207.09460v11/#bib.bibx242)\].
Even with significant abstraction by communication systems (such as SecondLife) social norms around personal space persist \[[262](https://arxiv.org/html/2207.09460v11/#bib.bibx262), [263](https://arxiv.org/html/2207.09460v11/#bib.bibx263), [264](https://arxiv.org/html/2207.09460v11/#bib.bibx264)\]. Bailenson & Blascovich found that even in Immersive Collaborative Virtual Environments (ICVE's) "participants respected personal space of the humanoid representation"\[[263](https://arxiv.org/html/2207.09460v11/#bib.bibx263)\] implying that this is a deeply held 'low-level' psychophysical reaction \[[265](https://arxiv.org/html/2207.09460v11/#bib.bibx265)\]. The degree to which this applies to non-humanoid avatars seems under explored.
Maeda et al. \[[266](https://arxiv.org/html/2207.09460v11/#bib.bibx266)\] found that seating position impacts the level of engagement in teleconferencing. Taken together with the potential for reconfiguration within the group as well as perhaps signalling for the attention of participants outside of the confines of the group in an open business metaverse setting.
When considering the attention of engaging with people outside the confines of a meeting Hager et al. found that gross expressions can be resolved by humans over long distances \[[267](https://arxiv.org/html/2207.09460v11/#bib.bibx267), [196](https://arxiv.org/html/2207.09460v11/#bib.bibx196)\]. It seems that social interaction begins around 7.5m in the so-called 'public space' \[[258](https://arxiv.org/html/2207.09460v11/#bib.bibx258)\]. Recreating this affordance in a metaverse would be a function of the display resolution, and seems another 'stretch goal' rather than a core requirement.
- #### 7.5.2 Attention
The study of attention is a discrete branch of psychology. It is the study of cognitive selection toward a subjective or objective sub focus, to the relative exclusion of other stimulae. It has been defined as "a range of neural operations that selectively enhance processing of information" \[[268](https://arxiv.org/html/2207.09460v11/#bib.bibx268)\]. In the context of interpersonal communication it can be refined to apply to selectively favouring a conversational agent or object or task above other stimuli in the contextual frame.
Humans can readily determine the focus of attention of others in their space \[[219](https://arxiv.org/html/2207.09460v11/#bib.bibx219)\] and preservation of the spatial cues which support this are important for technology-mediated communication \[[174](https://arxiv.org/html/2207.09460v11/#bib.bibx174)\] \[[220](https://arxiv.org/html/2207.09460v11/#bib.bibx220)\].
The interplay between conversational partners, especially the reciprocal perception of attention, is dubbed the perceptual crossing \[[269](https://arxiv.org/html/2207.09460v11/#bib.bibx269), [270](https://arxiv.org/html/2207.09460v11/#bib.bibx270)\].
This is a complex field of study with gender, age, and ethnicity all impacting the behaviour of interpersonal attention \[[271](https://arxiv.org/html/2207.09460v11/#bib.bibx271), [261](https://arxiv.org/html/2207.09460v11/#bib.bibx261), [196](https://arxiv.org/html/2207.09460v11/#bib.bibx196), [242](https://arxiv.org/html/2207.09460v11/#bib.bibx242), [272](https://arxiv.org/html/2207.09460v11/#bib.bibx272)\]. Vertegaal has done a great deal of work on awareness and attention in technology-mediated situations and the work of his group is cited throughout this chapter \[[273](https://arxiv.org/html/2207.09460v11/#bib.bibx273)\]. As an example it is still such a challenge to "get" attention through mediated channels of communication, that some research \[[274](https://arxiv.org/html/2207.09460v11/#bib.bibx274), [174](https://arxiv.org/html/2207.09460v11/#bib.bibx174)\] and many commercial systems such as 'blackboard collaborate', Zoom, and Teams use tell tale signals (such as a microphone icon) to indicate when a participant is actively contributing. Some are automatic, but many are still manual, requiring that a user effectively hold up a virtual hand to signal their wish to communicate.
Langton et al. cite research stating that the gaze of others is "able to trigger reflexive shifts of an observer's visual attention".
Regarding the attention of others, Fagal et el demonstrated that eye visibility impacts collaborative task performance when considering a shared task \[[224](https://arxiv.org/html/2207.09460v11/#bib.bibx224)\]. Novick et al. performed analysis on task hand-off gaze patterns which is useful for extension into shared task product design \[[209](https://arxiv.org/html/2207.09460v11/#bib.bibx209)\].
- #### 7.5.3 Behaviour
Hedge et al. suggested that gaze interactions between strangers and friends may be different which could have an impact on the kinds of interactions a metaverse might best support \[[208](https://arxiv.org/html/2207.09460v11/#bib.bibx208)\]. Voida et al. elaborate that prior relationships can cause "internal fault lines" in group working \[[275](https://arxiv.org/html/2207.09460v11/#bib.bibx275)\]. When new relationships are formed the "primary concern is one of uncertainty reduction or increasing predictability about the behaviour of both themselves and others in the interaction" \[[276](https://arxiv.org/html/2207.09460v11/#bib.bibx276)\]. This concept of smoothness in the conversation is a recurring theme, with better engineered systems introducing less extraneous artefacts into the communication, and so disturbing the flow less. Immersive metaverse are rife with artefacts.
In a similar vein the actor-observer effect describes the mismatch between expectations which can creep into conversation. Conversations mediated by technology can be especially prone to diverging perceptions of the causes of behaviour \[[277](https://arxiv.org/html/2207.09460v11/#bib.bibx277)\]. Basically this means misunderstandings happen, and are harder to resolve with more mediating technology.
Interacting subjects progress conversation through so-called 'perception-action' loops which are open to predictive modelling through discrete hidden Markov models \[[278](https://arxiv.org/html/2207.09460v11/#bib.bibx278)\]. This might allow product OKR testing of the effectiveness of engineered systems \[[279](https://arxiv.org/html/2207.09460v11/#bib.bibx279)\].
It may be that the perception-behaviour link where unconscious mirroring of posture bolsters empathy between conversational partners, especially when working collaboratively \[[280](https://arxiv.org/html/2207.09460v11/#bib.bibx280)\], and the extent to which posture is represented through a communication medium may be important.
Landsberger posited the Hawthorne effect \[[281](https://arxiv.org/html/2207.09460v11/#bib.bibx281)\]. Put simply this is a short term increase in productivity that may occur as a result of being watched or appreciated. The impression of being watched changes gaze patterns during experimentation, with even implied observation through an eye tracker modifying behaviour \[[282](https://arxiv.org/html/2207.09460v11/#bib.bibx282)\].
There are also some fascinating findings around the neural correlates of gratitude, which turn out not to be linked to gratitude felt by a participant, but rather the observation of gratitude received within a social context \[[283](https://arxiv.org/html/2207.09460v11/#bib.bibx283)\]. These findings have potentially useful implications for the behaviours of AI actors and avatars within an immersive social scene.
There is much historic work describing "the anatomy of cooperation\" \[[284](https://arxiv.org/html/2207.09460v11/#bib.bibx284)\], and this might better inform how educational or instructional tasks are built in metaverse applications.
Cuddihy and Walters defined an early model for assessing desktop interaction mechanisms for social virtual environments \[[285](https://arxiv.org/html/2207.09460v11/#bib.bibx285)\].
- ##### Perception Of Honesty
Hancock et al. state that we are most likely to lie, and to be lied to, on the telephone \[[286](https://arxiv.org/html/2207.09460v11/#bib.bibx286)\]. Technology used for communication impacts interpersonal honesty. It seems that at some level humans know this; lack of eye contact leads to feelings of deception, impacting trust \[[287](https://arxiv.org/html/2207.09460v11/#bib.bibx287)\]. This has a major impact on immersive social XR, which often does not support mutual gaze. Trust is crucial for business interactions.
Further there are universal expressions, micro-expressions, and blink rate which can betray hidden emotions \[[288](https://arxiv.org/html/2207.09460v11/#bib.bibx288)\], though the effects are subtle and there is a general lack of awareness by humans of their abilities in this regard \[[287](https://arxiv.org/html/2207.09460v11/#bib.bibx287)\]. Absence of support for such instinctive cues inhibits trust \[[289](https://arxiv.org/html/2207.09460v11/#bib.bibx289)\]. Support for these rapid and transient facial features demands high resolution reproduction in both resolution and time domains. There is detectable difference in a participant's ability to detect deception when between video conference mediated communication and that mediated by avatars \[[222](https://arxiv.org/html/2207.09460v11/#bib.bibx222)\]. Systems should aim for maximally faithful reproduction.
- #### 7.5.4 Presence, Co-presence, and Social Presence
Presence is a heavily cited historic indicator of engagement in virtual reality, though the precise meaning has been interpreted differently by different specialisms \[[290](https://arxiv.org/html/2207.09460v11/#bib.bibx290), [291](https://arxiv.org/html/2207.09460v11/#bib.bibx291)\]. It is generally agreed to be the 'sense of being' in a virtual environment \[[292](https://arxiv.org/html/2207.09460v11/#bib.bibx292)\]. Slater extends this to include the "extent to which the VE becomes dominant\".
Beck et al. reviewed 108 articles and synthesised an ontology of presence \[[290](https://arxiv.org/html/2207.09460v11/#bib.bibx290)\] which at its simplest is as follows:
1.  [1.]      Sentient presence
    1.  [(a)]      2.  [(b)]  2.  [2.]      Non-sentient
    1.  [(a)]      2.  [(b)]          Mental immersion = psychological state
When presence is applied to interaction it may be split into Telepresence, and Co/Social presence \[[293](https://arxiv.org/html/2207.09460v11/#bib.bibx293), [294](https://arxiv.org/html/2207.09460v11/#bib.bibx294)\]. Co-presence and/or social presence is the sense of "being there with another\", and describes the automatic responses to complex social cues \[[295](https://arxiv.org/html/2207.09460v11/#bib.bibx295), [296](https://arxiv.org/html/2207.09460v11/#bib.bibx296)\]. Social presence (and co-presence) refers in this research context to social presence which is mediated by technology (even extending to text based chat \[[297](https://arxiv.org/html/2207.09460v11/#bib.bibx297)\]), and has its foundations in psychological mechanisms which engender mutualism in the 'real'. This is analysed in depth by Nowak \[[298](https://arxiv.org/html/2207.09460v11/#bib.bibx298)\]. An examination of telepresence, co-presence and social presence necessarily revisits some of the knowledge already elaborated.
The boundaries between the three are blurred in research with conflicting results presented \[[299](https://arxiv.org/html/2207.09460v11/#bib.bibx299)\]. Biocca et al. attempted to enumerate the different levels and interpretations surrounding these vague words \[[300](https://arxiv.org/html/2207.09460v11/#bib.bibx300)\], and to distill them into a more robust theory which better lends itself to measurement. They suggest a solid understanding of the surrounding psychological requirements which need support in a mediated setting, and then a scope that is detailed and limited to the mediated situation.
Since 'social presence' has been subject to varied definitions \[[300](https://arxiv.org/html/2207.09460v11/#bib.bibx300)\] it is useful here to consider a single definition from the literature which defines it as "the ability of participants in the community of inquiry to project their personal characteristics into the community, thereby presenting themselves to the other participants as real people." \[[301](https://arxiv.org/html/2207.09460v11/#bib.bibx301), [290](https://arxiv.org/html/2207.09460v11/#bib.bibx290)\]. Similarly to specifically define co-presence for this research it is taken to be the degree to which participants in a virtual environment are "accesible, available, and subject to one another\" \[[300](https://arxiv.org/html/2207.09460v11/#bib.bibx300)\].
Social presence has received much attention and there are established questionnaires used in the field for measurement of the levels of perceived social presence yet the definitions here also remain broad, with some confusion about what is being measured \[[300](https://arxiv.org/html/2207.09460v11/#bib.bibx300)\].
Telepresence meanwhile is interaction with a different (usually remote) environment which may or may not be virtual, and may or may not contain a separate social/co-presence component.
Even in simple videoconferencing Bondareva and Bouwhuis stated (as part of an experimental design) that the following determinants are important to create social presence \[[302](https://arxiv.org/html/2207.09460v11/#bib.bibx302), [194](https://arxiv.org/html/2207.09460v11/#bib.bibx194)\].
1.  [1.]      Direct eye contact is preserved 2.  [2.]  3.  [3.]      Both remote participants appear life size 4.  [4.]      Possibility for participants to see the upper body of the     interlocutor 5.  [5.]      High quality image and correct colour reproduction 6.  [6.]      Audio with high S/N ratio 7.  [7.]  8.  [8.]      Minimization of the video and audio signal asynchrony 9.  [9.]      Availability of a shared working space.
Bondareva et al. went on to describe a person-to-person telepresence system with a semi-silvered mirror to reconnect eye gaze, which they claimed increased social presence indicators. Interestingly they chose a checklist of interpersonal interactions which they used against recordings of conversations through the system \[[302](https://arxiv.org/html/2207.09460v11/#bib.bibx302)\].
The idea of social presence as an indicator of the efficacy of the system, suggests the use of social presence questionnaires in the evaluation of the system \[[300](https://arxiv.org/html/2207.09460v11/#bib.bibx300)\]. Subjective questionnaires are however troublesome in measuring effectiveness of virtual agents and embodiments, with even nonsensical questions producing seemingly valid results \[[303](https://arxiv.org/html/2207.09460v11/#bib.bibx303)\]. Usoh et al. found that 'the real' produced only marginally higher presence results than the virtual \[[304](https://arxiv.org/html/2207.09460v11/#bib.bibx304)\]. It would be difficult to test products this way.
Nowak states that "A satisfactory level of co-presence with another mind can be achieved with conscious awareness that the interaction is mediated\" and asserts that while the mediation may influence the degree of co-presence it is not a prohibiting factor \[[298](https://arxiv.org/html/2207.09460v11/#bib.bibx298)\].
Baren and IJsselsteijn \[[305](https://arxiv.org/html/2207.09460v11/#bib.bibx305), [306](https://arxiv.org/html/2207.09460v11/#bib.bibx306)\] list 20 useful presence questionnaires in 2004 of which "Networked Minds\" seemed most appropriate for the research. Hauber et al. employed the "Networked Minds\" Social Presence questionnaire experimentally and found that while the measure could successfully discriminate between triadic conversation that is mediated or unmediated by technology, it could not find a difference between 2D and 3D mediated interfaces \[[307](https://arxiv.org/html/2207.09460v11/#bib.bibx307), [297](https://arxiv.org/html/2207.09460v11/#bib.bibx297)\].
In summary, social presence and co-presence are important historic measures of the efficacy of a communication system. Use of the term in literature peaked between 1999 and 2006 according to Google's ngram viewer and has been slowly falling off since. The questionnaire methodology has been challenged in recent research and while more objective measurement may be appropriate, the networked minds questions seem to be able to differentiate real from virtual interactions \[[306](https://arxiv.org/html/2207.09460v11/#bib.bibx306)\].
- ### 7.6 Other Systems to Support Business
There have been many attempts to support group working and rich data sharing between dispersed groups in a business setting. So called 'smart spaces' allow interaction with different displays for different activities and add in some ability to communicate with remote or even mobile collaborators on shared documents \[[182](https://arxiv.org/html/2207.09460v11/#bib.bibx182)\], with additional challenges for multi-disciplinary groups who are perhaps less familiar with one or more of the technology barriers involved \[[183](https://arxiv.org/html/2207.09460v11/#bib.bibx183)\].
Early systems like clearboard \[[184](https://arxiv.org/html/2207.09460v11/#bib.bibx184)\] demonstrated the potential for smart whiteboards with a webcam component for peer to peer collaborative working. Indeed it is possible to support this modality with Skype and a smartboard system (and up to deployments such as Accessgrid). They remain relatively unpopular however.
Displays need not be limited to 2 dimensional screens and can be enhanced in various ways.
Stereoscopy allows an illusion of depth to be added to a 2D image by exploiting the stereo depth processing characteristics of the human vision system. This technical approach is not perfect as it does not fully recreate the convergence and focus expected by the eyes and brain.
There are multiple approaches to separating the left and right eye images, these primarily being active (where a signal selectively blanks the input to left then right eyes in synchronicity with the display), passive, where either selective spectrum or selective polarisation of light allow different portions of a display access to different eyes, or physical arrangements which present different displays (or slices of light as in lenticular systems) to different eyes.
These barrier stereoscopy / lenticular displays use vertical light barriers built into the display to create multiple discrete channels of display which are accessed by moving horizontally with respect to the display. In this way it is possible to generate either a left/right eye image pair for 'autostereoscopic' viewing, or with the addition of head tracking and small motors. With these techniques multiple viewpoint or an adaptive realtime viewpoint update can be presented without the glasses required for active or passive stereoscopic systems.
- #### 7.6.1 Spatially Faithful Group
Hauber et al. combined videoconferencing, tabletop, and social presence analysis and tested the addition of 3D. They found a nuanced response when comparing 2D and 3D approaches to spatiality: 3D showed improved presence over 2D (chiefly through gaze support), while 2D demonstrated improved task performance because of task focus \[[308](https://arxiv.org/html/2207.09460v11/#bib.bibx308)\].
I3DVC reconstructs participants from multiple cameras and places them isotropically (spatially faithful) \[[309](https://arxiv.org/html/2207.09460v11/#bib.bibx309), [310](https://arxiv.org/html/2207.09460v11/#bib.bibx310)\]. The system uses a large projection screen, a custom table, and carefully defined seating positions. They discussed an "extended perception space\" which used identical equipment in the remote spaces in a tightly coupled collaborative 'booth'. It employed head tracking and multi camera reconstruction alongside large screens built into the booth. This system exemplified the physical restrictions which are required to limit the problems of looking into another space through the screen. Fuchs et al. demonstrated a similar system over a wide area network but achieved only limited resolution and frame rate with the technology of the day \[[311](https://arxiv.org/html/2207.09460v11/#bib.bibx311)\].
University of Southern California used a technically demanding real-time set-up with 3D face scanning and an autostereoscopic 3D display to generate multiple 'face tracked' viewpoints \[[312](https://arxiv.org/html/2207.09460v11/#bib.bibx312)\]. This had the disadvantage of displaying a disembodied head.
MAJIC is an early comparable system to support small groups with life size spatially correct video, but without multiple viewpoints onto the remote collaborators it was a one to 'some' system rather than 'some' to one. Additionally users were rooted to defined locations \[[313](https://arxiv.org/html/2207.09460v11/#bib.bibx313), [314](https://arxiv.org/html/2207.09460v11/#bib.bibx314)\].
There seems to be less interest recently in large display screens for spatially correct viewpoints between groups. The hardware is technically demanding and there may have been sufficient research done to limit investment in research questions. This doesn't mean that there is no future for metaverse applications. Imagine one of the new XR studio walls such as that used to film the Mandalorian. With application of telepresence research it would be possible to bring external metaverse participants into the 'backstage' virtual scene. These avatars would be able to explore the scene invisible to the actors, but could be given access to visual feeds from the stage side. This is a hybrid virtual/real metaverse with a well researched and understood boundary interface. It would be possible to give different access privileges to different levels of paying 'film studio tourist' or investor, with VIPs perhaps commanding a view onto the live filming. At the nadir of this it may be possible to bring producers and directors directly into the virtual studio as avatars on the screen boundary, with a spatially faithful view onto the set. For the purposes of this book it's also worth noting that NFTs of the experience and corresponding virtual objects from the scene could be monetised and sold within the metaverse.
- ##### Multiview
In order to reconnect directional cues of all kinds it is necessary for each party in the group to have a spatially correct view of the remote user which is particular for them. This requires a multi-view display, which has applications beyond telepresence but are used extensively in research which attempts to address these issues.
Nguyen and Canny demonstrated the 'Multiview' system \[[191](https://arxiv.org/html/2207.09460v11/#bib.bibx191)\]. Multiview is a spatially segmented system, that is, it presents different views to people standing in different locations simultaneously. They found similar task performance in trust tasks to face-to-face meetings, while a similar approach without spatial segmentation was seen to negatively impact performance.
In addition to spatial segmentation of viewpoints \[[315](https://arxiv.org/html/2207.09460v11/#bib.bibx315)\] it is possible to isolate viewpoints in the time domain. Different tracked users can be presented with their individual view of a virtual scene for a few milliseconds per eye, before another viewpoint is shown to another user. Up to six such viewpoints are supported in the c1x6 system \[[316](https://arxiv.org/html/2207.09460v11/#bib.bibx316)\] Similarly MM+Space offered 4 Degree-Of-Freedom Kinetic Display to recreate Multiparty Conversation Spaces \[[317](https://arxiv.org/html/2207.09460v11/#bib.bibx317)\]
- #### 7.6.2 Holography and Volumetric
Blanche et al. have done a great deal of research into holographic and volumetric displays using lasers, rotating surfaces, and light field technology \[[318](https://arxiv.org/html/2207.09460v11/#bib.bibx318), [319](https://arxiv.org/html/2207.09460v11/#bib.bibx319)\]. They are actively seeking to use their technologies for telepresence and their work is very interesting.
Similarly Jones et al. "HeadSPIN\" is a one-to-many 3D video teleconferencing system \[[312](https://arxiv.org/html/2207.09460v11/#bib.bibx312)\] which uses a rotating display to render the holographic head of a remote party. They achieve transmissible and usable framerate using structured light scanning of a remote collaborator as they view a 2D screen which they say shows a spatially correct view of the onlooking parties.
Eldes et al. used a rotating display to present multi-view autostereoscopic projected images to users \[[320](https://arxiv.org/html/2207.09460v11/#bib.bibx320)\].
Seelinder is an interesting system which uses parallax barriers to render a head which an onlooking viewer can walk around. The system uses 360 high resolution still images which means a new spatially segmented view of the head every 1 degreesof arc. They claim the system is capable of playback of video and this head in a jar multi-view system clearly has merit but is comparatively small, and as yet untested for telepresence \[[321](https://arxiv.org/html/2207.09460v11/#bib.bibx321)\].
These systems do not satisfy the requirement to render upper body for the viewers and are not situated (as described soon).
There's a future possible where real-time scanned avatar representation in persistent shared metaverse environments will be able to support business, but the camera rigs which currently generate such models are too bulky and involved for a good costs benefit analysis. It is more likely that recent advances in LIDAR phone scanning show the way. The allow realistic avatars to be quickly created for animation within metaverse scenes \[[322](https://arxiv.org/html/2207.09460v11/#bib.bibx322)\].
- ##### Project Skyline
Project Starline, is a next-generation video conferencing technology that aims to create a sense of presence, making you feel like you're sitting across the table from someone. It uses advanced hardware and software to achieve this.
-   [Hardware]      The newer Starline booth is a refined version of earlier models and     looks like a large 65-inch display on a stand. It contains color     cameras, depth sensors, microphones, and speakers. Additionally,     there are lights on the back of the display that serve as a key     light for the person on the call. These lights are mounted around     the person and used to create a depth map of the subject and the     room they're in. -   [Display]      The display creates an immersive 3D depth effect. It uses a barrier     lenticular light field display that shows a different image to your     left eye and to your right eye. This effect lets you compute depth     on the fly while doing all the head tracking in real time. The     display technology in Project Starline is significantly smoother and     more realistic than what you would experience with traditional 3D     movies. -   [Compute]      The computing side of Project Starline is responsible for rendering     the people using the system into realistic 3D models in real-time.     It uses AI and depth information gathered by the cameras to map the     exact shape, depth, texture, and lighting of the person. The result     is an ultra-realistic 3D representation of the person on the other     end of the call. -   [Audio]      The system features spatial audio such that the perceived audio     changes based on where you are leaning or moving, creating an even     more immersive and realistic experience.
At this point, Google has been working with several companies who are using these booths for meetings, and it's hoped that as the technology becomes cheaper and more refined, it they assert that it could revolutionize the way we communicate, though the cost of the system and 'single user to single user' restriction is likely to be a blocker to crucial business adoption.
- #### 7.6.3 Simulated Humans
- ##### Uncanniness
When employing simulation representations of humans it may be the case that there is an element of weirdness to some of these systems, especially those that currently represent a head without a body. Mori has demonstrated The Uncanny Valley \[[323](https://arxiv.org/html/2207.09460v11/#bib.bibx323)\] effect in which imperfect representations of humans elicit revulsion in certain observers. This provides a toolkit for inspecting potentially 'weird' representations, especially if they are 'eerie' and is testable through Mori's GODSPEED questionnaire.
With an improved analysis of the shape of the likeability curve estimated later showing a more nuanced response from respondents where anthropomorphism of characters demonstrated increased likeability even against a human baseline \[[324](https://arxiv.org/html/2207.09460v11/#bib.bibx324), [325](https://arxiv.org/html/2207.09460v11/#bib.bibx325)\].
A mismatch in the human realism of face and voice also produces an Uncanny Valley response \[[326](https://arxiv.org/html/2207.09460v11/#bib.bibx326)\].
However, there is a possibility that Mori's hypothesis may be too simplistic for practical everyday use in CG and robotics research since anthropomorphism can be ascribed to many and interdependent features such as movement and content of interaction \[[325](https://arxiv.org/html/2207.09460v11/#bib.bibx325)\].
Bartneck et al. also performed tests which suggest that the original Uncanny Valley assertions may be incorrect, and that it may be inappropriate to map human responses to human simulacrum to such a simplistic scale. They suggest that the measure has been a convenient 'escape route' for researchers \[[325](https://arxiv.org/html/2207.09460v11/#bib.bibx325)\]. Their suggestion that the measure should not hold back the development of more realistic robots holds less bearing for the main thrust of this telepresence research which seeks to capture issues with imperfect video representation rather than test the validity of an approximation.
Interestingly Ho et al. performed tests on a variety of facial representations using images. They found that facial performance is a 'double edged sword' with realism being important to robotic representations, but there also being a significant Uncanny Valley effect around 'eerie, creepy, and strange' which can be avoided by good design \[[327](https://arxiv.org/html/2207.09460v11/#bib.bibx327)\].
More humanlike representations exhibiting higher realism produce more positive social interactions when subjective measures are used \[[262](https://arxiv.org/html/2207.09460v11/#bib.bibx262)\] but not when objective measures are used. This suggests that questionnaires may be more important when assessing potential uncanniness.
A far more objective method would be to measure user responses to humans, robots, and representations with functional near-infrared spectroscopy and while this has been attempted it is early exploratory research \[[328](https://arxiv.org/html/2207.09460v11/#bib.bibx328)\], an emotional response to 'eerie' was discovered.
- ##### Embodiment through robots
Virtuality human representation extends beyond simple displays into robotic embodiments (which need not be humanoid \[[329](https://arxiv.org/html/2207.09460v11/#bib.bibx329)\]), shape mapped projection dubbed "shader lamps\", and hybridisations of the two.
Robots which carry a videoconference style screen showing a head can add mobility and this extends the available cues \[[330](https://arxiv.org/html/2207.09460v11/#bib.bibx330), [331](https://arxiv.org/html/2207.09460v11/#bib.bibx331), [332](https://arxiv.org/html/2207.09460v11/#bib.bibx332), [333](https://arxiv.org/html/2207.09460v11/#bib.bibx333), [334](https://arxiv.org/html/2207.09460v11/#bib.bibx334)\]. Interestingly Desai and Uhlik maintain that the overriding modality should be high quality audio \[[335](https://arxiv.org/html/2207.09460v11/#bib.bibx335)\].
Tsui et al. asked 96 participants to rate how personal and interactive they found interfaces to be. Interestingly they rated videoconferencing as both more personal and more interactive than telepresence robots, suggesting that there is a problem with the overall representation or embodiment \[[336](https://arxiv.org/html/2207.09460v11/#bib.bibx336)\].
Kristoffersson et al. applied the Networked Minds questionnaire to judge presence of a telepresence robot for participants with little or no experience of videoconferencing. Their results were encouraging, though they identified that the acuity of the audio channel needing improvement \[[337](https://arxiv.org/html/2207.09460v11/#bib.bibx337)\].
There are a very few lifelike robots which can be used for telepresence, and even these are judged to be uncanny \[[338](https://arxiv.org/html/2207.09460v11/#bib.bibx338)\]. This is only an issue for a human likeness since anthropomorphic proxies such as robots and toys perform well \[[323](https://arxiv.org/html/2207.09460v11/#bib.bibx323)\].
- ##### Physical & Hybrid embodiment
Embodiment through hybridisation of real-time video and physical animatronic mannequins has been investigated as a way to bring the remote person into the space in a more convincing way \[[339](https://arxiv.org/html/2207.09460v11/#bib.bibx339), [340](https://arxiv.org/html/2207.09460v11/#bib.bibx340), [341](https://arxiv.org/html/2207.09460v11/#bib.bibx341)\].  These include telepresence robots \[[331](https://arxiv.org/html/2207.09460v11/#bib.bibx331), [338](https://arxiv.org/html/2207.09460v11/#bib.bibx338), [332](https://arxiv.org/html/2207.09460v11/#bib.bibx332)\], head in a jar implementations such as SphereAvatar \[[342](https://arxiv.org/html/2207.09460v11/#bib.bibx342), [343](https://arxiv.org/html/2207.09460v11/#bib.bibx343), [344](https://arxiv.org/html/2207.09460v11/#bib.bibx344)\] and BiReality \[[345](https://arxiv.org/html/2207.09460v11/#bib.bibx345)\],  UCL's Gaze Preserving Situated Multi-View Telepresence System \[[343](https://arxiv.org/html/2207.09460v11/#bib.bibx343)\], or screen on a stick style representations \[[334](https://arxiv.org/html/2207.09460v11/#bib.bibx334)\].
Nagendran et al. present a 3D continuum of these systems into which they suggest all such systems can be rated from artificial to real on the three axes, shape, intelligence, and appearance \[[346](https://arxiv.org/html/2207.09460v11/#bib.bibx346)\].
Itoh et al. describe a 'face robot' to convey captured human emotion over a distance. It uses an 'average face' and actuators to manipulate feature points \[[347](https://arxiv.org/html/2207.09460v11/#bib.bibx347)\]. It seems that this is an outlier method for communication of facial affect but demonstrates that there are many development paths to a more tangible human display.
It seems increasingly likely that machine learning models which manipulate images in real time can simulate humans into metaverse applications with very little input data. One such example is Samsung's Megaportraits which can product a realistic human face from a single input stream such as a webcam \[[348](https://arxiv.org/html/2207.09460v11/#bib.bibx348)\].
- ##### Shader lamps
Projection mapping is a computational augmented projection technique where consideration of the relative positions and angles of complex surfaces allows the projection from single or multiple sources to augment the physical shapes onto which they appear. It was first considered by the [Disney corporation in 1969](https://rabcup.com/the-history-of-3d-projection-mapping/) and was given prominence by Raskar and Fuchs with "office of the future\" \[[349](https://arxiv.org/html/2207.09460v11/#bib.bibx349)\] and later by Raskar and other researchers \[[341](https://arxiv.org/html/2207.09460v11/#bib.bibx341)\]. It has since gained considerable commercial popularity in live entertainment.
Shader lamps \[[341](https://arxiv.org/html/2207.09460v11/#bib.bibx341)\] is the more formal academic designation for projection mapping. It is possible to use the technique alongside reconstruction to project onto a white facial mannequin. Researchers have attempted to use the technology for remote patient diagnostic, projecting onto styrofoam heads \[[350](https://arxiv.org/html/2207.09460v11/#bib.bibx350)\].
Bandyopadhyay et al. demonstrated \[[351](https://arxiv.org/html/2207.09460v11/#bib.bibx351)\] that it is possible to track objects and projection map \[[352](https://arxiv.org/html/2207.09460v11/#bib.bibx352)\] onto them in real time. This is beyond the scope of the proposed projection onto furniture since we wish to keep the system as simple as possible, but could be useful for shared tasks in the future work.
Lincoln et al. employed animatronic avatars which they projected with shader lamps. This combination recreated facial expression and head movement though they were limited in speed and range of control of the remote head \[[340](https://arxiv.org/html/2207.09460v11/#bib.bibx340)\].
While shader lamps are an important and useful technology, there are limitations imposed by its use. In particular if a realtime video feed or reconstruction of a subject is used then that scanned subject must either remain still enough to be correctly mapped onto geometry on the remote side (useful for some virtual patients for instance \[[353](https://arxiv.org/html/2207.09460v11/#bib.bibx353)\], or else there must be a computational adjustment made for their changing position to make them appear static, or the projection surface must move to match their movement as in Lincoln et al.
- ##### Metaverse
In supporting business it's not clear that performance is improved or even maintained by the use of a metaverse. Xi et al. found a significant negative impact to productivity within metaverse applications \[[354](https://arxiv.org/html/2207.09460v11/#bib.bibx354)\]. It lowers productivity, and may increase anxiety, nausea, VR sickness and even migraines \[[355](https://arxiv.org/html/2207.09460v11/#bib.bibx355), [356](https://arxiv.org/html/2207.09460v11/#bib.bibx356)\]. It seems at this stage that if we are determined to explore metaverse for business then we [should mitigate](http://www.sigtrapgames.com/vrtp/) the problems as much as possible using the understanding we have gained so far. It might seem that in so doing there is no difference between immersive collaborative mixed reality (described above) and metaverse at all. We feel that the point of metaverse may be in [access to] , if not reliance upon, a mechanism for global truth. What we will go on to describe is likely to look more like traditional telecollaboration for small focussed teams, working on real-world problems, but we will always maintain an access to both the ability to scale, and a global register of value, trust, and truth (digital assets).
- ### 7.7 Theoretical Framework toward metaverse
- #### 7.7.1 Problem Statement
It's very likely that the 'social first' metaverse attemps such as [Meta Horizons](https://www.theverge.com/2022/10/6/23391895/meta-facebook-horizon-worlds-vr-social-network-too-buggy-leaked-memo), Sandbox, and Decentraland are [failing](https://www.coindesk.com/web3/2022/10/07/its-lonely-in-the-metaverse-decentralands-38-daily-active-users-in-a-13b-ecosystem/) to capture audiences. They will likely crash back down the hype curve as 'Second Life' did before them. Games based worlds such as Roblox are fairing better, but it's unclear if they have any longevity, and they do not fulfil ambitions of an open metaverse.
Worse yet it seems that metaverse is not the most useful way to conduct business. It is evident that there are multiple factors which contribute to successful human-human communication. These factors remain important in telecommunication supported by technology, and are variously supported, unsupported, or modified by particular technologies. Third person large scale metaverse are clearly amongst the worse of the solutions.
Of particular importance is interpersonal gaze \[[223](https://arxiv.org/html/2207.09460v11/#bib.bibx223), [198](https://arxiv.org/html/2207.09460v11/#bib.bibx198), [224](https://arxiv.org/html/2207.09460v11/#bib.bibx224)\]. Non-verbal cues are also important across multiple modalities of sight, sound \[[200](https://arxiv.org/html/2207.09460v11/#bib.bibx200)\], and position of interlocutors \[[207](https://arxiv.org/html/2207.09460v11/#bib.bibx207)\], extending to the whole body \[[198](https://arxiv.org/html/2207.09460v11/#bib.bibx198), [236](https://arxiv.org/html/2207.09460v11/#bib.bibx236)\].
While formal meeting paradigms are pretty well supported by commercially deployed systems, such ICT can be expensive, may need to be professionally managed, and high end equipment in board rooms are generally booked well in advance. These meetings seem to demand many smaller supporting meetings between parties or groups of parties. The pressure here is clearly toward the now ubiquitous Teams and Zoom style formats, and these offer very poor support for social cues, and incur additional fatigue. These are known and well researched problems, and is is possible that the [strategic pairing of Meta Horizons and Microsoft Teams](https://www.oculus.com/blog/future-of-work-meta-connect-2022-quest-pro-microsoft-accenture/?) will succeed where previous attempted have failed. They seem to finally have the right assets and opportunity.
The 'problem' is a supporting technology for small less formal groups, or ad-hoc groups meeting to add clarity or context to formal meetings. Metaverse allows this kind of interaction, while not seeming to replace formal meeting utility. Metaverse also may connect home and work spaces without bringing in those backgrouds, creating a level playing field. A more advanced metaverse interface could also allow dynamism and movement, connection of natural non vocal cues, without too much encumbering technology overhead.
- #### 7.7.2 Core Assumptions
Figure [7.5](https://arxiv.org/html/2207.09460v11/#Ch7.F5 "Figure 7.5 ‣ 7.7.2 Core Assumptions ‣ 7.7 Theoretical Framework toward metaverse ‣ Chapter 7 Collaborative mixed reality ‣ Part I State of the art") shows the interlocking relationships between baseline communication where the participants are present, and technology which attempts to support across distance.
![Figure 7.5: The Venn diagram shows areas of research which have been identified in blue. These interlock and overlap as shown. The most relevant identified researchers from the literature are shown in black close to the fields of study which they represent. This diagram is a view of the core assumptions for the research, with the most important fields at the centre.](../assets/frameworkVenn.png) 
Of most interest to this research is the centre of the Venn where meeting styles which are less formal, and perhaps dynamic, may occur. Looking at these items one by one gives us our core assumptions.
1.  [1.]      Gaze is broadly agreed to be highly important for mediating flow.     Mutual gaze is a rich emotional channel. The research must consider     gaze. All of the researchers listed around the Venn have at some     point engaged with this topic. 2.  [2.]      The non-verbal communication channel employed in 'attention' is     assumed based upon the literature to be critical to smoothly leaving     and entering a fast flowing conversation where concentration around     a defined problem may be high (gesturing to a chair for instance).     Again, all of the listed researchers have made reference to     attention in their work. 3.  [3.]      Support for spatiality is important in a group setting so that     directional non-verbal cues can find their target. The topic of     spatial relationships between interlocutors cuts across all of the     researchers, but this is not true of immersion. Immersion in a     shared virtuality can certainly support the underlying requirements     spatial, but the technical infrastructure required is out of scope     (so this is struck through on the diagram). Roberts and Steed are     the main expertise referenced even though this element is not     expanded in the research. 4.  [4.]      Situated displays are those which are appropriate for their     surrounding context, in this case the informal meeting. Roberts,     Pan, Steed and Steptoe seem the most relevant researchers in these     technology spaces. 5.  [5.]      Based on the literature proxemics is believed to be relevant in a     meeting where subgroups can be instantiated and destroyed as the     meeting evolves, and those where people can be invited in from     outside the physical bounds of the meeting (informal spaces). Hall     is the best source for this work. If it is assumed that people may     come and go, and subgroups may be convened then Sermon and Benford     are the best references through their work blending real and virtual     spaces. This may be more consistent with less organised meetings     such as those convened on demand (ad-hoc).
- #### 7.7.3 Peripheral Assumptions
Surrounding the centre of the Venn are additional relevant topics from social science branches of theory
[From verbal communication] 
It is assumed that the directionality of sound is important \[[195](https://arxiv.org/html/2207.09460v11/#bib.bibx195)\], and this will be engineered into the experimental design. It is assumed that movement of the lips is an indicator and this is tied to latency and frame rate in the vision system.
[From non-verbal communication] 
It is assumed that eye gaze is of high importance, and that this information channel is supported by head gaze and body torque to a high degree. It is further assumed that mutual eye gaze is of less relevance in a multi party meeting where there is a common focus for attention but can be significant for turn passing. It is assumed that upper body framing and support for transmission of micro and macro gesturing is important for signaling attention in the broader group, and for message passing in subgroups.
[Now that we have an idea what's important for business social communication we can look at the available software to find a best fit.] 
- ### 7.8 Post 'Meta' metaverse
The current media around "metaverse" has been seeded by Mark Zuckerberg's rebranding of his Facebook company to 'Meta', and his planned investment in the technology. Kraus et al suggest that this seems more a marketing and communication drive than a true shift in the company business model \[[357](https://arxiv.org/html/2207.09460v11/#bib.bibx357)\], but despite this Park and Kim identify dozens of recent papers of metaverse research emerging from Meta labs \[[159](https://arxiv.org/html/2207.09460v11/#bib.bibx159)\].
In Stephenson's 'Snow Crash' the Hero Protagonist (drolly called Hiro Protagonist) spends much of the novel in a dystopian virtual environment called the metaverse. It is unclear if Facebook is deliberately embracing the irony of aping such a dystopian image, but certainly their known predisposition for corporate surveillance, alongside their attempt at a global digital money is [ringing alarm bells](https://www.politico.com/newsletters/digital-future-daily/2022/04/12/the-facebook-whistleblower-takes-on-the-metaverse-00024762), as is their [current plan](https://www.cnet.com/personal-finance/metas-new-47-5-fee-on-metaverse-items-has-nft-twitter-pissed/) for monetisation.
The second order hype is likely a [speculative play](https://www.goldmansachs.com/insights/pages/framing-the-future-of-web-3.0-metaverse-edition.html) by major companies on the future of the internet. Grayscale investment [published a report](https://grayscale.com/wp-content/uploads/2021/11/Grayscale_Metaverse_Report_Nov2021.pdf) which views Metaverse as a potential trillion dollar global industry. Such industry reports are given to hyperbole, but it seems the technology is becoming the focus of technology investment narratives. Some notable exerts from a [2021 report](https://www.jpmorgan.com/content/dam/jpm/treasury-services/documents/opportunities-in-the-metaverse.pdf) by American bank JPMorgan show how the legacy financial institutions see this opportunity:
    In the view of the report ["The metaverse is a seamless convergence     of our physical and digital lives, creating a unified, virtual     community where we can work, play, relax, transact, and     socialize." - this isn't the worst definition, and very much plays     into both the value and mixed reality themes explored in this     book.]      They agree with the industry that monetisation of assets in     metaverse applications is called "Metanomics". It's worth seeing     this word once, as it's clearly gaining traction, but it won't be     used in this book.     They make a point which is at the core of this book, that value     transaction within metaverses may remove effective border controls     for working globally. Be this teleoperation of robots, education, or     shop fronts in a completely immersive VR world. They say: ["One of     the great possibilities of the metaverse is that it will massively     expand access to the marketplace for consumers from emerging and     frontier economies. The internet has already unlocked access to     goods and services that were previously out of reach. Now, workers     in low-income countries, for example, may be able to get jobs in     western companies without having to     emigrate."]      There is a passage which foreshadows some of the choices made in     this book: ["Expanded data analytics and reporting for virtual     spaces. These will be specifically designated for commercial and     marketing usage and will track business key performance indicators     (this already exists in some worlds, such as     Cryptovoxels)"] . More on this later.     The report attempts to explore the web3 & cryptocurrency angles of     metaverse. That's also the aim of this book, but they have taken a     much more constrained approach, ignoring the possibilities within     Bitcoin.     They assert that strong regulatory capture, identification, KYC/AML     etc should underpin their vision of the metaverse. This is far from     the community driven and organically emergent narratives that     underpin Web3. This is their corporate viewpoint, something they     have to say. On the back of this they pitch their consultancy     services in these areas.
There has been a reactive pushback against commercialisation and corporateisation by the wider tech community, who are [concerned about](https://www.metaversethics.org/p/mde02-metaverse-data-privacy-1) the aforementioned monetisation of biometrics. [Observers do not trust](https://www.coindesk.com/layer2/2022/01/19/meta-leans-in-to-tracking-your-emotions-in-the-metaverse/) these 'web' players with such a potentially powerful social medium. It is very plausible that this is all just a marketing play that goes nowhere and fizzles out. It is by no means clear that people want to spend time socialising globally in virtual and mixed reality. These major companies are making an asymmetric bet that if there is a move into virtual worlds, then they need to be stakeholders in the gatekeeping capabilities of those worlds.
To paraphrase Olson; the salesmen peddling the inevitability of the metaverse are stuck clinging to aesthetic details because, without them, they're just talking about the internet. While virtual reality is enjoying hype right now, and will continue to develop, it faces significant challenges related to the human body's physiological limitations. For instance, the inner ear can become disoriented when a user experiences virtual movement without physically moving. This issue has led to the development of VR applications that require compromises between immersion and physical comfort.
- ### 7.9 Market analysis
The market penetration analysis for VR which rings most true for us is provided by Thrive Analytics, and ARtillery Intelligence. Their report is titled "[VR Usage & Consumer Attitudes, Wave VI](https://artilleryiq.com/reports/vr-usage-consumer-attitudes-wave-vi/)". In the USA (which is the cohort they surveyed) they found that adoption of VR headsets is slower than predicted (their work is longitudinal), but steady. Some highlight points are:
    23 percent of U.S. adults own or [have used]      VR technology. This is around 4% up from the previous survey     in 2020. Frustratingly, and very much in keeping with such industry     surveys they conflate 'own' with 'have used' making this data pretty     meaningless from an adoption point of view.     there is a skew toward male users of around 10%, and a far larger     skew toward younger users, and a bias toward richer households.     These are indicative of a technology that's still early in it's     adoption cycle.     Of the owners of the technology (no indication what percentage this     is) they found that around a third used the equipment regularly, but     that this retention number was gently falling.     Standalone headsets (Quest 2 and Pico 4) without a cabled connection     to a computer are far more popular, and have better user retention.     This makes sense as the alternative demands either space or setup     time.     Buyers of these more popular headsets are very sensitive to price.     Note here that Meta is selling Quest2 at a loss to drive the market.     This is unsustainable.     Overall this snapshot of adoption feels pretty neutral, and is being     driven by losses to Facebook/Meta share price.
Deloitte have just [conducted a UK survey](https://www2.deloitte.com/uk/en/pages/technology-media-and-telecommunications/articles/digital-consumer-trends-2022-metaverse.html). This covers "metaverse, virtual reality, and web3 (i.e. blockchain-based assets like Bitcoin", and so is perfect for our needs. They have similar results to the bigger US survey. Their key finding are quoted below verbatim:
    63% of respondents have heard of the term "metaverse". However,     roughly half of those know nothing about it.     Only 18% of VR headsets are used daily, from the 8% of individuals     that claim to have access to one.     Consumers may be wary of web 3. While most people (93%) have heard     of cryptocurrency, only one in five (19%) know at least a "fair     amount" about it. Knowledge of NFTs is rarer still.     70% of those who have heard of these assets say they are unlikely to     buy them in the next, and cite fraud, scams and a lack of regulation     as key concerns.
Deloitte feel that "content is key" for virtual reality to be a success, but we would instead argue that applications are key. Nearly half of their respondents were simply "not interested in VR". We think this matches our longstanding understanding of the reality of the market. A few vocal proponents of the technology does not necessarily lead to a developed and mature mass appeal. Again, we feel that real world use cases will drive adoption over a longer time frame. Virtual meetings do not feel like that application to us.
They feel that 'one metaverse' would require blockchain/web3 tooling for a common consensus frame, and we agree with this. It seems like a very long way to that point, and perhaps not worth the effort. They, like us, see compatible silos as being the interim step.
They (unusually) have a legal opinion in the text, and this is valuable enough to quote verbatim once again. ["The metaverse amplifies existing legal issues and raises new ones. Centralised metaverses, such as those focused on games, tend to engage consumers in a controlled space and operate within familiar legal frameworks. For example, users purchasing a virtual accessory are likely to understand its use will be within tightly prescribed parameters. Decentralised metaverses, which incorporate web3 (such as NFTs) are more challenging, as users may expect virtual assets to be portable. However, those assets are governed by inconsistent and often unclear terms, and the lack of technical standards can result in limited interoperability between metaverses. For the user, social interactions in virtual worlds can feel realistic, inviting scrutiny from policymakers and regulators focused on online safety. An increased legislative focus on children online will also require platforms to assess or verify the age of users. And collection of personal data -- such as eye movement within a VR headset -- will require informed consent under data protection laws, and a clear understanding of who is controlling that data at any given time. Finally, as content is key, clear contractual parameters are required to frame how intellectual property is used, whether user-generated content is permitted, and how illegal/harmful content is managed. Amid all of this, metaverse builders, content owners and brands must ensure they have a risk assessment and risk management framework in place to avoid costly mistakes, both reputational and financial, in an increasingly regulated space."] 
[The Drum](https://www.thedrum.com/about-us) is a market awareness website and [compiled](https://www.thedrum.com/news/2022/08/01/web3-the-numbers-key-metaverse-crypto-and-nft-stats-every-marketer-should-know) the following statistics, which have been linked back to their source and annotated for our needs.
    [89.4 million Americans are expected to use virtual reality (VR) in     2022, ] [according to     insiderintelligence](https://www.insiderintelligence.com/content/us-augmented-virtual-reality-users-forecast-2022)[.     That number, according to the same source, is expected to climb to     110.3 million in 2025. As a counter to this only around 16M VR     headsets were sold in 2022]      [51% of gen Z and 48% of millennials envision doing some of their     work in the metaverse in the next two years, according to     Microsoft's Work Trend Index 2022.]      [38% of respondents said they would "try extreme sports like     skydiving, bungee jumping, or paragliding" in the metaverse     according to a recent Statista survey called 'What things would you     do in the metaverse but never in real life?' Unsettlingly, 18% of     respondents said they would "conduct unethical experiments on     virtual humans"]      [87% of Americans between the ages of 13-56 would be interested in     engaging with a virtual experience in the metaverse "that is built     around a celebrity they love," according to new research from UTA     and Vox Media]      [\$678bn is forecasted to be the total market valuation of the     metaverse by 2030, per Grand View Research. According to the report,     that market value was just shy of]      [\$39bn in 2021, giving it a predicted compounded annual growth rate     over a 10-year period of around 39]      [46% of all people across age groups say that the ability to     visualize a virtual product in an IRL context -- "such as seeing a     digital painting in their home using augmented reality (AR) glasses"     -- is the primary factor that would motivate them to make a purchase     in the metaverse, per a Productsup survey]      [24% of US adult internet users say "that lower-priced VR headsets     were a very important factor when deciding whether to try using the     metaverse," per a recent Statista survey. On the other hand, 54% say     that their workplace using the metaverse would "not \[be\] important     at all" in their decision to give the metaverse a     try]      [15% of gen Zs' "fun budget" is spent in the metaverse, per a report     from Razorfish and Vice Media Group. In five years that number is     projected to climb to 20%]      [Nearly 77% believe that the metaverse "can cause serious harm to     modern society," per a recent survey from customer service platform     Tidio. The survey, which received feedback from 1,000 participants,     identified three major causes of anxiety related to the metaverse     and its potentially negative social impacts: "addiction to a     simulated reality" was the number one concern, followed by "privacy     issues" and "mental health issues," which were tied for     second]      [By 2026, about 2 billion people worldwide "will spend at least one     hour a day in the metaverse to work, shop, attend school, socialize     or consume entertainment," per McCann Worldgroup. By that same year,     the total value of the virtual goods market in the metaverse could     be as high as \$200bn]      [NFTs Over \$37bn has been spent in NFT marketplaces as of May 2022,     per data from Chainalysis. At their current rate, this year's NFT     sales could potentially surpass last year's, which had a total     valuation of around \$40bn, according to the     data]      [\$91.8m was the sale price of 'The Merge,' the most valuable NFT to     date. Created by the artist Pak, it sold for its record-breaking     value in December 2021]      [64% of sports fans are open to the idea of learning more about NFTs     and would consider purchasing one in the future, according to the     National Research Group. The report also found that 46% of sports     fans "would be more likely to attend live sporting events if they     were rewarded with a commemorative NFT -- for example, if their     ticket turned into a digital collectible after the     game"]      [Only 9% of people aged 16-44 own a NFT, and less than half (44%)     have purchased or invested in crypto, per a new survey from agency     SCS. On the other hand, among the survey's 600 respondents, 64% were     "aware" of the metaverse, and 65% of that subgroup say they are     "interested in exploring it further for everything from traveling to     new places and playing games to making money and     shopping"] 
Polling company IPSOS [have conducted](https://www.ipsos.com/en/global-advisor-metaverse-extended-reality-may-2022) a global survey for the World Economic Forum. Some highlights are:
    "Excitement about extended reality is significantly higher in     emerging countries than it is in most high-income countries. In     China, India, Peru, Saudi Arabia, and Colombia, more than two-thirds     say they have positive feelings about the possibility of engaging     with it."     "Familiarity and favorability toward the new technologies are also     significantly higher among younger adults, those with a higher level     of education, and men than they are among older adults, those     without a college-level education, and women."
Excitingly for our exploration of the topic it can be seen in Figure [7.6](https://arxiv.org/html/2207.09460v11/#Ch7.F6 "Figure 7.6 ‣ 7.9 Market analysis ‣ Chapter 7 Collaborative mixed reality ‣ Part I State of the art") that education within metaverse spaces is the most anticipated application, and we have seen that the emerging globals markets are the most optimistic about the technology overall. This is highly suggestive of an opportunity.
![Figure 7.6: [IPSOS poll predicted applications](https://www.ipsos.com/en/global-advisor-metaverse-extended-reality-may-2022)](../assets/applications.png) 
- ### 7.10 NFT and crypto as metaverse
Within the NFT, Web3 and crypto community it is normalised to refer to ownership of digital tokens as participation in a metaverse. This is reflected in the market analysis above. This fusing of narratives is reviewed in detail by Gadekallu et al in their excellent recent paper on Metaverse and Blockchain \[[358](https://arxiv.org/html/2207.09460v11/#bib.bibx358)\]. They conclude that much remains to be done here. This CNBC article highlights the confusion, as this major news outlet refers to [Walmart prepares to offer NFTs](https://www.cnbc.com/2022/01/16/walmart-is-quietly-preparing-to-enter-the-metaverse.html)" as an entry "into the metaverse".
- ### 7.11 Lessons from MMORGS
The concept of 'instrumental play' was introduced by literary theorist Wolfgang Iser in his 1993 essay "The Fictive and the Imaginary" \[[359](https://arxiv.org/html/2207.09460v11/#bib.bibx359)\]. Iser divided play into two categories, free play and instrumental play, based on their relationship to goals. In his view, play becomes instrumental the moment it has a goal or a set of rules. The application of this concept to massively multiplayer online games was later explored by sociologist T.L Taylor in her 2006 book 'Play Between Worlds' \[[360](https://arxiv.org/html/2207.09460v11/#bib.bibx360)\]. According to Taylor, instrumental play is a goal-oriented approach that values efficiency, expertise, and strategy optimization. The point of playing is not to reach the end but to find the best way to get there.
The distinction between instrumental play and fun is often seen as a false dichotomy. The two are not mutually exclusive but exist in tension. Optimization can result in player behaviours that are simply no fun, but achieving goals or improving skills can also bring enjoyment. René Glas in his book 'Battlefields of Negotiation' \[[361](https://arxiv.org/html/2207.09460v11/#bib.bibx361)\] describes the movement between instrumental and free play in World of Warcraft, which has the distinction of evolving across entirely different iterations of the Internet.
These virtual worlds of massively multiplayer online games are \"interactively stabilized\" systems, the result of the interaction between game designers and players. The social codes of practice established by players can shape what is considered legitimate play. Success in these games is dynamically defined by consensus, as seen in Mark Chen's study of World of Warcraft 'Leet Noobs' \[[362](https://arxiv.org/html/2207.09460v11/#bib.bibx362)\].
Tom Boellstorff conducted a study of user experiences in Second Life \[[363](https://arxiv.org/html/2207.09460v11/#bib.bibx363)\], which was criticized for not involving real life or other websites or software in the analysis. The virtual worlds of massively multiplayer online games are not enclosed and players can engage with these games through various platforms, such as Discord, Twitch, Twitter, and Google Docs, without physically inhabiting the virtual world. This concept of \"paratext\" was first introduced by French literary theorist Gerard Genette. He saw a book as containing the text of the book and additional components, such as the cover, title, foreword, etc., that are necessary to complete the book but not part of the primary text. These additional texts influence the meaning of the primary text. The definition was later expanded by Mia Consalvo, who defined paratext as any text that "may alter the meanings of a text, further enhance meanings, or provide challenges to sedimented meanings." Examples of paratext include reviews, pre-release trailers, etc. Kristine Ask observed the impact of paratext on theorycrafting expertise in World of Warcraft, which was later confirmed by the rise of twitch streams. Mark Chen's dissertation Leet Noobs focuses on how AddOns in World of Warcraft can become essential agents in raid groups by assuming cognitive load. The concept is based on the idea of object-oriented ontology and actor-network theory \[[364](https://arxiv.org/html/2207.09460v11/#bib.bibx364)\]. These theories are complex and contested, but the boundaries between real people and virtual AI actors in virtual social spaces are certainly blurred.
Virtual spaces are not separate from the real world, but are instead an extension of it. The key factor in making a virtual world compelling is not its realism, but the fact that people give meaning to their lives by entangling themselves in projects with others, even when those others are not other people. Worlds become real when people care about them, not when they look like the real world.
- ### 7.12 Immersive and third person XR
In considering the needs of business to business and business to client social VR is it useful to compare software platforms. We have seen that a global connected multiverse is a marketing proposition only, and may be a decade or more away. Contenders currently look more like one of three catagories; games, limited massively multiplayer worlds, or meeting support software. These will converge.
- #### 7.12.1 More like a digital twin
One of the most intuitive ways to view a metaverse is as a virtual landscape. This is how metaverse was portrayed in the original Neal Stephenson use of the word. 'Digital twin' is another much abused industry term which trends toward a 3D representation of real world spaces and objects. Sometimes these virtual objects are connected to the real by telemetry, allowing industrial monitoring applications. Much is made of such systems in simulation brochures, and on the web, but it's surprisingly hard to find real world applications of the idea outside of complex large scale systems engineering (aerospace). The costs of maintenance are simply too high. The US army owns the digital twin which could be called [closest to "The Metaverse"](https://www.army.mil/standto/archive/2018/03/26/) (note the intentional capitalisation). Their global simulation environment mirrors real world locations for their training needs. The European space agency is building an Earth digital twin for climate research, as [is Nvidia](https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41326/?playlistId=playList-9bb5405e-3e40-4ff3-88db-61cd3a4507e5#:~:text=Earth%2D2%20aims%20to%20improve,learning%20methods%20at%20unprecedented%20scale.), but again it's unclear what this offers over and above access to direct data feeds, and of course such an ambitious project likely has an ecological cost!
Within industry digital twins are seen as the primary use case for metaverse, with even the world economic forum [subscribing to the hype](https://www.weforum.org/agenda/2023/01/metaverse-biggest-impact-industry-davos2023/). To be clear, there is enormous effort, investment, and potential here, but it feels outside of the scope of this product at this time.
- ##### Omniverse
Key new capabilities announced: Integration of generative AI like Adobe Firefly to enhance creation workflows (wow!) Expanded ecosystem connections through OpenUSD (Adobe, Wonder Dynamics, Luma AI, etc) New developer tools and templates for building apps and experiences Semantic search capability with DeepSearch to find 3D assets easily Optimizations for photorealistic real-time rendering and path tracing with AI-accelerated denoising powered by new RTX GPUs XR capabilities native to the platform (so you can deploy on AR/VR headsets) Upgrades to core apps like Omniverse Audio2Face and USD Composer Graphics Delivery Network (GDN) to performantly serve your 3D experience around the world Support for new workflows across industrial use cases like digital twins
- ##### Geolocated AR
Overlaying geospecific data into augmented reality (think Pokemon Go) is probably the ultimate utility of digital twin datasets. It's such a compelling application space that we will have more on this later.
- #### 7.12.2 More like a metaverse
- ##### Second Life
Notable because it's the original and has a decently mature marketplace. Some \$80M was [paid to creators](https://www.zdnet.com/article/high-fidelity-invests-in-second-life-to-expand-virtual-world/) in Second Life in 2021 in a wider economic ecosystem of around \$650M. It's possible to write a whole book on Second life, and indeed many have. It's longevity means that there's more study of business uses of such systems than in any other platform.
- ##### Mozilla Hubs
Hubs is a great option for this proposal, and might be worth integrating later. It runs well in a browser and on VR hardware.
    Open source, bigger scale, more complex     Choose avatars, or import your own     Environments are provided, or can be designed     Useful for larger conferences with hundreds or thousands of members     but is commensurately more complex     Larger scenes within scenes
- ##### Counter social realms
A relatively new platform linked to a new model of social media which excludes countries which habitually spam. It uses Mozilla Hubs for it's engine.
- ##### Roblox
If anything can currently claim to be the metaverse it's probably Roblox. Around 60 billion messages are [sent daily](https://podcasts.apple.com/us/podcast/developments-investments-experiences-in-the-metaverse/id1593908027?i=1000540906629) in Roblox. Investment in the metaverse 'angle' of the platform is stepping up with recent announcements such as ["Spotify Island"](https://techcrunch.com/2022/05/03/spotify-becomes-first-music-streamer-to-launch-on-roblox/?). The company has announced text based generative art creation of scenes, and is integrating the playstation headset in their PS4 release. It's very notable that it still [hasn't become a profitable business](https://fortune.com/2022/06/03/roblox-gaming-ecosystem-metaverse-stocks-profit/). It is important to note that Roblox has banned NFTs. Nike have [garnered significant attention](https://www.thedrum.com/news/2022/09/22/21m-people-have-now-visited-nike-s-roblox-store-here-s-how-do-metaverse-commerce) for their metaverse store, front with their Roblox based metaverse. As [Theo Priestley](https://medium.com/@theo/why-nikeland-is-not-the-metaverse-success-story-you-think-it-is-46742dc2f231) points out this is likely just another expensive experiment, with a finite lifespan.\
- ##### Minecraft
- ##### Surreal
- ##### Sansar
- ##### Cornerstone
- ##### AltSpace
    Microsoft social meeting platform     Very good custom avatar design     Great world building editor in the engine     Doesn't really support business integration so it's a bit out of     scope     Huge numbers (many thousands) possible so it's great for global     events
- ##### VRChat
This text is from wikipedia and will be updated when we have a chance to try VRChat properly. It's much loved already by the Bitcoin community.
"VRChat's gameplay is similar to that of games such as Second Life and Habbo Hotel. Players can create their own instanced worlds in which they can interact with each other through virtual avatars. A software development kit for Unity released alongside the game gives players the ability to create or import character models to be used in the platform, as well as build their own worlds.
Player models are capable of supporting \"audio lip sync, eye tracking and blinking, and complete range of motion.
VRChat is also capable of running in \"desktop mode\" without a VR headset, which is controlled using either a mouse and keyboard, or a gamepad. Some content has limitations in desktop mode, such as the inability to freely move an avatar's limbs, or perform interactions that require more than one hand.
In 2020, a new visual programming language was introduced known as \"Udon\", which uses a node graph system. While still considered alpha software, it became usable on publicly-accessible worlds beginning in April 2020. A third-party compiler known as \"UdonSharp\" was developed to allow world scripts to be written in C sharp."
- ##### Meta Horizon Worlds & Workrooms
Horizon Worlds is the Meta (Facebook) meteverse, and Workrooms it's business offering and a subset of the "Worlds" global system. It is currently a walled garden without connection to the outside digital world, and arguably not therefore a metaverse.
The Financial Times took a look at their patent applications and noted that the travel is toward increased user behaviour tracking, and targeted advertising.
Facebook actually have a poor history on innovation and diversification of their business model. This model has previously been tracking users to target ads on their platform, while increasing and maintaining attention using machine learning algorithms.
It makes complete sense then to analyse the move by Meta into 3D social spaces as an attempt to front run the technology using their huge investment capacity. Facebook have recently taken a huge hit to their share price. Nothing seems to have changed in the underling business except Zuckerberg's well publicised shift to supporting a money losing gamble on the Metaverse. It is by no means clear that users want this, that Meta will be able to better target ads on this new platform, or that the markets are willing to trust Zuckerburg on this proactive move.
With all this said the investment and management capacity and capability at Meta cannot be dismissed. It is very likely that Meta will be able to rapidly deploy a 3D social space, and that it's development will continue to be strong for years. The main interface for Horizon Worlds is through the Meta owned and developer Oculus headset, which is excellent and reasonably affordable. It has been quite poorly received [by reviewers](https://kotaku.com/facebook-metaverse-horizon-worlds-vr-oculus-quest-2-cha-1848436740) but will likely improve, especially if users are encouraged to innovate.
- ##### Webaverse
[Webaverse](https://webaverse.com/) are an open collective using open source tools to create interoperable metaverses.
- ##### Vircadia
The applications and platforms detailed above have their benefits, but for the application stack in the next section of the book Vircadia has been chosen. The following text is from their website, and is a placeholder which gives some idea. This section will be written out completely to reflect our use of the product.
Vircadia is open-source software which enables you to create and share virtual worlds as virtual reality (VR) and desktop experiences. You can create and host your own virtual world, explore other worlds, meet and connect with other users, attend or host live VR events, and much more.
The Vircadia metaverse provides built-in social features, including avatar interactions, spatialized audio, and interactive physics. Additionally, you have the ability to import any 3D object into your virtual environment. No matter where you go in Vircadia, you will always be able to interact with your environment, engage with your friends, and listen to conversations just like you would in real life.
What can I do? You have the power to shape your VR experience in Vircadia.
    EXPLORE by hopping between domains in the metaverse, attend events,     and check out what others are up to!     CREATE personal experiences by building avatars, domains, tablet     apps, and more for you and others to enjoy.     SCRIPT and express your creativity by applying advanced scripting     concepts to entities and avatars in the metaverse.     HOST and make immersive experiences to educate, entertain, and     connect with your audience.     CONTRIBUTE to the project's endeavor.     DEVELOP the project and tailor it to your needs, or just to help     out.     SECURITY information about the project and its components.
- #### 7.12.3 More like crypto NFT virtual land
- ##### Decentraland
Decentraland is a large 3D (but not VR) space developed by Argentine developers Esteban Ordano and Ari Meilich. It is a decentralized metaverse purporting to be owned by its users, but actually owned completely by a foundation [based in Panama](https://www.crunchbase.com/organization/decentraland/people). The users can shop, buy things, invest, and purchase goods in a virtual space. The project is built on Ethereum and has a (speculative) valuation in the billions of dollars.
Decentraland was launched in February 2020, and its history includes an initial coin offering in August 2017, where their MANA token sale raised approximately \$24 million dollars in crypto coins. This was followed by a "terraforming event" where parcels of land, denominated in LAND tokens, were auctioned off for an additional \$28 million in crypto. The initial pitch for Decentraland emphasized the opportunity to own the virtual world, create, develop, and trade without limits, make genuine connections, and earn real money. However, the actual experience in Decentraland has faced criticisms such as poor graphics, performance issues, and limited content. They have recently dropped their pretence of ever supporting VR.
One example of these limitations is the now-defunct pizza kiosk that aimed to facilitate ordering Domino's pizza via the metaverse using cryptocurrency. This concept, though intriguing, was hindered by a lack of official support from Domino's and the inherent inefficiencies of using a virtual world as an intermediary for purchasing goods and services.
Similarly, attempts to create virtual amusement park rides and attractions within Decentraland have suffered from poor performance and a lack of interactivity. These issues stem from the limitations of the tools and resources available for building experiences within the platform, as well as the inherent difficulties in creating engaging experiences in a 'world' that is supposed to perform too many functions at once.
In addition to the technical challenges, Decentraland (and all these crypto metaverse projects) have clearly promoting unrealistic expectations to foster speculative investments. The notion that businesses and individuals will eventually "live inside" the metaverse is not only a poetic interpretation but also an unrealistic expectation given the current state of VR technology.
As it stands, Decentraland is unlikely realize its supposed potential as an invisible, seamless infrastructure for a wide range of digital experiences. Until the platform can address its core issues, it is likely that projects like the 'Decentraland Report' (it's user delivered news platform), and others will continue to fail to deliver on their promises. To quote [Olson's highly critical](https://www.youtube.com/watch?v=EiZhdpLXZ8Q) (and correct) presentation on Decentraland:\ textit"..it can't even handle properly emulating Breakout, a game from 1976 that you can play on goddamn Google images! Steve Wozniak built Breakout fifty years ago to run on 44 TTL chips and a ham sandwich and that's still somehow too demanding a gaming experience ..."
Like all of these attempts the actual information content of within Decentraland boils down to text on billboards, and links to the outside Web. It's a terrible product, and really just another example of a crypto scam which never really intended to be developed for the long haul.
- ##### Sandbox
The Sandbox, a decentralized gaming platform built on the Ethereum blockchain, has garnered attention for its promise of a vibrant ecosystem filled with user-generated content. However, despite its ambitious vision, the project has faced various challenges and criticisms similar to Decentraland. Limited use cases and adoption remain a significant challenge for The Sandbox. While the platform aims to create a vast and engaging gaming ecosystem, it has yet to gain widespread adoption, leading to a limited number of users and developers. This lack of user engagement raises questions about the long-term viability of the project, as the value of virtual land, assets, and in-game experiences may remain limited without a thriving community. Like Decentraland it is a manipulated hype bubble, attracting glowing paid press reports in some media, and 'interest' from national and regional 'branches' of global brands which are then spun to create artificial hype in main stream media. The tradable NFTs within these early platforms are obviously subject to insider trading, price volatility, wash trading, and other harmful activities.
The Sandbox places too much emphasis on the speculative aspect of virtual land and asset trading, rather than focusing on creating a genuinely engaging gaming ecosystem. This focus on speculation could lead to an unsustainable bubble with inflated asset prices, and it seems likely we have already seen most of the collapse of this ecosystem.
The actual experience of interacting with The Sandbox's gaming products leaves much to be desired. For instance, the platform's games may suffer from lag and poor performance due to the technical limitations of blockchain technology. Additionally, the quality of user-generated content can be highly variable, as not all creators possess the skills and resources to develop engaging gaming experiences. As a result, users might find themselves sifting through a plethora of low-quality games, which can be frustrating and time-consuming.
Concerns about centralization persist, as some critics argue that the project is not entirely decentralized. The team behind The Sandbox still holds a significant amount of control over the platform's development and governance, potentially undermining the project's core vision of a decentralized gaming ecosystem.
- ##### Space Somnium
Somnium Space is just another one of these, but with more VR. It allows users to join in either through a downloadable VR client or a browser-based version to function like any other web app. It suffered the same problems at Decentraland and Sandbox. They are terrible products, with hype, manufactured by money, extracted from users, often convinced by paid celebrity endorsements. It's the NFT space, but sadder, and technically worse, and likely not for very much longer.
- #### 7.12.4 More like industrial application
As the word metaverse has gained in use, so have some traditional users and researchers in mixed reality switched to use of the term. Siyaev and Jo describe an aircraft training metaverse which incorporates ML based speech recognition \[[365](https://arxiv.org/html/2207.09460v11/#bib.bibx365)\]. This class of mixed reality trainer traditionally finds positive results, but is highly task specific.
- ##### Global enterprise perspective
Microsoft have just bought Activision / Blizzard for around seventy billion dollars. This has been communicated by Microsoft executives as a "Metaverse play", leveraging their internal game item markets, and their massive multiplayer game worlds to build toward a closed metaverse experience like the one Meta is planning. This builds on the success of early experiments like the Fornite based music concerts, which attracted millions of concurrent users to live events.
There are three emerging focuses, the social metaverses for pleasure, and business metaverses for larger group meetings and training \[[366](https://arxiv.org/html/2207.09460v11/#bib.bibx366), [367](https://arxiv.org/html/2207.09460v11/#bib.bibx367)\], and a Nvidia's evolving [collaborative creation metaverse](https://blogs.nvidia.com/blog/2022/08/09/omniverse-siggraph/) for digital engineers and creatives. They're all pretty different 'classes' of problem. The social metaverse angle where Facebook is concentrating most effort is of less interest to us here, though obviously markets will exist in such systems for business to customer. The next section will explore some of the software tools available to connect people. Everything looks pretty basic right now in all the available systems, but that will likely [change over the next couple of years](https://www.youtube.com/watch?v=cRLnR4Kot2M).
- #### 7.12.5 More like meeting support
- ##### Spatial
Spatial is worth a quick look because it's a business first meeting tool, and comparatively well received by industry for that purpose.
    Very compelling. Wins at wow.     Great avatars, user generated     Intuitive meeting support tools
- ##### MeetinVR
    Good enough graphics, pretty mature system     OK indicative avatars, user selected     Writing and gestures supported     Some basic enterprise tools integration     Need to apply for a license?
- ##### Glue
    Better enterprise security integration     Larger environments, potential for breakouts in the same space.     Workshop capable     3D object support, screen sharing, some collaborative tools     Writing and gestures supported
- ##### FramesVR
    3D object support, screen sharing, some collaborative tools     Larger scenes within scenes
- ##### Engage
    Fully customisable avatars     Presentation to groups for education and learning     PC first, quest is side loadable but that's a technical issue     Seated in observation points in a defined shared theatre     Screen sharing virtual communal screen watching, aimed at gamers,     film watching
- ##### Gather
Gather is an oddball meeting space based around fully customisable 2D rooms with a game feel. It's really a spatialised twist on video conferencing but interesting.
- ##### NEOSVR
[Notable because](https://neos.com/) it's trying to integrate crypto marketplaces, but we haven't tried it yet.
- ### 7.13 Displays & Headset Hardware
Awaiting a bit more market stability for this section. Of note is that Microsoft seems to be [abandoning Hololens](https://www.windowscentral.com/microsoft/microsoft-has-laid-off-entire-teams-behind-virtual-mixed-reality-and-hololens), and Apple seem to have postponed their commodity AR headset.
Microsoft think that creating the Perfect Illusion, that of a life-likeness in VR will require a field of view of 210 horizontal and 135 vertical, 60 pixels per degree subtended, and a refresh rate of 1800 Mhz according to Microsoft. They expect this by as soon as 2028 \[[368](https://arxiv.org/html/2207.09460v11/#bib.bibx368)\].
With the advent of [WebGPU](https://developer.chrome.com/docs/web-platform/webgpu/) alongside WebGL everything is likely to converge on the browser experience.
- #### 7.13.1 The Apple in the Room
Following the announcement of The Apple Vision Pro we start to see the convergence of spatial computing, mixed reality, locally applied transformer based AI, and business. They have perhaps removed "gorilla arm syndrome" \[[369](https://arxiv.org/html/2207.09460v11/#bib.bibx369)\] where hands in the sky interfaces are potentially uncomfortable over long periods \[[370](https://arxiv.org/html/2207.09460v11/#bib.bibx370)\]. Nathan Gitter and Amy DeDonato from the Apple Design team [introduce spatial design for the device](https://developer.apple.com/videos/play/wwdc2023/10072/).
- ##### Spatial operating systems
    Enabling users to design experiences not previously possible.     The presentation outlines how to keep apps familiar, be     human-centered, take advantage of space, enhance immersion, and make     apps authentic to the platform.     The world serves as an infinite canvas for new apps and games.     Existing app elements should be kept familiar with common elements     like sidebars, tabs, and search fields.     In a spatial platform, interfaces are placed within windows to make     them easily accessible and part of the user's surroundings.
- ##### Windows in Spatial Design
    Windows are designed with a new visual language, made of a glass     material that provides contrast with the world, awareness of     surroundings, and adapts to different lighting conditions.     Windows can be moved, closed, and resized by users, with windows     facing the user during movement.     Windows are flexible and can be resized to fit comfortably within     the user's view.     Choosing Window Size and Layout Windows are designed to be flexible,     adapting to content, and the window size should be chosen based on     this. Windows can change size dynamically based on context.     Apps can use multiple windows to display content side by side or     show distinct actions, but should ideally stick to a single window     to avoid user overwhelm.
- ##### Designing with Points
    Interfaces are designed with points to ensure they scale well and     remain legible at different distances.     Points allow designers to set the size of interface elements with     familiar units. Human-Centered Design     Good spatial design places the user at the center, accounting for     their field of view and movement.     The most important content should be placed in the center of the     field of view and use landscape layouts.     Ergonomics should also be considered, placing content along a     natural line of sight for comfort.     Designers should avoid placing content behind users or anchoring     content to their view as it can be disorienting.     Spatial design should aim to create stationary experiences that     require minimal movement from users.
- ##### User Mobility
The presentation emphasizes the importance of designing applications that require minimal movement from users. It recommends using system-level recentering methods to adjust the app's view when a user moves.
- ##### Space Utilization
The importance of optimizing an app's usage of space is discussed, as the available physical space for users can vary. It advises against constraining your app based on the physical space available and instead creating an app that can function in any amount of space.
- ##### Dimensionality
The use of depth and scale in designing the user experience is emphasized. Depth can help with hierarchy and focus, and scale can be used to emphasize content. The text warns against overusing depth, especially with text, and encourages developers to experiment with scale to achieve the desired user experience.
- ##### Immersiveness
The passage introduces the concept of an immersion spectrum, where an app can transition between various states of immersion based on the user's experience. The importance of smooth transitions, designing with consideration to user focus, thoughtful blending with reality, and keeping the user comfortable are emphasized.
- ##### Sound Design
It also highlights the importance of using spatial audio to enhance the immersive experience of an app, which includes attaching sound to objects and creating soundscapes.
- ##### User Comfort
Recommendations for moving an immersive app, focusing on avoiding disorienting fast movements and instead recommending fade out and fade in techniques to keep the user comfortable during motion.
- ##### Transitions
It highlights the importance of clear, intuitive methods for entering and exiting immersive experiences. It suggests using easily recognizable symbols, such as arrows for expanding or collapsing views.
- ##### Authenticity
Emphasizes creating an authentic experience that takes full advantage of the platform's capabilities. An example given is Freeform, which uses a large creative space allowing users to view all their content at once.
- ##### Key Moments
Focusing on a "key moment" that provides a unique spatial or immersive experience is recommended. This could involve enhancing a moment with depth and scale or transforming the user's space to create a unique and memorable experience.
- ### 7.14 Unreal & Virtual Production
![Figure 7.7: Time magazine Metaverse Cover 2022](../assets/time.png) 
![Figure 7.8: Epic games flywheel by Matthew Ball](../assets/epicflywheel.jpg) 
Epic is a behemoth and has made better business development decisions, and have a better technology than their main competitor Unity3D. Unity didn't make the cut for this book, though their technology is great. Their recent merger with a [malware manufacturer](https://www.pcgamer.com/unity-is-merging-with-a-company-who-made-a-malware-installer/) and a history of poor data privacy have removed them from consideration at this time.
- #### 7.14.1 Virtual Production
ICVFX (in camera virtual effects) or "Volume shooting" is the application of large, bright LED walls to film and TV production. More broadly than this Virtual Production is a suite of real-time technologies that weaves through pre and post production to accelerate creativity, and reduce costs. These are collaborative, and often distributed tasks:
    Dry runs with actors to plan shots in mixed reality     Virtual set design and storyboarding in full VR     Shot camera track design (movement, focus, lens choices etc)
![Figure 7.9: John O'Hare (author) with a virtual production robot.](../assets/vprobot.jpg) 
- ### 7.15 Different modalities
- #### 7.15.1 Controllers, gestures, interfaces
- ##### Accessibility
    Hand tracking and gesture
- #### 7.15.2 Mixed reality as a metaverse
[Spatial anchors](https://docs.microsoft.com/en-us/windows/mixed-reality/design/spatial-anchors) allow digital objects to be overlaid persistently in the real world. With a global 'shared truth' of such objects a different kind of metaverse can arise. One such example is the forthcoming [AVVYLAND](https://avvyland.com/).
- #### 7.15.3 Augmented reality
Marc Petit, general manager of Epic Games envisages a 2 watt pair of glasses, connected to a 10 watt phone, connected to a 100 watt computer on the edge. This is a device cascade problem which has not yet been solved, and is at the edge of achievable thermodynamics and latency.
The closest technology at this time seems to be [Lumus' waveguide projectors](https://lumusvision.com/) which are light, bright and high resolution. Peggy Johnson, CEO of Magic Leap, one of the market leaders said: ["If I had to guess, I think, maybe, five or so years out, for the type of fully immersive augmented reality that we do."] 
In a [GQ profile](https://www.gq.com/story/tim-cook-global-creativity-awards-cover-2023?mbid=social_twitter) Cook, the Apple CEO talked at length about the challenges and opportunities of AR headsets. He has been emphasizing the importance of augmented reality over VR for almost a decade, believing that AR can enhance communication and connection by overlaying digital elements on the physical world. Cook's vision aligns with Apple's rumoured mixed reality headset, which is expected to cost around \$3,000 and focus on 'copresence', which we have discussed at length in this chapter. Apple's approach differs from Meta's metaverse, as Apple aims to integrate digital aspects into the real world rather than create purely digital spaces. This is an interesting area for our applications of bringing small teams together, but the pricing at this time is significantly at odds with our chosen market. Cook, like this book, has highlighted AR's potential in education and its ability to bring people together in the real world.
- #### 7.15.4 Ubiquitous displays
- ### 7.16 Risks
Metaverse is fraught with risks, partly because it's new, and partly because of the pace of adoption. Regulation is well behind the technology, to the alarm of some academic observers \[[372](https://arxiv.org/html/2207.09460v11/#bib.bibx372)\].
    Abuse; because of the real-time and spatio-temporal abuse happens     less like in the current web 2 social media, and more like in the     real world, but with less opportunity for repercussions. It might be     that natural language processing and machine learning can help with     this, but it's a tough problem. One idea might be to record the     speech to text of interactions between participants, and flag to     them if a "bullying, harassment, predation threshold" is met. This     could be encrypted with the public keys of the participants and a     notice sent to them that if they wished to follow up with     authorities then they have the necessary attestations and proofs.     This is minimally invasive and privacy preserving, and acts as a     strong disincentive to repeat offence. It can also feed into a     global "web of trust" reputation system in a 'zero knowledge' way.     Users who flag abuse to the reputation system can leverage the     machine learning opinion without revealing what happened (though     they would have the data). This would also act as a disincentive     without the social stigma issues of reporting.
    Reporting could be achieved without machine learning identification     of potential problems, but there would have to be a social cost to     reporting (like gossiping incessantly about others) which would     erode the social score of the reporting entity. This would mitigate     bot based reputation harm.     Miscommunication; which as we have seen in the early section of the     metaverse chapter is both complex and hard to mitigate     Jitter, judder, jagginess, and interruption of flow; because the     network overhead is higher than other communication media it's much     more exposed to latency effects     Physical harms, especially to developing brains and ocular systems
The UK is [positioning itself](https://bills.parliament.uk/bills/3137) to heavily regulate safeguarding in the space, with significant fines for non-compliance. This will of course simply lead to users operating on platforms which are not subject to UK law.\
- ### 7.17 Summary TL;DR
    The internet may be undergoing a transformation, driven by trust     abuses by incumbent providers, and popularization of concepts like     Web3 and the Metaverse.     Current large scale 'social' and immersive metaverse platforms have     low adoption, while more advanced games-based solutions don't     address societal or business needs.     Platforms like Roblox, VRChat, and Nvidia Omniverse emerge as     potential contenders in the metaverse landscape.     Distributed compute and large language models can help bridge the     digital divide by enhancing global access equity and addressing the     needs of emerging markets and less developed nations.     The potential lies in uniting individual ecosystems with     transferable goods across digital society through global ledgers     like blockchain, despite the associated risks and uncertainties.     Industry is looking towards an \"open metaverse\" to mitigate risks     observed in implementations like Meta, necessitating contributions     of open-source and federated approaches in telecollaboration     research.     By embracing Nostr protocol, we could enable connections and     federation of mixed reality spaces, mediate data synchronization,     and maintain secure communication.     AI, machine learning, and generative art play a crucial role in     driving innovation, with models like GPT4, Llama, Alpaca, generating     excitement, and deepening global discussions around AI.     Overcoming legislative and cultural barriers, alongside integrating     large language models and distributed compute, can help address     issues related to trust, accessibility, governance, and safeguarding     within the metaverse and digital society at large.     Open-source tools for supported creativity and augmented     intelligence using multi-modal models, can help tackle     accessibility, creativity, language barriers, and governance within     the metaverse landscape.     The application of these tools can lead to the development of new     collaborative frameworks across various sectors such as training,     research, biomedical, creative industries.     By utilizing these new AI-driven technologies and emphasizing on     trust, accessibility, and open-source approaches, we can create a     more inclusive, global digital society while promoting technological     empowerment and expansion of the global ideas market.
- ## Chapter 8 Artificial Intelligence
- ### 8.1 The Cambrian explosion of ML/AI
This section is full of rough edges and some repetition; Working on it!
- #### 8.1.1 Overview
Though the history of this field reaches back to the 1940's with McCulloch et al. exploration of the possible mathematical underpinnings of human brain neurons \[[373](https://arxiv.org/html/2207.09460v11/#bib.bibx373)\]. During the writing of this book we have seen an inflection point in machine learning, to the point where the term "artificial intelligence" is feeling intuitively and subjectively real for the first time. To be clear AI is still a pretty meaningless term. 'Intelligence' is one of those slippery words which is highly dependent on context. A satnav system running on a phone can make an intelligent choice about a route by synthesising data and presenting comprehensible results, but it seems absurd to ascribe an intelligence to it. It's possible that there's some kind of "spoooky" quantum activity in play in a conscious human brain, as described in mind bending mathematical depth by Penrose in 1989 \[[374](https://arxiv.org/html/2207.09460v11/#bib.bibx374)\]. It's something of an unknown unknown \[[375](https://arxiv.org/html/2207.09460v11/#bib.bibx375)\], and that we'll never get to what's called 'strong' or 'general' AI \[[376](https://arxiv.org/html/2207.09460v11/#bib.bibx376), [377](https://arxiv.org/html/2207.09460v11/#bib.bibx377)\], reserved by some scientists for "true consciousness", whatever that means \[[378](https://arxiv.org/html/2207.09460v11/#bib.bibx378)\]. With that said we may be approaching the threshold of the 'Turing Test' \[[379](https://arxiv.org/html/2207.09460v11/#bib.bibx379)\], initially posited by Alan Turing in 1950 \[[380](https://arxiv.org/html/2207.09460v11/#bib.bibx380)\], and the goalposts have begun to move in response to claims that there have been successful examples \[[381](https://arxiv.org/html/2207.09460v11/#bib.bibx381), [382](https://arxiv.org/html/2207.09460v11/#bib.bibx382), [383](https://arxiv.org/html/2207.09460v11/#bib.bibx383), [384](https://arxiv.org/html/2207.09460v11/#bib.bibx384)\]. It feels that in this moment it is appropriate to open with a risks section, and work backwards. This is grounded in the hypothesis that there is no agreed end goal here (as we saw with the Bitcoin/Crypto chapter).
To set the tone here let's have OpenAI's ChatGPT give us a definition: [Intelligence is the ability to acquire and apply knowledge and skills in order to solve problems and adapt to new situations. It can involve a range of cognitive abilities, such as perception, learning, memory, reasoning, and decision-making. Intelligence is a complex and multifaceted concept that has been studied by psychologists, philosophers, and scientists for centuries.] 
The Oxford English Dictionary defines Artificial intelligence as "The capacity of computers or other machines to exhibit or simulate intelligent behaviour". This is very murky territory. The boundary line between very capable trained systems and something that [feels]  like intelligence is obviously a subjective one, and different for each person and context, (Figure [8.1](https://arxiv.org/html/2207.09460v11/#Ch8.F1 "Figure 8.1 ‣ 8.1.1 Overview ‣ 8.1 The Cambrian explosion of ML/AI ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art").
![Figure 8.1: The terminology in the field is both somewhat blurred and highly 'nested'.](../assets/AI.jpg) 
We will use AI and ML interchangeably in this text, but is so doing we hope to draw attention to the moment we find ourselves in. It feels like there is an inflection point in human history happening right now, though to somewhat burst the bubble on this hyperbole it's worth reading the legendary Stephen Wolframs [explanation of these current systems](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) as glorified autocompletes.
Irrespective of the gap between the perception and truth around these systems there is now a feedback loop where the data that these systems are trained on will be learning from both human [and]  outputs from such systems. Todays young children will never know a world in which the information they encounter is verifiable as of purely human origin. The implications of this are unclear but exciting. In writing this book it became obvious to add this chapter in, and change the direction on the research and product development, because nothing in human history will remain untouched by this. As we will see 'metaverse' is likely to change at an incredible rate as a function of some parts of this technology.
![Figure 8.2: Major stands of generative AI and their associated models at the time of print.](../assets/sequoiacapLandscape.png) 
![Figure 8.3: Major stands of large language models from Yang et al \[[385](https://arxiv.org/html/2207.09460v11/#bib.bibx385)\]](../assets/llmlandscape.jpg) 
- ### 8.2 Generative art systems
Generative art refers to art that is generated algorithmically rather than manually created. In this report, we will provide an overview of generative art, including its history, key techniques, applications, and future outlook.
Generative art emerged in the 1960s alongside early computer art. Artists like Frieder Nake and Georg Nees used algorithms to create abstract visual art. In the 1970s and 80s, fractal geometry allowed for complex recursive patterns. More recently, neural networks have enabled new forms of image generation, and they are the focus of this text.
- #### 8.2.1 Modern Models and Systems
- #### 8.2.2 Image Generation
    [Stable Diffusion]  - An open source diffusion     model capable of creating realistic images from text prompts. Widely     used by artists due to:
    -   [[--] ]          Flexible and intuitive text prompts     -   [[--] ]          Many interfaces and extensions for control, most notably         controlnet for our puposes     -   [[--] ]          Ability to fine-tune on custom datasets     [DALL-E 3]  - First to market, it's a     proprietary generative AI system from OpenAI that creates images     from text captions. Key features:
    -   [[--] ]          Stunning contextual comprehension     -   [[--] ]          Diverse creative capabilities from prompts     -   [[--] ]          Works best with OpenAI access and paid credits     -   [[--] ]          Integrated with Microsoft Bing, and free for small use cases.     [Midjourney]  - Web-based generative art tool     with social community aspects. Notable for:
    -   [[--] ]          Easy to start generating images quickly     -   [[--] ]          Built-in sharing and voting on generations     -   [[--] ]      -   [[--] ]          Limited free tier with paid subscriptions     -   [[--] ]      -   [[--] ]          Subject to change, making consistency of approach impossible.     [Imagen]  - AI system from Google producing     images from text. Characteristics:
    -   [[--] ]          Very high-resolution outputs     -   [[--] ]          Control over lighting and detail     -   [[--] ]          Currently restricted to limited partners
- #### 8.2.3 How they work
There's a good detailed and in depth blog post by Weng [here](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#classifier-free-guidance).
The four main types are over-viewed [by Deshwal](https://www.linkedin.com/feed/update/urn:li:activity:7088752096853803008/) as below:
-   [GAN: Generative Adversarial Network]      \"Adversarial\" as the name suggests are two opposite networks. One     learns to create images out of noise (Artist) which is actually very     hard process and the other says \"umm okay! This isn't good\"     (Critic) which is a relatively easy process. So because of the fact     that being an artist is very hard than being a critic, these     networks are not stable and Critic learns faster than the Artist. -   [Auto Encoder style]      VAE, U-Nets etc belong to this category where same network breaks     down image in deeper level features using CNN and then rebuild it     again. That's like a child learning by breaking things. VAE and     U-Nets are almost same with minor differences and serve as a base     model in Diffusion process so that think of them ans analogues to     BERT in LLMs. -   [Flow Based]      : Well here it becomes complex. You apply function X to an image and     then you re-create the original image by applying the Exact inverse     of that function. For example, a very basic function is to add 5 and     then subtract 5 to get original stuff. -   [Diffusion processes]      VAE, U-Nets are used as base models. You insert some pseudo random     (because you know what you added based on a timestamp \"T\")     Gaussian noise to an image and instead of asking the model to     predict original image, you ask the model to predict the Noise that     you inserted. Since Gaussian is an additive noise independent of     original signal, you subtract that from image and get original     image. Another piece is that instead of predicting whole noise at     once, you predict the noise for previous (T-1) step.
Intuitive interfaces provide easy access to these models. [Automatic1111's Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) offers a full-featured frontend to Stable Diffusion. [Midjourney](https://www.midjourney.com/) provides a social platform to create, share and discuss AI art. [Runway](https://www.runwayml.com/) delivers generative models through a subscription service.
Fine-tuning techniques like [DreamBooth](https://arxiv.org/abs/2208.12242) allow customizing Stable Diffusion by training on datasets of specific concepts. [styleGAN-NADA](https://github.com/yfszzx/stylegan-nada) improves image quality through noise optimization. [styleGAN3](https://github.com/NVlabs/stylegan3) introduces a generator architecture that achieves state-of-the-art results. Extensions like [Gaugan](https://github.com/mit-han-lab/gaugan) bring control over seasons, weather, lighting and more.
Beyond generation, image processing techniques can manipulate existing visuals. [GFPGAN](https://github.com/TencentARC/GFPGAN) restores blurred faces using facial landmarks and semantic segmentation. [BRDNet](https://github.com/dazhizhong/BRDNet) removes unwanted objects through edge-aware propagation and diffusion. [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) super-resolves images up to 16x resolution. [DeOldify](https://github.com/jantic/DeOldify) colorizes black and white photos through deep learning. Such techniques enable restoring, retouching and enhancing images.
- ##### Stable Diffusion
- ##### Stable Diffusion Ecosystem
As an open-source diffusion model, [Stable Diffusion](https://arxiv.org/abs/2105.05233) has given rise to an extensive ecosystem of models, interfaces, extensions, and communities.
- ##### Models
The core Stable Diffusion repository provides strong baselines like [sd-v1-4](https://github.com/CompVis/stable-diffusion) optimized for speed and [sd-v2-1k](https://github.com/Stability-AI/stablediffusion) for 1024x1024 resolution. Models exist for specific domains like anime, manga, and hentai.
- ##### Interfaces
Many open-source frontends provide access to Stable Diffusion:
 ###### Extensions
Additional modules provide enhanced control:
    [Vedaso](https://github.com/alembics/vedaso) - creative effect     brushes     [InvokeAI](https://github.com/invoke-ai/InvokeAI) - optimized     inference and rendering
- ##### Training & Tuning
Stable Diffusion can be customized:
 ###### Community
Vibrant communities continually advance Stable Diffusion:
    [Civitai](https://civitai.com/) - central model hub with versioning
- ##### ComfyUI
[ComfyUI](https://github.com/comfyanonymous) is a feature-rich set of tools and libraries for building interactive web applications using the [React](https://reactjs.org/) framework. It makes creating beautiful, functional UIs easy through:
    [React-Based]  - Built on React for modular,     reusable components     [Declarative]  - Describe desired UI state     without implementation details     [Extensible]  - Easily add custom components     and functionality     [Testable]  - Designed for confident testing of     UI behavior     [Documented]  - Well-documented for easy     learning     [Community]  - Large active community for help     and support
Extensions provide additional capabilities:
    [Components]  - Pre-built React components for     common UI elements like buttons, menus, and forms     [Animations]  - Animated React components for     engaging UIs     [State Management]  - Tools for managing UI     state     [Testing]  - Utilities for testing ComfyUI     applications
Other notable features include:
    [Responsive Design]  - Components auto-adjust     layouts for any device size     [Internationalization]  - Support for global     applications in different languages     [Accessibility]  - Interface remains usable by     people with disabilities
The ComfyUI ecosystem is constantly evolving with new extensions created by the vibrant community. With its versatility, extensibility and helpful userbase, ComfyUI empowers developers to create beautiful, functional UIs for diverse web applications. The declarative programming style and reusable components help quickly build interfaces that are responsive, accessible, and internationalized.
- #### 8.2.4 Video generation
This is incredibly fast moving area and I have many many links in my Mindmap. This section is a placeholder really, I wouldn't act on it.
    [DALL-E 3D]  - 3D model generation by     Anthropic using principles from DALL-E 2. Allows:
    -   [[--] ]      -   [[--] ]          Control over materials and lighting     -   [[--] ]          Interaction with object geometry     [Xformation]  - Proprietary 3D generation     system capable of modifying shape from images.
    -   [[--] ]          Deforms template 3D objects to match 2D images     -   [[--] ]          Controllable 3D effects from image edits     [Text2Mesh]  - Leveraging Stable Diffusion for     text-based 3D model generation.
    -   [[--] ]          3D stylization based on natural language input     -   [[--] ]          Control mesh topology and appearance     [Gaussian Splatting]  - A development from the     NeRF technology research, and likely the main contender for all     future tech right now..
    -   [[--] ]          Fast and efficient models     -   [[--] ] 
Extending image synthesis techniques, models like [Stable Diffusion](https://arxiv.org/abs/2105.05233) are being adapted to generate artificial video content. Dedicated systems like [Phenaki](https://www.anthropic.com/research/phenaki) and [Runway](https://runwayml.com/) enable text-to-video generation with control over length, resolution and scene contents.
Creating smooth, consistent video requires modeling inter-frame coherence. Techniques like [EBSynth](https://ebsynth.com/) achieve this through interpolation and style transfer between frames. [FastVid2Vid](https://www.fastvideoai.com/) matches latent vectors between frames to improve consistency. [Instant Neural Graphics Primitives](https://nvlabs.github.io/instant-ngp) (Instant-NGP) learns a temporal model over sequences of frames.
Existing videos can also be enhanced through diffusion models. Techniques enable automatically increasing resolution, translating scenes to different styles, editing objects seamlessly, and more. However, concerns exist around deepfake videos and synthetic media. Moderation systems like [Anthropic's Claude](https://www.anthropic.com/) may provide remedies.
Overall, rapid progress in generative video hints at possibilities of creating immersive films, VR experiences, lifelike avatars and more. But thoughtful governance frameworks are necessary to manage risks.
- #### 8.2.5 Audio generation
Recent breakthroughs have also extended AI synthesis to the audio domain, enabling applications like text-to-speech, music composition, and editing podcasts.
Models like [Jukebox](https://jukebox.openai.com/) and [Facebook's Jukebox](https://github.com/%20facebookresearch/jukebox) generate novel music conditioned on genres, artists, and lyrics through a hierarchical VQ-VAE framework. Meanwhile, [Coqui TTS](https://github.com/coqui-ai/TTS) and [Tacotron 2](https://github.com/NVIDIA/tacotron2) convert text to human-like speech using end-to-end neural architectures.
For editing audio, tools like [Riptide](https://riptide.ai/) remove vocals from songs, while [Descript](https://www.descript.com/) inserts music and trims silences in podcasts. However, bad actors could exploit such capabilities for impersonation fraud and fake media. Strong governance models are critical.
Looking ahead, advances in generative audio may enable creating interactive AI companions, realistic speech synthesis, and personalized music experiences. But maintaining public trust through transparency and accountability will be essential.
- #### 8.2.6 3D generation
3D shape generation has also made strides through AI, transitioning text-to-image breakthroughs to the 3D domain. Methods like [GA-Fusion](https://nv-tlabs.github.io/GA-fusion) combine GANs with gradient-based optimization for high quality results. [CLIP-Forge](https://github.com/autodeskailab/clip-forge) matches rendered images with CLIP embeddings to guide optimization. [3D-Highlighter](https://threedle.github.io/3dhighlighter) localizes text prompts on shapes by comparing CLIP similarities.
Such techniques could enable creators to manifest their ideas into 3D worlds. However, thoughtful governance is critical to reduce risks associated with impersonation, toxic content, and intellectual property. Community building, education, and responsible deployment will help realize the positive potential of AI.
- #### 8.2.7 Conclusion
Rapid progress in AI has unlocked breakthrough capabilities for synthesizing realistic content across images, video, audio, and 3D geometry. However, concerns around biases, misinformation, and toxic content necessitate responsible development and deployment of these technologies. Maintaining public trust through transparency, accountability and inclusivity will be key to ensuring that the benefits of AI progress outweigh the risks. If harnessed judiciously and ethically, generative AI could augment human creativity in unprecedented ways. But it should empower rather than replace us. Ongoing advances in AI safety and governance will help achieve this vision
- #### 8.2.8 Major trends in AI
- ##### The concentration of AI power
In recent times, the arena of artificial intelligence (AI) has seen the emergence of colossal entities that have taken the helm of AI research and development. Prominent players such as Google, Microsoft, Meta, and OpenAI have plunged billions into the cultivation of potent AI architectures, with a special emphasis on large language models (LLMs) like GPT-3 and ChatGPT.
Originating in 2015 as a non-profit entity dedicated to the open exploration of AI for the collective good, OpenAI transitioned from its foundational ethos following a pivotal investment of \$1 billion from Microsoft in 2019. This infusion of capital marked a turn towards a more proprietary and competitive orientation, with the endgame of reaching the pinnacle of artificial general intelligence (AGI). In this paradigm shift, OpenAI's formidable 175 billion parameter behemoth, GPT-4, became an enigmatic entity, shielded from public scrutiny. The rationale provided for this clandestine stance revolved around safety and competitive considerations.
Contrastingly, Meta adopted a path of openness, fully disclosing its 65 billion parameter LLaMA 2 LLM, inclusive of the model weights, to the public domain. This ethos is rooted in the belief that a culture of openness propels rapid advancement by paving the way for widespread experimentation and communal contributions. However, it is pertinent to note that Meta's LLaMA 2 does carry stipulations on commercial exploitation.
Initially, Google was at the forefront of AI innovation with its TensorFlow framework, but has seen its leading position eroded by Meta's PyTorch. Post the commercial success of its products, Google's AI endeavors have veered towards a more proprietary model, with novel models and academic publications seeing the light of day post commercialization.
- ##### Concentration of Power and Control
The centralization of AI evolution within the confines of a select few private behemoths such as Microsoft-backed OpenAI or Google engenders a nucleus of power and control over AI advancements. Contrary Research has an [excellent report](https://research.contrary.com/reports/the-openness-of-ai) on this. As AI melds deeper into the fabric of products and services, these titans stand to wield extensive sway over the modalities of human communication, thought processes, and information accessibility.
The dependency on a sparse set of centralized LLMs harbors risks such as a widespread dissemination of confidential data in the face of a security breach. Moreover, the consolidated control furnishes these corporations with the means to potentially curtail information or mold narratives in alignment with their vested interests. For instance, OpenAI exercises centralized control over the narrative frameworks of its influential models like ChatGPT.
- ##### Lack of Transparency and Innovation
Centralized LLMs exhibit a veil of opacity regarding their operational mechanics and training methodologies. The elusive nature of OpenAI's GPT-4 renders it a veritable black box, impervious to audits aimed at uncovering issues such as bias within the 175 billion parameter model's training data. This shroud of mystery precludes informed discourse on the ethical employment of this technology.
On the flip side, Meta's open-sourced LLaMA facilitates a level of public oversight that could preemptively address inherent issues. The clandestine nature of closed models stifles innovation as it bars a significant portion of the research community from building atop these models. Open ideologies foster a milieu of collaboration, propelling progress forward.
In summation, the monopolization of AI progression and influence within a handful of private juggernauts engenders risks spanning lack of transparency, potential censorship, stifled innovation, and single points of vulnerability. A paradigm shift towards more open and decentralized methodologies is imperative to mitigate these looming threats.
- ##### Some ways of thinking
Poulter [CEO](https://twitter.com/jamespoulter) of Vixen Labs has come up with a somewhat strained analogy he calls "The Central Intelligence Agent". I'm going to include it until I find something better because I think he's struck on something by dividing up the company needs with his taxonomy (Figure [8.4](https://arxiv.org/html/2207.09460v11/#Ch8.F4 "Figure 8.4 ‣ Some ways of thinking ‣ 8.2.8 Major trends in AI ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art")).
![Figure 8.4: Vixen Labs anthropomorphic taxonomy of business functions](../assets/vixenAnthro.png) 
He sees five groups (per the article):
    "Curious creators" argue everything you can do, you should do.     (Venture capitalist Marc Andreessen recently expressed a similar     view in a blog post about A.I.)     "Euphoric true believers" only see the good in technology.     ["Commercial profiteers" don't necessarily understand the new     technology but are enthusiastically seeking to cash in on the     hype.]      "Alarmist activists" advocate for restricting A.I.     "Global governance advocates" support regulation and necessary     crackdown.
This seems a pretty simplistic set of buckets, but he's got a decent dataset, and he's -very- eminent so perhaps we should realistically see ourselves in the emboldened commercial profiteer category. I think this kind of self reflection is important when dealing with things this potentially transformational. Sonnenfeld said: ["As Robert Oppenheimer warned us, it can be very dangerous to think that technology only takes us to the best of the world."] 
- ##### Trusted enterprise AI
Enterprise AI, specifically designed or retrofitted for professional use cases, is becoming a significant theme. This trend is driven by leading companies such as Google, Microsoft, and latterly, curiously, Salesforce. Trust has become a primary consideration. At this early stage in the technology it is important that corporate and private users alike bear in mind that the LLMs are 'leaky' and are using the data fed into them to train themselves. They are [explicit about this](https://help.openai.com/en/articles/6783457-chatgpt-general-faq). Anything that goes into ChatGPT can resurface, as Samsung have found out [to their cost](https://cybernews.com/news/chatgpt-samsung-data-leak/). At this time the following companies have responded by banning the use of the technology internally.
    Amazon's lawyers recommended caution, though a recently leaked     document suggests that managers are recommending it's use.
There are likely [many]  more who have done this less publicly, and indeed I am aware of examples. In response to this [OpenAI announced](https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt) a business version of its tool, ChatGPT Business. Clearly this is a premium subscription service designed to be private by default. The service manages multiple users and does not train its future models on any conversations that flow through the business. This approach is a significant step towards establishing trust in AI tools, as it ensures that sensitive business conversations are not used to train AI models.
As mentioned, Salesforce has been partially AI-powered for years. They recently announced a series of AI-related developments, including the pilot of 'Einstein GPT', dubbed "the world's first generative AI for CRM". This tool builds on an existing underlying intelligence layer called Einstein, which has been running in Salesforce since 2016. The new generative Einstein GPT is more content-oriented, helping businesses auto-generate text, pictures, and code. This tool is designed to help sales teams find the most likely next customer to buy. More interestingly they are leveraging their expertise in 'Salesforce Ventures' a \$500 million fund focused on funding generative AI startups. They have already invested in major projects like Humane, Triple, Anthropic, and Cohere.
They have [also]  announced an AI Cloud suite: 'Salesforce's AI Cloud'. It includes nine GPT-powered applications designed to automate and enhance various business processes. These applications include Sales GPT, Service GPT, Marketing GPT, Commerce GPT, [Slack GPT] , Tableau GPT, Flow GPT, and Apex GPT.
Each of these applications is designed to cater to specific business needs, such as personalizing text generation for emails, automating mundane tasks, customizing messages for different audience segments, and creating workflows from natural language prompts. You can see that our work is already a customer here and this could be built upon.
This suite emphasizes the 'Einstein GPT Trust Layer', designed to ensure no potential data leakage, allowing enterprises to use AI for their most sensitive needs. They say this trust layer sits between customer data and the AI models, ensuring that the AI capability can provide predictions without actually looking inside the data. This approach would allow our work to leverage the power of enterprise AI without sacrificing data privacy and/or security.
Elsewhere in enterprise AI:
    Accenture announced a \$3 billion investment into their data and AI     practice. This investment includes doubling their talent to 80,000     people, launching an AI navigator for Enterprise platform, and     starting accelerators for data and AI readiness across 19 different     industries.     Contextual, an enterprise-focused AI startup, recently launched out     of stealth with \$20 million in seed funding.     Glean announced a \$100 million round and introduced a workplace     chatbot called the Glean Chat Business.     Olive: This healthcare automation startup has raised \$450 million     in fresh capital to build out its enterprise AI for hospitals 1.     Welltok: This company provides a cloud-based employee wellness     platform and has raised \$355 million 1.     Outreach: This sales engagement platform has raised \$114 million in     series E funding 1.     Stackpath: This cybersecurity startup has raised \$396 million in     funding 1.     Cohere, which is another business-focused AI startup, recently     announced a \$270 million series C funding round 2, and are     partnered by     [Oracle](https://www.oracle.com/news/announcement/oracle-to-deliver-powerful-and-secure-generative-ai-service-for-business-2023-06-13/).     Tunisian enterprise AI startup InstaDeep has also raised \$100     million in Series B financing led by Alpha 3.     There's a raft of tools like [CustomGPT](https://customgpt.ai/), or     day planner [Before Sunset](https://www.beforesunset.ai/) that     promise to take your data and make it smart by leveraging their     deals with the big cloud providers. The prime example of this     approach is [Dropbox     AI](https://blog.dropbox.com/topics/product/introducing-AI-powered-tools),     which claims to bring the Apple spotlight experience, with ChatGPT     smarts, to clouds data. I don't have confidence that I know enough     about this, and it seems to be the purview of the AI-Club. If you     have a use case there's likely a product, but we'd have to project     plan it in specifically and find the right fit and cost/benefit.
Taken overall these investments indicate a strong belief in the transformative potential of AI in the enterprise sector.
With all this said it's possible the technology is over-hyped. While some believe that AI will disrupt industries in unimaginable ways, others argue that the technology still has a long way to go. Some even argue that the distracting nature of the platforms may be net negative in the short term. Regardless, the current state of Enterprise AI represents a pivotal moment, with companies trying to productise AI and change workflows within large corporations. The impact of these developments could range from a promise of transformation with AI being every bit as disruptive as everyone says it is, to an overhyped flop, as often happens with new technologies. Some industry analysts argue that we're still in the early stages of AI's potential impact. Tech analyst Dan Ives likens the current state of AI to a "Gold Rush" moment, suggesting that we're closer to 1995 than 1999 in terms of AI's evolution and impact on industries. This perspective suggests that while AI has made significant strides, there's still a long way to go before it fully transforms the business landscape. I would tentatively agree with this analysis, and avoid over investing in low confidence FOMO plays.
- ##### Brute forcing ChatGPT4 with contexts
While we await 'ChatGPT Business' it's still possible to explore using the tooling. The ChatGPT4 system is so far out ahead of everyone else it's sometimes useful to consider using it for business by adding in carefully crafted chunks of context data to refine how it answers. This is prompt engineered. A fine example of this everyday use of the technology can be found in [this clarify capital report](https://clarifycapital.com/the-future-of-investment-pitching) which finds that ChatGPT created pitch decks are [far]  more compelling than human created ones. It can be developed onward for more complex corporate proposals like this through the API, which is a subscription service, with additional tiers for heavy corporate use cases ([8.2.8](https://arxiv.org/html/2207.09460v11/#Ch8.S2.SS8.SSSx8 "Go all in with Microsoft ‣ 8.2.8 Major trends in AI ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art")).
    [Advantages] 
    -   [[--] ]          Can be trialled in the web interface, spending a few hours or         perhaps days building a custom context that solves a specific         use case for the business, then simply copy/pasting.     -   [[--] ]          [OpenAI GPT is incredibly cost         effective]  (\$1 for around 700         pages for GPT3.5 performance), or \$20 a month for the web         subscription.     -   [[--] ]          GPT4 is [way]  ahead in terms of         performance. Possibly 18 months ahead of open source.     -   [[--] ]          Incredible team and partnership. Plug-ins are arriving very fast         to solve real business problems. They have scale and velocity.     [Disadvantages] 
    -   [[--] ]          In terms of future project planning 18 months isn't that long,         open source is worth investing in for the sake of         differentiation in those time-scales.     -   [[--] ]          It's a very general model, refining this through the API means         programming work. This is a known unknown with staffing costs.     -   [[--] ]          You're necessarily giving your commercial data to a cloud         service.     -   [[--] ]          Their "corporate" private package has trust implications, and         cost implications (more in the next section).     -   [[--] ]          [It's reliant on a reliable internet connection, so it's         suitable for the office but perhaps not 'site'. Using it might         therefore mean ending up investing time in two development         tracks] .
One of the incredibly frustrating things about the GPT series is that OpenAI are changing the code behind the models all the time as seen in figure [8.5](https://arxiv.org/html/2207.09460v11/#Ch8.F5 "Figure 8.5 ‣ Brute forcing ChatGPT4 with contexts ‣ 8.2.8 Major trends in AI ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art"). This makes it hard to build upon in a trustable way \[[386](https://arxiv.org/html/2207.09460v11/#bib.bibx386)\]. The team built a dataset with 50 easy problems from LeetCode and measured how many GPT-4 answers ran without any changes. The March version succeeded in 52% of the problems, but this dropped to a pale 10% using the model from June. I assume that OpenAI pushes changes continuously, but we don't know how the process works and how they evaluate whether the models are improving or regressing. In my opinion, this is a red flag for anyone building applications that rely on GPT-4. Having the behavior of an LLM change over time is not acceptable.
![Figure 8.5: Changes to GPT models performance over time - denied by OpenAI (Chen et al) \[[386](https://arxiv.org/html/2207.09460v11/#bib.bibx386)\]](../assets/gptchanges.jpg) 
- ##### ChatGPT - Code Interpreter
The ChatGPT Code Interpreter Plugin, introduced in March 2023, offers a sandboxed environment featuring a working Python interpreter. This environment, which is isolated from other users and the Internet, supports an impressive array of functionalities. It comes pre-loaded with over 330 libraries, including popular ones such as pandas, matplotlib, seaborn, and TensorFlow, among others.
As illustrated in Figure [8.8](https://arxiv.org/html/2207.09460v11/#Ch8.F8 "Figure 8.8 ‣ Current LLM Capabilities ‣ 8.2.11 Consumer tools ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art"), the Code Interpreter Plugin is capable of performing a myriad of tasks. For example, it can visualize any data inputted by the user, generate GIFs of the visualizations, and perform file uploads and downloads. It can extract colors from an image to create a color palette, and autonomously compress large images when memory is running low. Moreover, the plugin can clean and process data, generate insightful visualizations, and convert files to different formats quickly and efficiently.
The Code Interpreter Plugin can be installed by ChatGPT Plus users in a few simple steps. However, it is worth noting that while this plugin is powerful, it does have certain limitations, such as frequent environment resets, limited Optical Character Recognition (OCR) capabilities, and an inability to access the web. Despite these limitations, OpenAI continues to work on improving the capabilities of the Code Interpreter Plugin, promising a future with substantial impacts on the world of programming.
    The Code Interpreter Plugin introduces a sandbox and an advanced     language model, both of which are critical to its functionality.     The emphasis of the plugin is on the quality of the model, which can     generate code, debug it, and even decide when not to proceed without     human input.     The plugin offers substantial model autonomy, enabling it to work     through multiple steps of code generation autonomously.     Despite its powerful capabilities, the plugin does have limitations,     such as frequent environment resets and limited OCR capabilities.     The plugin is only available to ChatGPT Plus users, and requires a     few simple steps for installation.     The Code Interpreter Plugin represents a significant advancement in     the realm of programming, changing the way programmers interact with     AI systems.
- ##### Go all in with Microsoft
- ##### Anthropic - Claude 2
Claude-2, Anthropic's ChatGPT competitor was just released. It's cheaper, stronger, faster, can handle PDFs, and supports longer conversations.
    Claude is 5x cheaper than GPT-4.     It has more recent data. A a mix of websites, licensed data sets     from third parties and voluntarily-supplied user data from     early 2023.     It outperforms GPT4 on the GRE writing and HumanEval coding     benchmarks.     It features a context window of 100,000 tokens, the largest of any     commercially available model.     It can analyze roughly 75,000 words, about the length of "The Great     Gatsby\".     It can easily handle any code related tasks.
- ##### Llama 2
The new Llama 2 model from Meta looks initially exciting but is pretty mired in legal detail compared to the emerging open source community efforts.
- ##### License Grant
You are granted a non-exclusive, worldwide, non-transferable, royalty-free license to use, reproduce, distribute, modify, and create derivative works of Llama 2.
- ##### Attribution and Acceptable Use
You must retain the attribution notice in all copies of Llama 2. Your use must comply with Meta's Acceptable Use Policy, which prohibits illegal, deceptive, dangerous, or harmful uses.
- ##### Commercial Use and Model Improvement
You cannot use Llama 2 to improve any other large language model besides Llama 2. If your products or services have over 700 million monthly active users, you must obtain a separate license from Meta.
- ##### Disclaimer, Liability, and Ownership
Llama 2 is provided \"as is\" with no warranties. You assume all risks from use. Meta has no liability for damages arising from use of Llama 2. You own any derivative works and modifications you create, subject to Meta's ownership of Llama 2.
- ##### Termination and Risks
Meta can terminate the license if you breach it. You must delete Llama 2 on termination. Be aware of regulations like Article 28b of the AI Act in the EU. Do appropriate diligence to comply with laws and address risks around bias, fairness, transparency, and safety.
- ##### Key Takeaways
Understand attribution requirements, acceptable use policy, commercial use limits, disclaimer, risks, and ownership provisions. Seek legal counsel given complexities.
- ##### Roll your own trained LLM
This costs around \$500k to train something up from a trillion tokens that you bring to the party. This gets to 'last years' GPT3 level. It's too much, but it's worth being aware of. It's worth noting that Cerebras are offering access to their [Andromeda cluster](https://www.cerebras.net/press-release/cerebras-unveils-andromeda-a-13.5-million-core-ai-supercomputer-that-delivers-near-perfect-linear-scaling-for-large-language-models) which can train a significant model in the vein of Llama in around 11 days.
- ##### Wait for the Google integrations
I very strongly suspect that corporate level ML assistance is coming in force to the Google stack already employed at our work. This is [by far the most likely and pragmatic solution for the 'project planning assistant' business case] . Vertex AI cloud based generative art support shows the direction of travel in this regard.
    Advantages
    -   [[--] ]          our work already trusts Google with it's business data     -   [[--] ]          Single repository potential to leverage that fact     -   [[--] ]          Will likely be very cheap as a customer incentive. Currently         it's around 700 pages per dollar.     Disadvantages
    -   [[--] ]          GCHQ have [taken the unusual         step](https://www.ncsc.gov.uk/blog-post/chatgpt-and-large-language-models-whats-the-risk)         of warning that the data put into these systems goes into their         training and can thereby resurface in competitors searches later     -   [[--] ]          The likes of Apple and Samsung have banned the use of these         cloud tools as a result. There's anecdotal evidence of         commercially sensitive details surfacing, though it's hard to         validate these     -   [[--] ]          The products can change over time, in ways that are outside of         your control
With all that said there is potentially business advantage to learning how these systems work through doing.
- ##### Build something custom self hosted
Building something custom here means taking an open source model, with a permissive license. There's a lot of these now and they are 'decent'. It's possible to add in some code engineering around the edges to give it access to private datasets through a chat interface.
    [Advantages] 
    -   [[--] ]          It's private, local, under your control, and so you can trust         your data will be within the company walled garden     -   [[--] ]          It's building toward IP and knowledge, in the likely scenario         where GPT4 level models are less than 2 years away. This is real         internal investment     -   [[--] ]          There are NO legal repercussions to using it in a purely         off-line way. You don't even need to tell people you're doing         it. 'Probably' no GPDR, data governance, compliance overheads if         designed right.     [Disadvantages] 
    -   [[--] ]          It will still lie, and the company and individuals are still         responsible for the legal repercussions of acting on what the         model says.     -   [[--] ]          In a live deployment to the public it will occasionally say "bad         things". It's just impossible to control edge cases. Even         without the issues incurred by being online, the exposure around         that in terms of waivers is uncertain.     -   [[--] ]          Making it fit for purpose by adding memory is hard. The token         limits on these models are really small. They're just not that         smart yet.
Below is output from the top two models from the [global leader board](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). The second highest rated worked better (supercot). Into it I uploaded the 'International Code of Practice for Entertainment Rigging' which is a [meagre 35 page document](https://www.plasa.org/wp-content/uploads/2017/11/ICOPER_V1.0.pdf). I have done 500 pages in the past but the quality starts to break down with scale. Section 3.5 of the document says
You can see the output from the model running at home (without internet) in Figure [8.6](https://arxiv.org/html/2207.09460v11/#Ch8.F6 "Figure 8.6 ‣ Build something custom self hosted ‣ 8.2.8 Major trends in AI ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art"). It has royally embellished the facts, but it's actually delivering up good advice, and the memory injection from the uploaded document means it's specific enough to the context of the question.
![Figure 8.6: Results from home hosted LLM](../assets/ICOPERLLM.png) 
I have very much seen this when playing with both my own \"landmark attention\" models, and Claude. I expect they will find a way round it at some point. Because humans write with a focus on the beginning and end of documents, so large language models pay far more attention at the beginning and end of their context input \[[387](https://arxiv.org/html/2207.09460v11/#bib.bibx387)\].
That's tricky because if you load in a stack of PDFs and they get translated by the system into one big chunk (which may or may not be in the expected order of the PDFs), then the PDFs in the middle of the submission are subject to more hallucination, garbling, forgetting etc.
The \"other\" system, vector databases is more even handed, and so commensurately more predictable. Still got huge problems though.
I have seen both effects. They're currently both bad enough that I wouldn't trust document interrogation to these things. There are workarounds obviously: Ask for references, ask if to tell you if it thinks it's making it up, as for quotes, ask for locations, check the working, ask in many sperate ways etc.
These \"tips\" can be encoded into the preamble that goes into every query, as a standing command, so you don't have to, except for the checking bit of course. You can do that with either self host models or the web ones, but it's something you need to do often with the web interfaces. There's no silver bullet yet but it feels like months away, so I am not advocating learning these tricks. Might be better to wait for the fixes. Just don't trust them, these are the reasons, and they are all basically seeded in the way humans write.
There's a lot that can be done here, but the cost benefit is unclear. If this were sensitive internal planning documents, with a lot of complexity, then there's potentially a strong case, but we'd need tight procedures to check on it's homework. This tool, as usual, is best as a way to rapidly construct a framework.
As a side note there's a lot of open source tools like [SuperAGI](https://github.com/TransformerOptimus/SuperAGI) for 'Agents' and [AnythingLLM](https://github.com/Mintplex-Labs/anything-llm) for document analysis which straddle the line between using cloud vendors and local hosting. Building on one of these gives optionality, but they are new and 'flakey'.
- ##### LoRA training
LoRA stands for Low-Rank Adaptation and it's a cheap way to optimise models. A few hundred dollars of rented GPU time can nuance a model to be more performant for a specific task. It sounds great but basically you're optimising for a task that you need to understand very well, possibly/probably to the detriment of the generality of the tool. If there's something you know you want, then this is an option that's achievable and affordable.
Goat, a 7B LLaMA model finetuned for arithmetic tasks notably outperformed the  75x larger 540B PaLM model and GPT-4. Goat's success can be attributed to two primary factors: task-specific finetuning and LLaMA's unique digit tokenization. The problem here is Llama is arguably derived from data with a non-commercial license. OpenLlama gets around this with their Apache2 (do what you like) copy of the model. The situation is rapidly evolving. Another example of task-specific finetuning is the Gorilla project, where the LLM was trained to generate API calls. This is a really important area and we might be able to get ahead in this niche. This controlling complex whole site systems with voice control. The model was finetuned using 1,645 API calls from various sources and demonstrated superior performance compared to non-finetuned models. We can easily repeat that.
Recent findings suggest that less LoRA training gets better results, so this is increasingly being adopted as a way to improve business fit \[[388](https://arxiv.org/html/2207.09460v11/#bib.bibx388)\].
- ##### Enormous token limits
There's a few models now boasting staggering token input limits. With 1 million token windows it's possible that each query to the LLM can be 'the whole corporate database and filesystem' and then the question you want an answer to. This is a beguiling option, probably the most performant (though I have not played with one), and also pretty much useless as nobody yet offers a legally trustable way to upload your whole business into a chat window.
    [Advantages] 
    -   [[--] ]          Likely excellent at it's job     -   [[--] ]      -   [[--] ]      -   [[--] ]          Knows all about the business     [Disadvantages] 
    -   [[--] ]          Web based right now, and knows all about the business     -   [[--] ]          Not actually available, I'm on wait lists.     -   [[--] ]          Longer latency as everything will need uploading in the token         window (assuming you don't buy a service)
Very recently this in beginning to shift with the [emergence](https://github.com/eugenepentland/landmark-attention-qlora) of a locally run 'landmark attention' model. There are some issues with it at time of writing but this could be run on an our work cloud instance. I have 4000 tokens running at home. I am exploring tree of thought which needs these bigger windows and elevates reasoning by 70 percent \[[389](https://arxiv.org/html/2207.09460v11/#bib.bibx389)\].
- ##### Memory and other systems
There's many ways to use a database (in this case a 'vector' DB) to prime the context window with appropriate information. It's also potentially useful to look at combining older and well understood ML techniques like knowledge graphs in combination \[[390](https://arxiv.org/html/2207.09460v11/#bib.bibx390)\]. I've not personally had much luck with these yet.
- ##### Cost implication for self build
It's less than \$20k to build a system that can do anything you need, on site, rack mount in a ship, whatever. You can do it for less in the cloud, see the next section. You can also spend much, much more. Use-case dependent.
Here's the output from the 'local' open source model for clarity. Note it drifts off into nonsense toward the end then stops. This can be dealt with but I chose not to:
- ##### Build something custom in a private cloud
This is exactly the same as the previous section but you hire a private cloud system 'on demand' to do the work. This is [currently priced at](https://lambdalabs.com/service/gpu-cloud/pricing) \$1.10/hr and only costs you money when you're using it (though you have to shut it down yourself). This is both secure, and fairly cost effective. Also, it scales in that if you find a real serious application you can just get bigger rental GPUs and open a private/public interface. [It's my preferred path of all the private use cases except for mission critical on site stuff] , and edge cases like boats at sea etc. For that you need to buy GPUs. AMD have [recently announced](https://www.amd.com/en/newsroom/press-releases/2023-6-13-amd-expands-leadership-data-center-portfolio-with-.html) a partnership with opensource behemoth Huggingface to allow access to large and capable models like Falcon in their enormous new memory architecture. Falcon is from the UAE and has odd views on human rights. This is one to watch.
- #### 8.2.9 Whistlestop tour of terms
- ##### Transformers
Machine learning transformers are a groundbreaking architecture in the field of natural language processing (NLP) that have redefined tasks such as text generation, translation, and sentiment analysis. [This link](https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g13dd67c5ab8_0_2648) is a very technical but excellent overview of how they work. Transformers have gained popularity due to their ability to efficiently capture long-range dependencies and model complex relationships between words in a sentence.
At the core of the transformer architecture lies the self-attention mechanism. This mechanism allows the model to weigh the importance of each word in a sentence relative to the others, effectively capturing context and dependencies. In contrast to traditional neural networks, like recurrent neural networks (RNNs), transformers process input sequences in parallel, rather than sequentially. This parallel processing enables transformers to efficiently understand and remember long-range dependencies, which is particularly important in NLP tasks.
RNNs, on the other hand, process input sequences one element at a time, making it difficult for them to capture relationships between words that are far apart in a sentence. As a result, RNNs can struggle with tasks that involve complex sentences or require a deep understanding of context.
Transformer-based models, such as GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), have become the go-to models for many NLP tasks.
The layers contain weight matrices that are responsible for encoding the model's knowledge and language understanding.
- ##### GANs
Generative adversarial networks are used for generating synthetic data, and are incredibly useful for our fine tuning use cases. GANs consist of two neural networks that are trained to compete with each other, with one network generating synthetic data and the other trying to distinguish between the synthetic data and real data. This process allows GANs to learn the underlying distribution of the data and generate samples that are highly realistic.
Reinforcement learning is a type of machine learning that involves an agent learning through trial and error in order to maximize a reward.
- ##### LoRA
LoRA, or Low-Rank Adaptation \[[391](https://arxiv.org/html/2207.09460v11/#bib.bibx391)\], is a technique that enables efficient adaptation of large language models to specific tasks or domains while maintaining their expressive power. It does so by introducing a small modification to the pre-trained model's weight matrices, enabling the fine-tuning process to be more computationally efficient without sacrificing performance. Visually you can think of this as slipping modification layers in between the transformer layers, which are far more interdependent and thereby expensive to retrain. We are already experimenting with these systems for our use cases.
- ##### Embeddings and Latent Space
Embeddings play a crucial role in both generative AI art and large language models, as they provide a way to represent complex data types, such as text or images, in a continuous vector space. In both contexts, embeddings capture the underlying structure and semantics of the input data, enabling AI models to learn and generate new content based on these representations. This is what the user sees happening with both AI generative art, and LLMs.
In the context of generative AI art, embeddings are often used to represent visual elements, such as images or shapes. Latent space is a continuous vector space where each point corresponds to an embedding that encodes the features and semantics of an image. Once the AI model is trained, new images can be generated by sampling points from this latent space and decoding them back into the image domain. Embeddings can similarly represent styles or artistic techniques. Style transfer techniques, for example, utilize embeddings to extract and apply the style of one image to the content of another.
In the context of large language models, embeddings are used to represent words, phrases, or sentences in a continuous vector space. These models are trained on vast amounts of text data, learning to generate contextually relevant and semantically meaningful embeddings for language. Similarly to the visual use case in generative art these embeddings capture various aspects of language, such as syntax, semantics, and relationships between words or phrases. Once trained, the large language models can generate new text by sampling from the distribution of embeddings and decoding them back into the text domain.
Embeddings are also used in tasks like sentiment analysis, machine translation, and text classification, where the AI model must understand the meaning and context of the input text.
- ##### Vector databases
One of the highest points of human 'friction' when dealing with and AI model, and especially LLMs is the lack of a persistent and/or contextual memory within the systems. This is beginning to be addressed using vector databases. A vector database is designed to efficiently store, manage, and query the high-dimensional vectors, often used in the context of machine learning and artificial intelligence. These high-dimensional vectors are the embeddings previously discussed.
Using a vector database with embeddings for AI data retrieval and processing can significantly improve efficiency, scalability, and performance. In the context of a stored item of data, embeddings allow the storage of complex 'concepts' as fixed-length vector, which interacts with the enormous latent space in the trained model. This makes storage and retrieval more efficient.
Vector databases allow and optimise for efficient nearest neighbor search, which is crucial for data retrieval tasks in AI applications. To do this, given a query input, the AI system first converts the input into an embedding using the same technique as for the stored data. The vector database then performs a nearest neighbour search to find the most similar embeddings in the database. At scale this can result in more consistency when using models, but crucially it doesn't train the models on events that have happened. It is not a 'memory'.
- ##### Memory Streams
In the paper 'Generative Agents: Interactive Simulacra of Human Behavior' Park et al present a solution and working example for the problem of contextual memory in AI systems \[[392](https://arxiv.org/html/2207.09460v11/#bib.bibx392)\]. This is a pretty stunning paper for our purposes in collaborative XR where we would hope to interact with virtual agents.
As they point out in the paper virtual agents should be able to manage constantly-growing memories and handle cascading social dynamics that unfold between multiple agents. Their architecture uses a large language model to generate a memory stream, reflection, and planning. The memory stream contains a comprehensive list of the agent's experiences (written as a kind of internal monologue), and the planning module synthesizes higher-level inferences over time. These memory transcriptions are highly compressible and would be excellent as RGB style private data blobs between our federated virtual worlds. It will therefore me possible to 'meet' virtual agent friends across instances of virtual spaces [and]  through nostr social media. This is a key technology for our uses now.
- ##### Gradient descent
Gradient descent is an optimization algorithm widely used in machine learning and deep learning, including large language models, to minimize a loss function. The loss function measures the difference between the predicted output and the actual output (also known as the target) for a given input. The goal of the training process is to [minimize this loss](https://societyofai.medium.com/gradient-descent-basics-and-application-1cef98179ee6) to improve the model's performance.
In the context of large language models, gradient descent helps to adjust the model's parameters (weights and biases) so that it can generate more accurate predictions. These models consist of multiple layers of neurons with a large number of parameters that need to be fine-tuned. It is this training process that takes so much time and energy.
    Initialize parameters: The model's parameters are initially set to     random values. These parameters are then iteratively adjusted using     gradient descent.     Calculate loss: For a given input and target, the model generates a     prediction, and the loss function calculates the difference between     the prediction and the target.     Compute gradients: The gradients of the loss function with respect     to each parameter are computed. A gradient is a vector that points     in the direction of the greatest increase of the function, and its     components are the partial derivatives of the function with respect     to each parameter. The gradients indicate how much each parameter     contributes to the loss.     Update parameters: The model's parameters are updated using the     gradients. This is done by subtracting a fraction of the gradient     from the current parameter value. The fraction is determined by a     hyperparameter called the learning rate. A smaller learning rate     results in smaller updates and slower convergence, while a larger     learning rate can result in faster convergence but might overshoot     the optimal values.     Iterate: Steps 2-4 are repeated for a certain number of iterations,     a specified tolerance, or until convergence is reached (i.e., when     the change in the loss function becomes negligible).     Gradient descent has several variations, such as Stochastic Gradient     Descent (SGD) and mini-batch gradient descent. These methods differ     in how they use the training data to compute the gradients and     update the parameters. In SGD, the gradients are computed and the     parameters are updated using only one data point at a time, while in     mini-batch gradient descent, a small batch of data points is used to     compute the gradients and update the parameters. LLMs like GPT can     use meta optimisers to train as they operate     \[[393](https://arxiv.org/html/2207.09460v11/#bib.bibx393)\].
- ##### TPUs
Tensor Processing Units (TPUs) are specialized hardware accelerators for machine learning workloads, developed by Google. TPUs are designed to speed up the training and inference of machine learning models, particularly large deep neural networks. They are highly parallel and optimized for low-precision arithmetic, which allows them to perform computations much faster than traditional CPUs or GPUs. TPUs can be used in a variety of machine learning applications, such as natural language processing, computer vision, and speech recognition. Google has integrated TPUs into its cloud platform, allowing developers to easily use them for their machine learning workloads. Overall, TPUs provide a powerful and efficient platform for machine learning. The top of the line Nvidia tensorflow unit at this time is the v4, and it is comparable if more generalised hardware.
- ##### Tensorflow
TensorFlow is a popular open-source machine learning framework developed by Google and was instrumental in kicking off a lot of this research area. It is still widely used for training and deploying machine learning models in a variety of applications, such as natural language processing, computer vision, and speech recognition, but is being somewhat superceded by JAX. The consensus seems to be that JAX itself is more specialised and harder to use, but works well with Googles hardware cloud systems. Time will tell if this upgrade gets community traction. TensorFlow provides a flexible and high-performance platform for building and deploying machine learning models. It allows users to define, train, and evaluate models using a variety of deep learning algorithms, such as convolutional neural networks and recurrent neural networks. TensorFlow also has a strong emphasis on scalability and performance, with support for distributed training and deployment on a variety of platforms, including GPUs and TPUs. Overall, TensorFlow is a powerful tool for building and deploying machine learning models.
- ##### PyTorch
PyTorch is a popular open-source machine learning framework developed by Facebook's AI research group. It is primarily used for applications such as natural language processing and computer vision. PyTorch is based on the Torch library and provides two high-level features: tensor computations with strong GPU acceleration and deep neural networks built on a tape-based autograd system. PyTorch offers a variety of tools and libraries for machine learning, including support for computer vision, natural language processing, and generative models. It also allows for easy and seamless interaction with the rest of the Python ecosystem, including popular data science and machine learning libraries such as NumPy and scikit-learn.
- ##### NumPy
NumPy is a popular open-source library for scientific computing in Python. It provides a high-performance multidimensional array object, as well as tools for working with these arrays. NumPy's array class is called ndarray, which is a flexible container for large datasets that can be processed efficiently. The library provides a wide range of mathematical functions that can operate on these arrays, including linear algebra operations, Fourier transforms, and random number generation. NumPy also has a powerful mechanism for integrating C, C++, and Fortran code, which allows it to be used for high-performance scientific computing in a variety of applications. Overall, NumPy is an essential library for working with numerical data in Python.
- ##### Latent space
In the context of generative artificial intelligence (AI), a latent space is a high-dimensional space in which the model represents data as points. This space is \"latent\" because it is not directly observed, but is inferred by the model based on the data it is trained on. In the case of a generative model, the latent space is often used to encode the underlying structure of the data, such that samples can be generated by sampling from the latent space and then decoding them into the data space.
For example, in a generative model for images, the latent space may encode the features or characteristics of the image, such as the shape, color, and texture. By sampling from this latent space and decoding the sample, the model can generate new images that are similar to the training data, but are not exact copies. This allows the model to generate novel and diverse samples that capture the essence of the training data.
The latent space is an important aspect of generative models because it allows the model to capture the underlying structure of the data in a compact and efficient way. It also provides a way to control the generation process, such as by interpolating between latent space points to generate smooth transitions between samples. At this time the navigation through that mathematical space is steered by vectors into the space, which come from a separate and parallel integration of a natural language model. This crucial bridge came from research at OpenAI, and has been instrumental in the current explosion of usability of the systems \[[394](https://arxiv.org/html/2207.09460v11/#bib.bibx394)\].
- ##### Edge AI compute and APUs
    Nvidia's [latest in the     Jetson](https://www.okdo.com/p/nvidia-jetson-agx-orin-64gb-developer-kit/)     Edge AGX line is a high performance general AI unit for industrial     applications     Microsoft are rumoured to be looking to mitigate the staggering     costs of running ChatGPT (\$1M/day) using forthcoming [hardware of     their own     design](https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge?)     [Cerebras systems](https://www.cerebras.net/) have built an AI     architecture from the ground up and claim incredible numbers.
These systems will drive the compute to less 'constrained' but somewhat less capable AI systems, distributing the access but increasing risks.
- ##### Prompt engineering
The art of prompting constrains the generation space into documents that contain correct answers.
A better performance in few-shot prompting can be achieved by constructing fictional scenarios.
Using flattery or painting a clear fictional narrative, such as referring to the LLM as a \"masterful French translator,\" can help to guide the model towards producing the desired output.
Much of the initial prompt engineering involves constructing scenarios that can only be completed correctly.
A good strategy often involves imagining what kind of document might contain the correct answer.
Fine-tuning models for tasks helps make the model more capable and more able to do as it's told.
Fine-tuning involves giving many examples of a task being done correctly, resulting in a model that acts almost as though it had been prompted by thousands of correct examples. Use of assisted learning: Working with assistants or partners on the project can help identify problems and improve the model.
Correct absurdity instead of playing along: Even if the question seems absurd, the model should provide an answer grounded in reality instead of going along with the absurdity.
Satisfaction of preference model: The model should aim to fulfill a preference model emulating what a human would want.
Consciousness of self: The model should be conscious in a sense of what it is, what it's doing, and where it's situated in the world.
Avoidance of interpolation: Instead of just interpolating what humans might do, the model should do something other than that.
Use of instruction tuning: This makes the desired outcome of the model clearer.
Reinforcement learning with a reward model: This technique takes the results to another level and provides feedback to improve the model.
Avoidance of mode collapse: The model should avoid fixating on a particular way of answering.
- #### 8.2.10 Evaluation Metrics for Language Models
["The key questions of the debate about understanding in LLMs are the following: 1) Is talking of understanding in such systems simply a category error, mistaking associations between language tokens for associations between tokens and physical, social, or mental experience? In short, is it the case that these models are not, and will never be, the kind of things that can understand? Or conversely, 2) do these systems (or will their near-term successors) actually, even in the absence of physical experience, create something like the rich concept-based mental models that are central to human understanding, and, if so, does scaling these models create ever better concepts? Or, 3) if these systems do not create such concepts, can their unimaginably large systems of statistical correlations produce abilities that are functionally equivalent to human understanding? Or, indeed, that enable new forms of higher-order logic that humans are incapable of accessing? And at this point will it still make sense to call such correlations "spurious" or the resulting solutions "shortcuts?" And would it make sense to see the systems' behavior not as "competence without comprehension" but as a new, nonhuman form of understanding? These questions are no longer in the realm of abstract philosophical discussions but touch on very real concerns about the capabilities, robustness, safety, and ethics of AI systems that increasingly play roles in humans' everyday lives."] 
- ##### Debate on True Understanding and Intelligence
Melanie Mitchell highlights the ongoing debate about whether large language models like GPT truly understand and are intelligent. Some experts see their abilities as merely superficial, mimicking comprehension without truly understanding. On the other side, some argue that these models show signs of genuine comprehension, albeit limited.
- ##### Lack of Sensory Grounding
Understanding in humans is deeply grounded in sensory experiences that inform flexible and adaptable mental models. Unlike humans, language models lack this essential grounding, which questions their capacity for true understanding.
- ##### Need for Revised Evaluation Metrics
Mitchell emphasizes the need to rethink the metrics used for evaluating AI systems. Current benchmarks often focus on aggregate performance, which can easily overlook failure modes and obscure the actual mechanisms of action within the model.
- ##### Granular Testing and Abstract Generalization
To properly assess the capabilities of these models, more rigorous and granular testing methods are essential. Testing should focus on tasks like abstract generalization to probe their true capabilities. Mitchell advocates for an experimental science of machine cognition to fuel progress in this area.
- ##### Risks of Anthropomorphism
Mitchell warns against the risks involved in anthropomorphizing machine intelligence. Intelligence is not a universal, unlimited capacity; it is adapted to solve specific problems in specific environments.
- ##### Human-Machine Performance Gap
Benchmark performance should not be directly equated between humans and machines. Just because an AI system can pass a standardized test does not mean it possesses human-like generalization abilities.
- ##### Importance of Reporting Failures
Reporting instance-level failures is crucial to understanding the behavior of these models. Aggregate metrics often mask these failures, leading to an inflated sense of capability.
- ##### Multidimensional Understanding
Understanding is a complex, multidimensional concept. When evaluating machine cognition, it's important to specify what aspects are being tested and to avoid making category errors in attribution.
- ##### Interdisciplinary Collaboration
Mitchell encourages collaborations with fields like psychology and cognitive science to bring experimental rigor into the evaluation of machine cognition.
- ##### Ultimate Goal
The end goal, according to Mitchell, should be to develop AI systems that act as beneficial, truth-seeking \"thinking machines,\" rather than systems that display blind competency without understanding.
Evaluating the performance of language models involves a multi-faceted approach that considers both unsupervised and supervised metrics, along with human evaluation, bias and safety factors, and efficiency. The following sections expand on these aspects.
- ##### Perplexity
Perplexity measures the likelihood that the model will predict the next word in a sequence correctly. Mathematically, it can be expressed as:
  -- ------------------------------------------------------------------------------------- --      $$\text{Perplexity} = 2^{- {\frac{1}{N}{\sum_{i = 1}^{N}{{\log_{2}q}{(x_{i})}}}}}$$      -- ------------------------------------------------------------------------------------- --
where $N$ is the length of the text and $q{(x_{i})}$ is the predicted probability of each word $x_{i}$.
- ##### Diversity
Diversity can be quantified using Self-BLEU scores and unique n-grams. These metrics help in understanding the richness and uniqueness of the generated text. Higher diversity scores suggest a less repetitive and more novel generation.
- ##### Supervised Evaluation
    [Accuracy:]  The ratio of correct predictions     to the total number of samples in a classification test set.     [F1 Score:]  The harmonic mean of precision     and recall, used primarily in classification tasks.     [BLEU:]  Stands for Bilingual Evaluation     Understudy. Measures the similarity between candidate translations     and reference translations using n-gram overlap.     [ROUGE:]  Metrics designed to compare the     similarity between generated summaries and reference summaries.     [Confidence:]  Reflects the model's predicted     confidence on test examples. This is often calibrated using metrics     like Expected Calibration Error (ECE).
- ##### Human Evaluation
Human evaluation involves subjective assessment on various qualitative aspects such as:
 ###### Bias and Safety
    [Toxicity:]  The rate at which the model     generates language that can be considered toxic, hateful, or biased.     [Stereotyping:]  Measures the model's     propensity for unfair generalizations and stereotypes.
- ##### Efficiency Metrics
    [Parameters:]  Indicates the size and     complexity of the model. Fewer parameters usually mean more     efficiency.     [FLOPs:]  The number of Floating Point     Operations performed by the model.     [Latency:]  Measures the time required to     generate an output.     [Power Usage:]  Quantifies the energy consumed     during the inference phase. Lower energy consumption is preferable.
- #### 8.2.11 Consumer tools
Gozalo-Brizuela and Garrido-Merchan provide a helpful review and taxonomy of recent generative ML systems in their paper 'ChatGPT is not all you need' \[[395](https://arxiv.org/html/2207.09460v11/#bib.bibx395)\], with Figure [8.7](https://arxiv.org/html/2207.09460v11/#Ch8.F7 "Figure 8.7 ‣ 8.2.11 Consumer tools ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art") showing some of the main categories and systems.
![Figure 8.7: Taxonomy of [recent generative ML systems](https://arxiv.org/abs/2301.04655) by Gozalo-Brizuela and Garrido-Merchan used with permission.](../assets/MLtaxonomy.jpg) 
- ##### ChatGPT
ChatGPT is a neural network-based natural language processing (NLP) model developed by OpenAI. It is a continuation of the OpenAI programme which binds iteratively more capable Generative Pre-trained Transformer models to a web and API based text chat interface. It uses self-attention mechanisms to generate high-quality text in a variety of different languages. ChatGPT is specifically designed for conversational text generation, and has been trained on a large corpus of dialogue data in order to produce responses that are natural, diverse, and relevant to a given conversation. Because it is a large language model, ChatGPT has a vast amount of knowledge and can generate responses to a wide range of questions and prompts and meta-prompts \[[396](https://arxiv.org/html/2207.09460v11/#bib.bibx396)\]. This allows it to generate responses that are relevant, natural-sounding, and diverse in nature. It has proved incredibly popular, demonstrating uncanny abilities for natural conversation, code generation, copy writing and more. It is substantially flawed in that it 'speaks' with authority but often makes things up completely. This extended recently to creating academic references to back it's assertions, completely out of thin air. The interface and APIs seem to be evolving and improving in real time.
The model uses a transformer-based architecture, which means that it consists of a series of interconnected "blocks" that process the input data and generate the output text. Each block contains multiple self-attention mechanisms, which allow the model to focus on different aspects of the input data and generate a response that is coherent and relevant to the conversation. In addition to its transformer-based architecture, ChatGPT also uses a variety of other techniques to improve its performance. For example, it uses beam search to generate multiple candidate responses for each input, and then selects the best one based on a combination of factors such as relevance, coherence, and diversity. This allows the model to generate high-quality responses that are appropriate for a given conversation. Additionally, ChatGPT uses a technique called "response conditioning" to bias the model towards generating responses that are appropriate for a given conversation context. This allows the model to generate more relevant and coherent responses, even when faced with challenging input data. Microsoft have [integrated GPT4](https://medium.com/@owenyin/scoop-oh-the-things-youll-do-with-bing-s-chatgpt-62b42d8d7198) with Bing, their internet search engine, and plugins for other websites are coming soon.
One key aspect of GPT-4 is reinforcement learning with human feedback (RLHF), which helps align the model with human preferences, making it more useful and easier to interact with. The process involves using human feedback to fine-tune the model, requiring relatively less data compared to the initial training phase. The development of GPT-4 involves multiple components: the design of neural network algorithms, data selection, and human supervision through RLHF. Researchers have made significant progress in understanding the behavior of the fully trained system using evaluation processes, although the complete understanding of the model remains a challenge. GPT-4 has the ability to perform 'reasoning' based on the knowledge it has gained from the training data. While some interactions with the model may display wisdom, others might lack it. The dialog format used in the model enables it to answer follow-up questions, admit mistakes, challenge incorrect premises, and reject inappropriate requests. In a recent report, researchers revealed groundbreaking advancements in GPT-4, hinting at sparks of artificial general intelligence \[[397](https://arxiv.org/html/2207.09460v11/#bib.bibx397)\]. The Microsoft researchers had access to the unrestrained GPT-4 during its early development, allowing them to experiment for around six months.
    GPT-4 can use tools with minimal instruction, displaying an emergent     capability to utilize calculators, character APIs, and text-to-image     rendering.     GPT-4 can pass mock technical interviews on LeetCode and could     potentially be hired as a software engineer.     When tasked with creating a complex 3D game, GPT-4 produces a     working game in a zero-shot fashion.     GPT-4 can tackle the 2022 International Mathematics Olympiad,     demonstrating a high level of mathematical ability.     It can answer Fermi questions, which are complex questions often     used in difficult interviews.     GPT-4 can serve as an AI personal assistant, coordinating with     others over email, booking events, and managing calendars.     It can help diagnose and solve real-life problems, such as fixing a     leak in a bathroom.     GPT-4 can build mental maps of physical locations, which may be     useful when embodied.     It displays a theory of mind, capable of understanding what others     may be thinking or believing about a situation.
There are obviously still limitations to GPT-4. As an autoregressive model, it cannot plan ahead and struggles with discontinuous tasks. This issue could potentially be addressed by augmenting language models with external memory. The paper raises concerns about the unrestricted GPT-4's ability to create propaganda and conspiracy theories, and the ethical implications of giving AI intrinsic motivation. The researchers call for a better understanding of AI systems like GPT-4 to address these challenges.
When asked about controversial figures or topics, GPT-4 can provide nuanced and balanced answers, highlighting its potential to reintroduce nuance to conversations. It is important to consider AI safety and alignment when developing powerful models like GPT-4. After its completion, the model underwent extensive internal and external testing, including red teaming and safety evaluations, to ensure alignment with human values. To ensure that AI systems align with various human values, it is necessary to establish broad societal boundaries, which may differ across countries and individual users. The art of crafting effective prompts for GPT-4 involves understanding the model's behavior and composing prompts in a way that elicits the desired response. This process is similar to human conversation, where individuals adapt their phrasing to communicate more effectively. As the model becomes more advanced, it may increasingly resemble human interactions, which can offer insights into human communication and behaviour.
Although impressive, it is generally agreed that GPT-4 is not an AGI due to its limitations in approximating human-level intelligence. The concept of consciousness in AI is debated, with some believing AI can be conscious, while others argue that AI can convincingly fake consciousness without being truly aware. Potential risks associated with even a 'simply' and unconcious AI include disinformation, economic shocks, and geopolitical impacts. These concerns do not necessarily require superintelligence but could result from large-scale deployment of powerful language models without proper safety controls.
- ##### GPT API and programming
In the context of programming, GPT-4's advancements may have a significant impact on how developers interact with AI systems,, and their productivity and creativity. The impact of AI systems like GPT-4 on programming is significant, changing the way programmers work by allowing an iterative process and back-and-forth dialogue with the AI as a creative partner. This development is seen as a major step forward in programming.
It seems almost inevitable that at some stage a large language model will be optimised for computer instruction sets, and simply be able to bridge directly from human intentionality to bytecode, running it's own tests and refinements without external consultation. The degree to which this would still be considered 'a compiler' is unclear.
- ##### ChatGPT - Advanced Data Analysis
The ChatGPT Code Interpreter Plugin, introduced in March 2023, offers a sandboxed environment featuring a working Python interpreter. This environment, which is isolated from other users and the Internet, supports an impressive array of functionalities. It comes pre-loaded with over 330 libraries, including popular ones such as pandas, matplotlib, seaborn, and TensorFlow, among others.
As illustrated in Figure [8.8](https://arxiv.org/html/2207.09460v11/#Ch8.F8 "Figure 8.8 ‣ Current LLM Capabilities ‣ 8.2.11 Consumer tools ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art"), the Code Interpreter Plugin is capable of performing a myriad of tasks. For example, it can visualize any data inputted by the user, generate GIFs of the visualizations, and perform file uploads and downloads. It can extract colors from an image to create a color palette, and autonomously compress large images when memory is running low. Moreover, the plugin can clean and process data, generate insightful visualizations, and convert files to different formats quickly and efficiently.
The Code Interpreter Plugin can be installed by ChatGPT Plus users in a few simple steps. However, it is worth noting that while this plugin is powerful, it does have certain limitations, such as frequent environment resets, limited Optical Character Recognition (OCR) capabilities, and an inability to access the web. Despite these limitations, OpenAI continues to work on improving the capabilities of the Code Interpreter Plugin, promising a future with substantial impacts on the world of programming.
    The Code Interpreter Plugin introduces a sandbox and an advanced     language model, both of which are critical to its functionality.     The emphasis of the plugin is on the quality of the model, which can     generate code, debug it, and even decide when not to proceed without     human input.     The plugin offers substantial model autonomy, enabling it to work     through multiple steps of code generation autonomously.     Despite its powerful capabilities, the plugin does have limitations,     such as frequent environment resets and limited OCR capabilities.     The plugin is only available to ChatGPT Plus users, and requires a     few simple steps for installation.     The Code Interpreter Plugin represents a significant advancement in     the realm of programming, changing the way programmers interact with     AI systems.
- ##### CodeLlama
Meta is now preparing to launch CodeLama, an open source code generating model to rival OpenAI's Codex and Microsoft's GitHub Copilot. This could disrupt the industry.
- ##### Google Gemini
- ##### Current LLM Capabilities
    [GPT-4]  - Best for difficult reasoning tasks     based on comparisons with Claude 2. Most advanced reasoning of     current LLMs.     [Claude 2]  - 100k token context window allows     ingesting entire documents and books. Useful for QA and     summarization with large knowledge bases.     [Bard]  - Integrated with web like Google     Search. New multimodal features understand images. 40 language     support.     [Codex]  - Unlocks new abilities like running     and refining code iteratively. Suspected separate model accessed via     API.     [Pi]  - Focused on being a friendly personal     assistant. Asks followup questions to continue conversations.     [LLaMA 2]  - Open source alternative to GPT-4     from Meta. Could pose threat as safer open LLM.     [Personal LLMs]  - Allow customization with     your own data. Create personalized AI assistants.
This is the list of libraries that code interpreter can call.
  ------------------------------------------------------------------- -------------------------------------------------------------------------- ------------------------------------------------------------------- -------------------------------------------------------------------   [absl-py==1.4.0]                    [affine==2.4.0]                            [aiohttp==3.8.1]                    [aiosignal==1.3.1]    [analytics-python==1.4.post1]       [anyio==3.7.1]                             [anytree==2.8.0]                    [argcomplete==1.10.3]    [argon2-cffi-bindings==21.2.0]      [argon2-cffi==21.3.0]                      [arviz==0.15.1]                     [asttokens==2.2.1]    [async-timeout==4.0.2]              [attrs==23.1.0]                            [audioread==3.0.0]                  [babel==2.12.1]    [backcall==0.2.0]                   [backoff==1.10.0]                          [backports.zoneinfo==0.2.1]         [basemap-data==1.3.2]    [basemap==1.3.2]                    [bcrypt==4.0.1]                            [beautifulsoup4==4.12.2]            [bleach==6.0.0]    [blinker==1.6.2]                    [blis==0.7.9]                              [bokeh==2.4.0]                      [branca==0.6.0]    [brotli==1.0.9]                     [cachetools==5.3.1]                        [cairocffi==1.6.0]                  [cairosvg==2.5.2]    [camelot-py==0.10.1]                [catalogue==2.0.8]                         [certifi==2019.11.28]               [cffi==1.15.1]    [chardet==4.0.0]                  [charset-normalizer==2.1.1]              [click-plugins==1.1.1]            [click==8.1.4]    [cligj==0.7.2]                    [cloudpickle==2.2.1]                     [cmudict==1.0.13]                 [comm==0.1.3]    [compressed-rtf==1.0.6]           [countryinfo==0.1.2]                     [cryptography==3.4.8]             [cssselect2==0.7.0]    [cycler==0.11.0]                  [cymem==2.0.7]                           [dbus-python==1.2.16]             [debugpy==1.6.7]    [decorator==4.4.2]                [defusedxml==0.7.1]                      [deprecat==2.1.1]                 [dill==0.3.6]    [distro-info==0.23ubuntu1]        [dlib==19.22.1]                          [dnspython==2.3.0]                [docx2txt==0.8]    [ebcdic==1.1.1]                   [ebooklib==0.18]                         [einops==0.3.2]                   [email-validator==2.0.0.post2]    [entrypoints==0.4]                [et-xmlfile==1.1.0]                      [exceptiongroup==1.1.2]           [exchange-calendars==3.4]    [executing==1.2.0]                [extract-msg==0.28.7]                    [faker==8.13.2]                   [fastapi==0.92.0]    [fastjsonschema==2.17.1]          [fastprogress==1.0.3]                    [ffmpeg-python==0.2.0]            [ffmpy==0.3.0]    [filelock==3.12.2]                [fiona==1.8.20]                          [flask-cachebuster==1.0.0]        [flask-cors==4.0.0]    [flask-login==0.6.2]              [flask==2.3.2]                           [folium==0.12.1]                  [fonttools==4.40.0]    [fpdf==1.7.2]                     [frozenlist==1.3.3]                      [future==0.18.3]                  [fuzzywuzzy==0.18.0]    [gensim==4.1.0]                   [geographiclib==1.52]                    [geopandas==0.10.2]               [geopy==2.2.0]    [gradio==2.2.15]                  [graphviz==0.17]                         [gtts==2.2.3]                     [h11==0.14.0]    [h2==4.1.0]                       [h5netcdf==1.1.0]                        [h5py==3.4.0]                     [hpack==4.0.0]    [html5lib==1.1]                   [httpcore==0.17.3]                       [httptools==0.6.0]                [httpx==0.24.1]    [hypercorn==0.14.3]               [hyperframe==6.0.1]                      [idna==2.8]                       [imageio-ffmpeg==0.4.8]    [imageio==2.31.1]                 [imapclient==2.1.0]                      [imgkit==1.2.2]                   [importlib-metadata==6.7.0]    [importlib-resources==5.12.0]     [iniconfig==2.0.0]                       [ipykernel==6.24.0]               [ipython-genutils==0.2.0]    [ipython==8.12.2]                 [isodate==0.6.1]                         [itsdangerous==2.1.2]             [jax==0.2.28]    [jedi==0.18.2]                    [jinja2==3.1.2]                          [joblib==1.3.1]                   [json5==0.9.14]    [jsonpickle==3.0.1]               [jsonschema-specifications==2023.6.1]    [jsonschema==4.18.0]              [jupyter-client==7.4.9]    [jupyter-core==5.1.3]             [jupyter-server==1.23.5]                 [jupyterlab-pygments==0.2.2]      [jupyterlab-server==2.19.0]    [jupyterlab==3.4.8]               [keras==2.6.0]                           [kerykeion==2.1.16]               [kiwisolver==1.4.4]    [korean-lunar-calendar==0.3.1]    [librosa==0.8.1]                         [llvmlite==0.40.1]                [loguru==0.5.3]    [lxml==4.9.3]                     [markdown2==2.4.9]                       [markdownify==0.9.3]              [markupsafe==2.1.3]    [matplotlib-inline==0.1.6]        [matplotlib-venn==0.11.6]                [matplotlib==3.4.3]               [mistune==3.0.1]    [mizani==0.9.2]                   [mne==0.23.4]                            [monotonic==1.6]                  [moviepy==1.0.3]    [mpmath==1.3.0]                   [mtcnn==0.1.1]                           [multidict==6.0.4]                [munch==4.0.0]    [murmurhash==1.0.9]               [mutagen==1.45.1]                        [nashpy==0.0.35]                  [nbclassic==1.0.0]    [nbclient==0.8.0]                 [nbconvert==7.6.0]                       [nbformat==5.9.0]                 [nest-asyncio==1.5.6]    [networkx==2.6.3]                 [nltk==3.6.3]                            [notebook-shim==0.2.3]            [notebook==6.5.1]    [numba==0.57.1]                   [numexpr==2.8.4]                         [numpy-financial==1.0.0]          [numpy==1.21.2]    [odfpy==1.4.1]                    [olefile==0.46]                          [opencv-python==4.5.2.54]         [openpyxl==3.0.10]    [opt-einsum==3.3.0]               [orjson==3.9.1]                          [packaging==23.1]                 [pandas==1.3.2]    [pandocfilters==1.5.0]            [paramiko==3.2.0]                        [parso==0.8.3]                    [pathy==0.10.2]    [patsy==0.5.3]                    [pdf2image==1.16.3]                      [pdfkit==0.6.1]                   [pdfminer.six==20200517]    [pdfplumber==0.5.28]              [pdfrw==0.4]                             [pexpect==4.8.0]                  [pickleshare==0.7.5]    [pillow==8.3.2]                   [pip==20.0.2]                            [pkgutil-resolve-name==1.3.10]    [platformdirs==3.8.0]    [plotly==5.3.0]                   [plotnine==0.10.1]                       [pluggy==1.2.0]                   [pooch==1.7.0]    [preshed==3.0.8]                  [priority==2.0.0]                        [proglog==0.1.10]                 [prometheus-client==0.17.0]    [prompt-toolkit==3.0.39]          [pronouncing==0.2.0]                     [psutil==5.9.5]                   [ptyprocess==0.7.0]    [pure-eval==0.2.2]                [py==1.11.0]                             [pyaudio==0.2.11]                 [pycountry==20.7.3]    [pycparser==2.21]                 [pycryptodome==3.18.0]                   [pydantic==1.8.2]                 [pydot==1.4.2]    [pydub==0.25.1]                   [pydyf==0.7.0]                           [pygments==2.15.1]                [pygobject==3.36.0]    [pygraphviz==1.7]                 [pylog==1.1]                             [pyluach==2.2.0]                  [pymc3==3.11.5]    [pymupdf==1.19.6]                 [pynacl==1.5.0]                          [pypandoc==1.6.3]                 [pyparsing==3.1.0]    [pypdf2==1.28.6]                  [pyphen==0.14.0]                         [pyproj==3.5.0]                   [pyprover==0.5.6]    [pyshp==2.1.3]                    [pyswisseph==2.10.3.2]                   [pytesseract==0.3.8]              [pytest==6.2.5]    [pyth3==0.7]                      [python-apt==2.0.1+ubuntu0.20.4.1]       [python-dateutil==2.8.2]          [python-docx==0.8.11]    [python-dotenv==1.0.0]            [python-multipart==0.0.6]                [python-pptx==0.6.21]             [pyttsx3==2.90]    [pytz==2023.3]                    [pywavelets==1.4.1]                      [pyxlsb==1.0.8]                   [pyyaml==6.0]    [pyzbar==0.1.8]                   [pyzmq==25.1.0]                          [qrcode==7.3]                     [rarfile==4.0]    [rasterio==1.2.10]                [rdflib==6.0.0]                          [referencing==0.29.1]             [regex==2023.6.3]    [reportlab==3.6.1]                [requests-unixsocket==0.2.0]             [requests==2.31.0]                [resampy==0.4.2]    [rpds-py==0.8.8]                  [scikit-image==0.18.3]                   [scikit-learn==1.0]               [scipy==1.7.3]    [seaborn==0.11.2]                 [semver==3.0.1]                          [send2trash==1.8.2]               [sentencepiece==0.1.99]    [setuptools==45.2.0]              [shap==0.39.0]                           [shapely==1.7.1]                  [six==1.14.0]    [slicer==0.0.7]                   [smart-open==6.3.0]                      [sniffio==1.3.0]                  [snuggs==1.4.7]    [sortedcontainers==2.4.0]         [soundfile==0.10.2]                      [soupsieve==2.4.1]                [spacy-legacy==3.0.12]    [spacy==3.1.7]                    [speechrecognition==3.8.1]               [srsly==2.4.6]                    [stack-data==0.6.2]    [starlette==0.25.0]               [statsmodels==0.12.2]                    [svglib==1.1.0]                   [svgwrite==1.4.1]    [sympy==1.8]                      [tables==3.6.1]                          [tabula==1.0.5]                   [tabulate==0.8.9]    [tenacity==8.2.2]                 [terminado==0.17.1]                      [text-unidecode==1.3]             [textblob==0.15.3]    [textract==1.6.4]                 [theano-pymc==1.1.2]                     [thinc==8.0.17]                   [threadpoolctl==3.1.0]    [tifffile==2023.7.4]              [tinycss2==1.2.1]                        [toml==0.10.2]                    [tomli==2.0.1]    [toolz==0.12.0]                   [torch==1.10.0]                          [torchaudio==0.10.0]              [torchtext==0.6.0]    [torchvision==0.11.1]             [tornado==6.3.2]                         [tqdm==4.64.0]                    [traitlets==5.9.0]    [trimesh==3.9.29]                 [typer==0.4.2]                           [typing-extensions==4.5.0]        [tzlocal==5.0.1]    [ujson==5.8.0]                    [unattended-upgrades==0.1]               [urllib3==1.25.8]                 [uvicorn==0.22.0]    [uvloop==0.17.0]                  [wand==0.6.11]                           [wasabi==0.10.1]                  [watchfiles==0.19.0]    [wcwidth==0.2.6]                  [weasyprint==53.3]                       [webencodings==0.5.1]             [websocket-client==1.6.1]    [websockets==10.3]                [werkzeug==2.3.6]                        [wheel==0.34.2]                   [wordcloud==1.8.1]    [wrapt==1.15.0]                   [wsproto==1.2.0]                         [xarray-einstats==0.5.1]          [xarray==2023.1.0]    [xgboost==1.4.2]                  [xlrd==1.2.0]                            [xlsxwriter==3.1.2]               [xml-python==0.4.3]    [yarl==1.9.2]                     [zipp==3.15.0]                           [zopfli==0.2.2]                      ------------------------------------------------------------------- -------------------------------------------------------------------------- ------------------------------------------------------------------- -------------------------------------------------------------------
![Figure 8.8: A multi-model conversation with chatGPT4 'code interpreter plugin' by [Mollick](https://www.oneusefulthing.org/p/it-is-starting-to-get-strange)](../assets/chatGPTdata.png) 
- #### 8.2.12 Researcher toolkits
- ##### COG containers for ML
Cog is an [open-source tool](https://github.com/replicate/cog) for packaging machine learning models into production-ready containers. It simplifies Docker container creation, resolves compatibility issues between CUDA, cuDNN, PyTorch, TensorFlow, and Python, and uses standard Python to define model inputs and outputs. Cog generates an OpenAPI schema, validates inputs and outputs with Pydantic, creates an automatic HTTP prediction server using FastAPI, and offers automatic queue worker functionality. It supports cloud storage with Amazon S3 and Google Cloud Storage (coming soon), and allows model deployment on any infrastructure that supports Docker images, including Replicate.
- #### 8.2.13 Enterprise and convergence
Startups are clearly eager to create innovative products and business models, while established companies are exploring ways to respond to the rapid advancements in generative AI. There seems to be a sense of urgency for enterprises worldwide to develop AI strategies. Into the space Nvidia AI have emerged as the clear market enabler, offering more accessible and faster infrastructure.
Their flagship product, the Nvidia DGX H100 is in full production and available through partners like Microsoft Azure. They're also launching Nvidia DGX Cloud in collaboration with Microsoft Azure, Google GCP, and Oracle OCI, making AI supercomputers accessible from a browser. Nvidia AI Foundations might be a suitable solution as a cloud service that includes:
    Language, visual, and biology model-making services     Nvidia NeMo for building custom language models     Picasso, a visual language model-making service for custom models     trained with licensed or proprietary content
Nvidia Picasso could transform how visual content is created by allowing enterprises, ISVs, and service providers to deploy their own models. This might enable the generation of photorealistic images, high-resolution videos, and detailed 3D geometry for various applications, so it's certainly something to watch closely. Our alignment to self hosted and open source pipelines makes this less of a priority for exploration however.
Companies like Getty Images and Shutterstock intend to use Picasso for building generative models with their extensive libraries. Nvidia say they will also expand its partnership with Adobe to integrate generative AI into creative workflows, focusing on commercial viability and proper content attribution.
In the field of biology, Nvidia's Clara could be a healthcare application framework for imaging instruments, genomics, and drug discovery. Their Bio NeMo might help researchers create fine-tuned custom models with proprietary data. Nvidia BioNeMo service could provide generative AI models for drug discovery as a cloud service for easy access to accelerated workflows.
As discussed Nvidia still hope that their enterprise metaverse offering Onmiverse will gain worldwide traction. They are investing heavily in bringing ML into this product line. This is an incrediby similar proposition to flossvers, our proposal in this book, but operating at a different level of investment and technology, with commensurately more gated access.
Nvidia's partnerships with TSMC, ASML, and Synopsys could lead to advancements in chips and efficiency. Grace, Grace Hopper, and Bluefield 3 are designed for energy-efficient accelerated data centers.
Microsoft's investment in OpenAI and especially ChatGPT is clearly paying dividends right now, but it is possible that they are mindful of the transition to less centrally controlled 'edge' hardware as previously mentioned has forced their hand toward offering their generalised systems for use by corporations as plugins. It's not clear at all that this is the correct model. With that said the current response from Microsoft it yielding incredible powerful results. Their plugin 'code interpreter' allows uploading of data files of up to 100MBin size, and both writing and execution of Python code, one of the languages of data analystics. This toolkit is the first major leveraging of a unified multi-modal model. It can create media rich documents with charts, images, and diagrams, providing appropriate descriptive analysis and diagnostics of the statistics employed. This is said to be happening at the level of a junior data analytics specialist so far, and could represent the beginning of a distributive democratisation of data science Figure [8.8](https://arxiv.org/html/2207.09460v11/#Ch8.F8 "Figure 8.8 ‣ Current LLM Capabilities ‣ 8.2.11 Consumer tools ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art") and Figure [8.9](https://arxiv.org/html/2207.09460v11/#Ch8.F9 "Figure 8.9 ‣ 8.2.13 Enterprise and convergence ‣ 8.2 Generative art systems ‣ Chapter 8 Artificial Intelligence ‣ Part I State of the art").
![Figure 8.9: The planned integration of these tools with Office Suite is likely to be a historic moment](../assets/MLtaxonomy.jpg) 
- #### 8.2.14 Accessibility
- ##### Open source LLM chat and assistants
Sheng at el present FlexGen which allows execution of large language model chat bots in powerful but affordable hardware\[[398](https://arxiv.org/html/2207.09460v11/#bib.bibx398)\]. The paper presents FlexGen, a high-throughput generation engine for large language models (LLMs) that can be run with a single commodity GPU. FlexGen can be configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk, and it uses a linear programming optimizer to store and access tensors. FlexGen compresses weights and attention key/value cache to 4 bits with negligible accuracy loss, allowing for a larger batch size and increased throughput. When running OPT-175B on a single 16GB GPU. A PC running alongside our metaverse server could provide ML assistance services to users of the collaborative space immediately. We are currently using [Alpaca Llama 4-bit quantised models](https://huggingface.co/Pi3141/alpaca-lora-30B-ggml/tree/main).
[This leak](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) purporting to be from a Google employee rings very true against the research we have done (paraphrased highlights):
    Open-source models are outpacing Google and OpenAI in terms of     development speed and capabilities.     Examples of open-source achievements include LLMs on a phone,     scalable personal AI, responsible release, and multimodal advances.     Google's models have a slight edge in quality, but open-source     models are faster, more customizable, more private, and overall more     capable.     Google has no secret sauce and should consider collaborating with     the open-source community and enabling third-party integrations.     Large models might slow down progress; smaller, faster models should     be prioritized for quicker iteration.     Meta's LLaMA was leaked and sparked an outpouring of innovation in     the open-source community, lowering the barrier to entry for     training and experimentation.     LoRA, an inexpensive fine-tuning method, has been underexploited     within Google and should be paid more attention to.     Retraining models from scratch is expensive and time-consuming;     using LoRA allows for stackable improvements that can be kept up to     date more easily.     Large models may not be advantageous in the long run compared to     rapid iteration on small models.     Data quality scales better than data size; high-quality, curated     datasets are becoming the standard in open-source training.     Competing with open source is a losing proposition; Google should     consider working with them instead.     Owning the ecosystem, as Meta does, allows them to benefit from free     labor and innovation, which Google could adopt by cooperating with     the open-source community.
- ##### Real time transcription
Real-time language translation can be applied to text interfaces within metaverse applications. This can be useful in situations where users are typing or reading text, rather than speaking.
To apply NMT to text interfaces in the metaverse, the algorithm can be integrated into the interface itself. When a user types text in a specific language, the NMT algorithm can automatically detect the language and generate a translation in the desired language. This can be done in real-time, allowing for fast and seamless communication between users speaking different languages. NMT algorithms are well-suited for use in text interfaces, allowing for fast and accurate translations between multiple languages. As the technology continues to advance, we can expect to see more and more applications of NMT in the metaverse.
- ##### Real time translation
One of its most impressive recent applications is real-time language translation. In this section we will explore how this technology works, and how it can be used in metaverse applications.
Real-time language translation refers to the ability of a machine learning model to instantly translate spoken or written text from one language to another. This is different from traditional translation methods, which often involve human translators and can be slow and error-prone.
One of the key technologies behind real-time language translation is neural machine translation (NMT). This is a type of machine learning algorithm that is based on neural networks. NMT algorithms are trained on large datasets of text that has been translated by human experts. This allows the algorithm to learn the patterns and nuances of each language, which it can then use to generate accurate translations.
One of the key references for the use of neural machine translation in real-time language translation is the paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" by Bahdanau et al \[[399](https://arxiv.org/html/2207.09460v11/#bib.bibx399)\]. This paper describes the use of a neural network-based approach to machine translation, which has shown impressive results in terms of accuracy and speed.
One of the key advantages of NMT is its ability to handle complex and varied sentences. Traditional translation algorithms often rely on fixed rules and dictionaries, which can be limiting. NMT algorithms, on the other hand, can learn to handle a wide range of sentence structures and vocabulary. This makes them well-suited for translating natural languages, which are often full of irregularities and exceptions.
Another advantage of NMT is its ability to handle multiple languages at once. Traditional translation algorithms often require the user to specify the source and target languages, but NMT algorithms can automatically detect the languages of the input and output text. This makes them well-suited for use in metaverse applications, where users may be speaking different languages at the same time.
One of the challenges of using NMT in metaverse applications is the need for real-time performance. Metaverse applications often involve fast-paced interactions, and any delay in language translation can hinder the user experience. To overcome this challenge, NMT algorithms can be optimized for speed, using techniques such as parallel processing and batching. It seems likely that in our proposed systems we will require API calls to external services for this functionality, and this will almost certainly incur a cost.
The use of NMT in metaverse applications is also an active area of research, with a number of papers exploring the potential of this technology. For example, the paper \"Real-Time Neural Machine Translation for Virtual Reality\" by Chen et al. describes the use of NMT algorithms in virtual reality environments, showing how they can be used to support real-time communication between users speaking different languages.
Overall, the use of machine learning for real-time language translation is a rapidly-evolving field, with many exciting developments and applications. As the technology continues to advance, we can expect to see even more impressive results and applications in the future. [OpenAI whisper](https://openai.com/blog/whisper/)
- ##### Real time description
- ##### Interfaces
- ##### Text to sound
Complex acoustic environments are possible using [text to sound](https://anonymous.4open.science/w/iclr2023_samples-CB68/report.html) prompting.
![Figure 8.10: SD tools website shows elements of creation and training.](../assets/SDtools.jpg) 
- #### 8.2.15 Virtual humans
- ##### Real time human to avatar mapping
- #### 8.2.16 AI actors
This is the next major section to be written.
- #### 8.2.17 Chatbots
We are using Flexgen \[[398](https://arxiv.org/html/2207.09460v11/#bib.bibx398)\] on local hardware with various large language models. Response time is over a minute and the accuracy of the results is poor, but we are excited that it runs at all.
![Figure 8.11: A16Z view the nominal deployment of and AI tech stack in this way, but we are not using any of these models.](../assets/aitechstack.jpeg) 
- ##### Faces
Faces and their corresponding personae are already paired in the the Tavern AI ecosystem, encoding the metadata for the AI character into the PNG files. Obviously these could be inscribed and sold as Bitcoin ordinals. It would be a nice touch to encode the personality for the characters into a larger, high resolution file using image steganography \[[400](https://arxiv.org/html/2207.09460v11/#bib.bibx400)\], which would allow PKI type ownership too. This would be more suitable for our RGB use case.
- ##### Voices
- ##### Autonomous tasks (AutoGPT & SaSa
Autonomous General Purpose Language Models (AutoGPTs) are tools that can perform any task, leveraging connection to the internet and LLMs. Recently there has been a shift in the AutoGPT space towards specialized agents that cater to specific tasks or industries, providing more focused and useful solutions. This represents part of a more general move away from centralised general models toward more task specific systems.
Autonomous agents for research for instance search the internet for information relevant to a specific research topic and extract information from trustworthy sources. One example is \[insert\], an agent designed explicitly for research purposes. Similarly, medical research agents can call medical APIs and cite sources, providing targeted assistance in the medical field.
These agents can leverage a chain of GPT calls and fine-tuned models to perform tasks efficiently and effectively.
It has been suggested that such systems be dubbed semi-autonomous specialized agents (SASAs). These agents can streamline processes, automating multiple steps without requiring user mediation for each task. It can be seen that these are already being integrated with messenger services such as Telegram bots, very similarly to the planning and approach seen in this book. We propose:
Extrinsic AI actors which link multiple\ intrinsic virtual spaces.\ Bespoke news and current affairs synthesis\ Bespoke interactive subject matter training\ bots that bring you what you want as bespoke audio visual packages
- #### 8.2.18 Governance and safeguarding
- ##### Governance in the Virtual Reality Space
The governance of the virtual world will be a critical element in the success of the Metaverse. The virtual world will need to be policed and governed in a way that will not only protect the rights of the citizens of this new digital environment but also protect them from cybercrime. As a somewhat strained but interesting example; [Interpol see](https://www.reuters.com/technology/interpol-says-metaverse-opens-up-new-world-cybercrime-2022-10-27/) simulated environments as a way for terrorist groups to gather and plan attack. Governments and regulatory bodies will play a key role in the governance of the virtual world, but so will the industry and businesses. Nair et al describe the "unprecedented privacy risks" of the metaverse, finding that wearing a headset can currently reveal 25 data points about the user, simply by analysis of the data \[[401](https://arxiv.org/html/2207.09460v11/#bib.bibx401), [402](https://arxiv.org/html/2207.09460v11/#bib.bibx402)\]. This included inference about ethnicity, disability, and economic status. Strong data protection laws will be needed to safeguard privacy, data ownership and reduce the risk of data breaches. The governance of the virtual world will be critical to success, safeguarding will be needed to protect citizens from cyberattacks.
- ##### Safeguarding in the Metaverse
When it comes to safeguarding in the Metaverse, people need to be made aware of the risk of using VR technology. There are still many questions around the health implications of using VR and the impact it may have on a person's eyesight. In terms of safeguarding in the Metaverse, this is just one area that needs to be addressed. Users will also need to be made aware of the risks of hacking. Users will need to be educated on the need to be careful when it comes to sharing personal information and be careful what websites they access on a virtual computer. They will need to be made aware of the potential risk of having malware installed on their computer by visiting untrusted websites. Users will also need to be made aware of the potential risk of being manipulated in the virtual world. This risk is particularly high when it comes to children who are growing up in the digital world. They will need to be educated on the potential risks of being groomed or manipulated in the Metaverse.\
The problem with large social metaverse systems seems to be somehow wrapped up in humans need to test boundary conditions in novel surroundings:
    Despite 'best efforts' by the software vendors there is a chaotic     mix of levels of maturity amongst the participants. Ostensibly safe     games are themselves 'gamed' by [slightly     older](https://futurism.com/mom-horrified-her-kids-seeing-roblox)     players.     No recording of action, and reaction, creating a feeling of impunity     of action. At it's best 'The philosophers Island', but in     safeguarding terms it seems more a school yard without a teacher, or     perhaps worse, Lord of the Flies     \[[403](https://arxiv.org/html/2207.09460v11/#bib.bibx403)\].     Even adults in exclusively adult meeting places seem to go slightly     off the rails trying to find technical and social boundaries     instinctively. This leads to the now somewhat famous (TTP) "time to     penis" problem     \[[404](https://arxiv.org/html/2207.09460v11/#bib.bibx404)\]     ([coined at GDC     2009](http://gamedesignreviews.com/reviews/little-big-planet-browsing-content/)).     The research on this is pretty thin.     People seem to be suffering genuine psychological harm.
[Article in immersive wire](https://www.immersivewire.com/p/harassment-metaverse-how-address)
- ##### How to fight against cybercrime in the Metaverse?
The best way to fight against cybercrime in the Metaverse is to educate the general public on the potential risks and dangers in order to prevent them from being targeted. This can be done through various channels and mediums, such as social media, blogs and podcasts. People will need to be made aware of the risks of opening emails or clicking on links sent by unknown people. They will also need to be aware of the risks of clicking on ads and links that may lead them to websites that host malware or that steal personal information.
- #### 8.2.19 The emergent role of AI in education
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Ut purus elit, vestibulum ut, placerat ac, adipiscing vitae, felis. Curabitur dictum gravida mauris. Nam arcu libero, nonummy eget, consectetuer id, vulputate a, magna. Donec vehicula augue eu neque. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris ut leo. Cras viverra metus rhoncus sem. Nulla et lectus vestibulum urna fringilla ultrices. Phasellus eu tellus sit amet tortor gravida placerat. Integer sapien est, iaculis in, pretium quis, viverra ac, nunc. Praesent eget sem vel leo ultrices bibendum. Aenean faucibus. Morbi dolor nulla, malesuada eu, pulvinar at, mollis ac, nulla. Curabitur auctor semper nulla. Donec varius orci eget risus. Duis nibh mi, congue eu, accumsan eleifend, sagittis quis, diam. Duis eget orci sit amet orci dignissim rutrum.
Nam dui ligula, fringilla a, euismod sodales, sollicitudin vel, wisi. Morbi auctor lorem non justo. Nam lacus libero, pretium at, lobortis vitae, ultricies et, tellus. Donec aliquet, tortor sed accumsan bibendum, erat ligula aliquet magna, vitae ornare odio metus a mi. Morbi ac orci et nisl hendrerit mollis. Suspendisse ut massa. Cras nec ante. Pellentesque a nulla. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam tincidunt urna. Nulla ullamcorper vestibulum turpis. Pellentesque cursus luctus mauris.
Nulla malesuada porttitor diam. Donec felis erat, congue non, volutpat at, tincidunt tristique, libero. Vivamus viverra fermentum felis. Donec nonummy pellentesque ante. Phasellus adipiscing semper elit. Proin fermentum massa ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. Sed lacinia nulla vitae enim. Pellentesque tincidunt purus vel magna. Integer non enim. Praesent euismod nunc eu purus. Donec bibendum quam in tellus. Nullam cursus pulvinar lectus. Donec et mi. Nam vulputate metus eu enim. Vestibulum pellentesque felis eu massa.
Quisque ullamcorper placerat ipsum. Cras nibh. Morbi vel justo vitae lacus tincidunt ultrices. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. In hac habitasse platea dictumst. Integer tempus convallis augue. Etiam facilisis. Nunc elementum fermentum wisi. Aenean placerat. Ut imperdiet, enim sed gravida sollicitudin, felis odio placerat quam, ac pulvinar elit purus eget enim. Nunc vitae tortor. Proin tempus nibh sit amet nisl. Vivamus quis tortor vitae risus porta vehicula.
Fusce mauris. Vestibulum luctus nibh at lectus. Sed bibendum, nulla a faucibus semper, leo velit ultricies tellus, ac venenatis arcu wisi vel nisl. Vestibulum diam. Aliquam pellentesque, augue quis sagittis posuere, turpis lacus congue quam, in hendrerit risus eros eget felis. Maecenas eget erat in sapien mattis porttitor. Vestibulum porttitor. Nulla facilisi. Sed a turpis eu lacus commodo facilisis. Morbi fringilla, wisi in dignissim interdum, justo lectus sagittis dui, et vehicula libero dui cursus dui. Mauris tempor ligula sed lacus. Duis cursus enim ut augue. Cras ac magna. Cras nulla. Nulla egestas. Curabitur a leo. Quisque egestas wisi eget nunc. Nam feugiat lacus vel est. Curabitur consectetuer.
Suspendisse vel felis. Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu. Aenean faucibus pede eu ante. Praesent enim elit, rutrum at, molestie non, nonummy vel, nisl. Ut lectus eros, malesuada sit amet, fermentum eu, sodales cursus, magna. Donec eu purus. Quisque vehicula, urna sed ultricies auctor, pede lorem egestas dui, et convallis elit erat sed nulla. Donec luctus. Curabitur et nunc. Aliquam dolor odio, commodo pretium, ultricies non, pharetra in, velit. Integer arcu est, nonummy in, fermentum faucibus, egestas vel, odio.
Sed commodo posuere pede. Mauris ut est. Ut quis purus. Sed ac odio. Sed vehicula hendrerit sem. Duis non odio. Morbi ut dui. Sed accumsan risus eget odio. In hac habitasse platea dictumst. Pellentesque non elit. Fusce sed justo eu urna porta tincidunt. Mauris felis odio, sollicitudin sed, volutpat a, ornare ac, erat. Morbi quis dolor. Donec pellentesque, erat ac sagittis semper, nunc dui lobortis purus, quis congue purus metus ultricies tellus. Proin et quam. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos hymenaeos. Praesent sapien turpis, fermentum vel, eleifend faucibus, vehicula eu, lacus.missing [\\chapterimage] orange11.jpg
- ## Chapter 9 Affecting global change
In his latest book, Runciman, professor of Politics at Cambridge University, traces contemporary anxieties about artificial intelligence back centuries to the origins of the modern state and corporation. There are interesting an striking parallels between the apparatus of state, and the emergent field of AI.
In the 17th century, Thomas Hobbes described the ideal state as a kind of \"automaton\" - a human-made machine that could provide stability and security beyond fickle, emotional human politics. Later, the invention of the limited liability corporation allowed artificial entities to take on previously unthinkable risks and debts. Runciman argues that states and corporations function essentially as "robots" - artificial, human-made creations constructed to make decisions and take actions. Much like our worries about AI today, these entities were designed to take over certain tasks and responsibilities from human hands.
States and corporations have acquired immense, sometimes unchecked power, persisting and protecting themselves as emergent features of their creation. They remain fundamentally inhuman; they do not think, feel, or have a conscience as individual humans do. Runciman suggests that the story of the modern world is the story of handing over decision-making and control to these robots, AI and systemic alike.
Today, our prosperity, health, and safety depend deeply on these state and corporate machines. Yet Runciman warns they could also lead to catastrophe if we fail to maintain control. Their vast powers, from mass surveillance to nuclear weapons, remind us of their inhuman, robotic nature. Runciman argues we must focus not just on regulating new AI, but on democratizing and improving oversight over existing state and corporate \"robots\" we rely upon daily. More transparency, public input, and innovation in governance is needed to retain human agency. Though we created them, these powerful machines can take on a life of their own.
This chapter attempts to speak to these issues, and the wider global need to reassert control over the human condition. We will start by looking at global economics, then talk briefly about the global approaches to AI which are emerging, before exploring AI in detail in it's own chapter.
Malone, an ex central banking analyst now working in crypto, links across the last two chapters of blockchain, and money, [in a Twitter thread](https://twitter.com/brendanpmalone/status/1628067806984937472). He believes that policymakers should focus on the underlying problems in the financial system, rather than just focusing on crypto. He has a lot of appreciation for US policymakers worrying about risk in the financial system. Crypto gets attention because it's an easy target, but Malone believes that the real problems are so much bigger. According to Malone, people want to hold USD money to store value and make payments. Most are familiar with cash and bank deposits, but there's actually a spectrum of assets of varying quality that act like money, as we saw in the previous chapter. These include Euro dollars, repo, commercial paper, and more. This is what people are talking about when they reference the shadow banking system - money moving around the financial system outside of traditional banks, primarily in non-banks. As an aside, the name 'euro dollar' predates the Euro currency, and has nothing to do with it. The origins of the eurodollar market can be attributed to the Cold War in the 1950s. At that time, the Soviet Union and its Eastern European allies began depositing their US dollar holdings in European banks, primarily in London, to avoid the risk of their assets being frozen by the US government. These dollar-denominated deposits held outside the United States became known as eurodollars. Malone notes that some amount of shadow banking activity is good because it allows the money supply to be more reactive and expand and contract with economic activity, which helps fuel economic growth. However, the regulatory and political apparatus and the underlying systems weren't really designed for a system this large, opaque, and multi-dimensional. This was seen in 2008 and 2009, which was as much about shadow banking and financial plumbing as it was about subprime housing and complex derivatives. The same was seen in 2020 with COVID-19.
In times of crisis, people want to be able to freely convert whatever they are holding into something safer on the spectrum. Sadly, sometimes market liquidity isn't there, so the central banks and come to save the day, and this kicks the can down the road. The core issue is that people an institutions want to store capital in places they can't access due to technical, institutional, or geopolitical reasons. Sovereigns hold US treasuries, hedge funds and HRTs use repo, and we have seen that the crypto and Bitcoin economies have stablecoins.
Since 2008 and 2009, the Treasury Market has gotten significantly larger, more fragile, and more complex. Banks have even more restrictions on creating deposits, and the demand for safe assets keeps skyrocketing. On top of that, the geopolitical landscape has changed dramatically, with US sanctions and seizure of Russian USD assets. Malone notes that crypto is a response to these underlying problems. Although it is not perfect, it is getting better as people learn from past experiences and begin to build regulatory clarity. This issue of regulatory clarity leads us into this section of the book, which looks are implicit or explicit corruption of governance. As a uueful example; The New York Magazine article provided an in-depth interview with Gary Gensler, the head of the Securities and Exchange Commission, in which he shared his thoughts on the cryptocurrency industry. One of the key takeaways was his belief that all cryptocurrencies, except for Bitcoin, should be considered securities, as they involve relying on the work of others to give them value. Gensler is an ex banker, and an ambitious politician, with his eyes on bigger prizes. He openly courted the attention of the now disgraced top team at FTX which failed so spectacularly. His assertions have sparked controversy, as it raises questions about the feasibility of registering all tokens as securities, given the unique challenges posed by open-source protocols and the changing nature of blockchain technologies. Critics argue that Gensler's stance could harm innovation and capital formation, as companies and entrepreneurs may struggle to comply with onerous regulations or abandon their projects altogether. The current system simply doesn't fit this new self forming marketplace, and his implication seems to be that the legal end game here is the destruction of the invested capital, because of non compliance. This has led to frustration and concern among crypto advocates and investors, who worry about the impact of such policies on the industry's growth and development.
The discourse should be on the much more fundamental questions of the monetary system and fragility of past assumptions and their ability to predict what comes next. Even as these conversations happen, however, the Bitcoin and stable coin builders will keep building because they are not going to sit around and wait for solutions to be presented to them.
- ### 9.1 Global politics & digital society
- #### 9.1.1 Inequality as the driving force
- ##### Inequality on the Rise
In Britain inequality has returned to levels not seen since the 1930s. After steadily rising between 1600 to 1913, Britain's wealth as a share of the global total peaked and then began falling until the end of the 1970s \[ref required\]. During this time, Britain became one of Europe's most equal countries, even without the support of its Empire \[ref needed\]. Some argue this relative equality enabled Britain's economic growth and international standing to keep pace with its European neighbours, despite the loss of imperial power \[ref needed\]. During this period there was much upheaval in global monetary systems. More recently we have seen that trust has diminished, and inequality has risen, with social media perhaps acting as an accelerator.
- #### 9.1.2 The Social Cost of Inequality
Four decades later, the social impacts of rising inequality are becoming clear. Of the 14 million people living in poverty in Britain today, most are in working families \[ref needed\]. Upward mobility is declining, as the continued dominance of the privately educated elite in top jobs hinders meritocracy \[The Gender Wage Gap Among University Vice Chancellors in the UK 2022\] . The lack of affordable housing and regulation in the rental market has led to increasing homelessness \[ref needed\]. And with the super-rich able to avoid taxes, the burden falls more heavily on lower income groups \[ref needed\].
- #### 9.1.3 When Inequality Declines, Life Improves
However, in societies that prioritize equality, life improves for all citizens. Infant mortality falls, lifespans lengthen, and population health increases \[dorling, finland, ref\]. Access to education rises, enabling greater social mobility \[The Parenthood Effect on Gender Inequality 2013 \]. With reduced poverty and homelessness, there is less crime and violence \[ref needed\].
- #### 9.1.4 Tackling Inequality
Dorling \[oxford, reference\] Tackling inequality requires recognizing that excessive wealth concentration is detrimental to social cohesion and national prosperity. A modicum of inequality may be inevitable, but the widening chasm between rich and poor in Britain has passed sustainable limits. With common purpose and political will, a more equitable path is possible. As inequality lessened for decades before, supportive policies enabled the rise of a thriving middle class \[The Persistence in Gendering: Work-Family Policy in Britain since Beveridge \]. By pursuing greater fairness once more, Britain can regain its balance.
- #### 9.1.5 Anacyclosis
It's interesting in the current global political moment to look briefly at Anacyclosis. This is a political theory attributed to the ancient Greek historian Polybius, which posits that political systems evolve in a cyclical manner. The theory is based on the observation that governments tend to progress through six stages, each corresponding to a specific form of governance: monarchy, tyranny, aristocracy, oligarchy, democracy, and ochlocracy (mob rule). These stages are organized into three pairs, with each pair consisting of a 'good' form of governance and its corresponding 'bad' form.
    Monarchy (benign) -\> Tyranny (corrupt): Monarchy is the rule by a     single individual, such as a king or queen, who is considered to be     a wise and benevolent ruler. However, as the monarchy endures, there     is a risk that the ruler becomes corrupted or that a less competent     or tyrannical successor takes over. This leads to tyranny, the     degenerate form of monarchy, where the ruler becomes oppressive and     self-serving.     Aristocracy (benign) -\> Oligarchy (corrupt): To counter the     tyranny, a group of nobles or elites may overthrow the tyrant and     establish an aristocracy, which is the rule by a select group of     individuals who are considered wise and virtuous. Over time, the     aristocracy may become more focused on their own interests and     power, leading to an oligarchy. This is the degenerate form of     aristocracy, where a small group of elites control the government     for their own benefit.     Democracy (benign) -\> Ochlocracy (corrupt): The populace,     dissatisfied with the oppressive rule of the oligarchs, may rise up     and establish a democracy, which is the rule by the majority of the     people through voting and participation in the political process.     Democracy has the potential to create a fair and representative     system of governance. However, as the democratic process becomes     more susceptible to demagoguery, populism, and factionalism, it can     devolve into ochlocracy or mob rule, where the government is     influenced or controlled by unruly masses.
According to Polybius, these stages form a continuous cycle, as one form of governance gives way to another, and each form eventually becomes corrupted and degenerates into its corresponding 'bad' form. The theory of anacyclosis suggests that political systems are inherently unstable, with each form of governance containing the seeds of its own destruction.
- #### 9.1.6 The World Economic Forum
The World Economic Forum (WEF) is a non-governmental organization founded in 1971 by Klaus Schwab. It is well known for its annual meeting in Davos, Switzerland, where world leaders, CEOs, and various stakeholders gather to discuss global issues and potential solutions. Although the WEF does not have direct control over policymaking, its influence on global policy arises from its role as a platform for dialogue and idea exchange, as well as its ability to bring together influential individuals.
As unelected technocrats, the WEF's impact on global policy can be observed through these aspects:
    Convening power: The WEF's Davos meeting is a high-profile event     that attracts prominent political figures, business executives, and     other influential individuals. This ability to assemble people     allows the WEF to initiate conversations on global issues, create     networks, and establish connections among key players. These     interactions can lead to ideas and initiatives that might eventually     shape global policy.     Knowledge sharing and thought leadership: The WEF produces a range     of publications, reports, and research that provide insights into     various global challenges. By disseminating this knowledge, the WEF     contributes to the broader understanding of complex issues and helps     to inform policymaking by governments, businesses, and other     organizations.     Agenda-setting: Through its conferences and publications, the WEF     identifies and highlights emerging trends, risks, and opportunities,     which can help to set the agenda for global policy discussions. By     bringing attention to specific issues, the WEF can indirectly     influence the priorities of governments and other decision-makers.     Public-private cooperation: The WEF actively promotes collaboration     between the public and private sectors in addressing global     challenges. By fostering partnerships and facilitating dialogue     between these sectors, the WEF can help drive the development and     implementation of policies that require cooperation between     governments, businesses, and civil society.
Despite its influence, critics argue that the WEF's position as unelected technocrats raises concerns about the organization's legitimacy and accountability. They contend that the WEF's ability to shape global policy without being directly answerable to citizens can undermine democratic processes and result in policies that prioritize the interests of elites over the broader public. However, others argue that the WEF's role in facilitating dialogue and collaboration is essential for tackling complex global challenges that require coordinated action across sectors and borders.
Interesting for us the WEF recently released its annual [Global Risks Report](https://www3.weforum.org/docs/WEF_The_Global_Risks_Report_2022.pdf), which highlights various threats and challenges facing the world today, and which intersect with all of the narratives in this book. The report discusses issues related to cybersecurity, public trust, and social cohesion, and underscores the importance of a comprehensive approach to addressing these challenges.
The WEF's founder, Klaus Schwab, has previously argued for a "great reset" in society and the economy, which involves revamping various aspects of our lives, from education to social contracts and working conditions. This reset would require the construction of new foundations for economic and social systems.
The WEF Global Risks Report 2022 focuses on five main categories, which are also part of their "Great Narrative for Humankind" initiative:
 The report emphasizes that the erosion of social cohesion has been a significant global issue since the start of the COVID-19 crisis. In addressing these challenges, the WEF suggests that public-private collaborations are necessary to ensure effective decision-making and to safeguard the future of humanity.
The report also highlights the increasing digital dependency that intensifies cyberthreats, as the WEF has long warned of the potential for a significant cyber pandemic. The rapid spread of a cyber attack with "COVID-like characteristics" could potentially cause more damage than any biological virus.
The WEF Global Risks Report 2022 delves further into the potential consequences of a cyber pandemic. In a section titled "Shocks to Reflect Upon" the report explores the possibility of a wide-ranging and costly attack that could lead to cascading failures in systemically important businesses and disrupt services, ultimately undermining digital transformation efforts made in recent year.
The report also emphasizes the need for governments to address cyberthreats and warns that without mitigation, the escalation of cyberwarfare and the disruption of societies could result in a loss of trust in governments' ability to act as digital stewards.
To better understand the risks associated with technology, the WEF report explores the concept of the fourth industrial revolution, which Schwab believes will lead to the fusion of our physical, biological, and digital identities. This fusion will be facilitated by technologies such as artificial intelligence, internet of things-enabled devices, edge computing, blockchain, and 5G. You can see they're examining similar things to this book.
As explaining in this work, these technologies present numerous opportunities for businesses and societies, they also expose users to elevated and more pernicious forms of digital and cyber risk. The report also discusses the potential emergence of the metaverse, which could create new vulnerabilities for malicious actors by increasing the number of entry points for malware and data breaches, again a central theme of this text.
In light of these risks, the WEF report suggests that users will need to navigate security vulnerabilities inherent in complex technologies characterized by decentralization and a lack of structured guardrails or sophisticated onboarding infrastructure.
The report also touches on the issue of digital identity as we do. They view digital identity is a crucial component of accessing products, services, and information in a digital world, but again, this raises concerns about privacy, security, and the potential for misuse.
Finally, the WEF Global Risks Report 2022 addresses the issue of public trust, noting that the growth of deepfakes and disinformation-for-hire can deepen mistrust between societies, businesses, and governments. We can already see this starting to happen as Musk's defence lawyers [point to possible deepfake use](https://www.theguardian.com/technology/2023/apr/27/elon-musks-statements-could-be-deepfakes-tesla-defence-lawyers-tell-court) around video comments he is alleged to have made with regard to Tesla's software safety. To rebuild trust and social cohesion, the report calls for leaders to adopt new models, look long term, renew cooperation, and act systemically. Quite what they think they can do in the face of images like the recent memes of the Pope is unclear [9.1](https://arxiv.org/html/2207.09460v11/#Ch9.F1 "Figure 9.1 ‣ 9.1.6 The World Economic Forum ‣ 9.1 Global politics & digital society ‣ Chapter 9 Affecting global change ‣ Part I State of the art") .
It's absolutely crucial to note that the WEF is a powerful organisation, with global sway over policy, and is an enormous concentration of power in the hands of unelected technocrats. The authors are very sceptical of the WEF, but this report highlights what both technocrats and policy makers are thinking.
![Figure 9.1: Midjourney 5 fake images of The Pope Francis which are [circulating as memes](https://www.reddit.com/r/midjourney/comments/120vhdc/the_pope_drip/) and show the power and the danger of the technology even at this early stage.](../assets/pope.jpg) 
- #### 9.1.7 Money and The State
It seems a pretty reasonable that the best 'systemic' approach is a separation between major centralising forces such as state, church, and money. In practice we can see that globally, this isn't the case, with bad hotspots of high corruption where all three meld together into kleptocratic dictatorships, or theocracies. For our purposes in the UK it's useful to look at the concept of 'austerity'.
Austerity is a term used to describe a set of economic policies that aim to reduce government spending and debt, often through cuts to public services and welfare programs. The concept of austerity has its origins in the 1920s, following the end of World War I and the economic crisis that ensued. In the wake of the war, many Western European countries were struggling with high levels of debt and inflation. In response, governments began implementing policies to reduce spending and balance their budgets.
We have seen in the previous chapter that the concept of inflation itself is complex, and somewhat argued about still. Globally, on aggregate, the efficiencies of increasing technology are thought to be deflationary to the tune of between 3 and 5 percent annually, though this may radically spike up in the era of AI which will be covered later. This is counter to the current need for inflation to maintain debt repayments at a national level. Central banks manipulate interest rates to control inflation, aiming to keep it at sustainable levels. This process is necessary because as national debt and deficits grow, governments need inflation to prevent these debts from spiraling out of control. Higher inflation results in higher nominal GDP, which in turn increases the tax base, providing governments with the revenue needed to pay down debt. To achieve this. The natural progression of humanity inherently deflationary, which forces central banks to print more money and further manipulate the monetary system in order to generate the desired inflation. This can be seen as a hidden tax on citizens, as it devalues their money over time. The negative effects of this system are disproportionately felt by lower-income groups. As inflation rises, the cost of living increases, and many households struggle to make ends meet. This has led to a situation where households need multiple incomes to maintain their standard of living, forcing individuals to work longer hours and take on multiple jobs. As a result, people have less free time and energy to engage in rewarding activities or spend time with their families. This need for constant economic growth, as measured by GDP, has led to an environment where individuals are pushed to be more productive at the expense of their well-being. This has resulted in a society where many people are overworked and struggling to keep up with the rising cost of living. Booth discussed this at length in his book 'The Price of Tomorrow'. His is a rare thesis based around the ideas that technology is deflationary, that the marginal cost of goods trends over zero over time, and that the current system of debt and inflation are inherently unsustainable in the face of exponential technology improvements and automation. We discuss the concept of inflation and deflation, and both their risks throughout the book, but Booth has been very clear on this for many years. He thinks the current global monetary system ill-suited to handle the challenges and opportunities presented by deflation. He suggests that embracing deflation is the key to unlocking a prosperous and sustainable future. The book delves into the implications of deflation on various aspects of society, including wealth distribution, job markets, and the role of governments in shaping economic policies. \[[138](https://arxiv.org/html/2207.09460v11/#bib.bibx138)\].
In the 1920s, Keynes was one of the first to argue against the austerity measures which seem part of the cyclical playbook around debt and inflation. He argued that that cutting government spending during a recession would only worsen the economic downturn. Instead, he advocated for increased government spending to stimulate economic growth and reduce unemployment. Despite this, many governments continued to implement austerity policies throughout the 1920s and 1930s.
In the post-World War II period, the rise of the welfare state and the adoption of Keynesian economic policies led to a shift away from austerity in many countries. However, in the 1970s, a new economic crisis led to a resurgence of austerity policies, particularly in the United States and United Kingdom. In the 1980s, the rise of neoliberalism and the influence of economists such as Milton Friedman led to further cuts to government spending and the rolling back of the welfare state.
Today, the concept of austerity continues to shape economic policy, particularly in the wake of the 2008 financial crisis. Many governments, particularly in Europe, have implemented austerity measures in response to the crisis, leading to cuts to public services and welfare programs. The effectiveness of these policies remains a contentious issue, with some arguing that they have helped to reduce debt and stabilize economies, while others argue that they have led to increased inequality and hindered economic growth. Looking around at the state of the world, and the widening gap between the rich and the poor, it is possible to have some sympathy with those who see patterns in the bahaviour of political leaders and the controllers of Western capital and global resources. The system seems engineered to reward a few. It is possible to view 'austerity' as a means of political control of economic levers, in order to de-democratise populations. This mantra of 'do more, consume less' has perhaps become a defacto methodology to constrain popular ideas, diverting capital back into the hands of incumbents, land owners, and the politically and economically motivated \[[139](https://arxiv.org/html/2207.09460v11/#bib.bibx139)\]. It seems that the controlling nexus of this political framework globally is the concept of the central bank, unelected technocrats whose tenures span across political administrations. Again, this can be traced back to the 1920's. Hawtrey's 1925 "Currency & Public Administration" asserts that a central bank should ["Never explain; never regret; never apologise."] , and speaks glowingly of the selfish market \[[140](https://arxiv.org/html/2207.09460v11/#bib.bibx140)\]. This economic model is referred to as Dirigisme and feels increasingly the global norm \[[141](https://arxiv.org/html/2207.09460v11/#bib.bibx141)\]. We can perhaps here see the divergent point at which the lionization of the market began. Again, to be clear, the authors are not economists, but it does seem that in a global digital society there is room to explore more equitable models of global value, governance, and trust.
Remember that these centrally planned national and global actions provide liquidity to the private banking sector. Like the digital money analogues discussed earlier in the book private banks operate fractional reserve banking. This is a banking system where banks hold only a fraction of the deposits they receive as reserves, while the rest is lent out to customers. This means that the money supply in an economy can be increased through the lending activities of banks (itself a complex inflationary force which devalues money over time, feeding back into the policy directives of the central banks. The fractional reserve system is useful for capital creation in times of growth, but relies on the confidence of the depositors. Historical examples of bank runs which threatened systemic risk or caused failures of the banking system include:
    The Bank of United States crisis in the 1930s: This was the largest     bank failure in American history and was a result of a bank run     caused by rumors of financial mismanagement.     The Savings and Loan crisis of the 1980s: This was a result of a     large number of failed savings and loan associations in the United     States, which were caused by a combination of factors including poor     management, risky lending practices, and a decline in real estate     values.     The Nordic banking crisis of the 1990s: This crisis was caused by a     combination of factors including a real estate bubble, high levels     of debt, and a lack of regulation. It resulted in the collapse of     several major banks in Sweden, Finland, and Norway, and had a     significant impact on the economies of the region.     The Bank of Japan crisis in the late 1990s: This crisis was caused     by a combination of factors including a real estate bubble, high     levels of debt, and a lack of regulation. It resulted in the     collapse of several major banks and had a significant impact on the     Japanese economy.     The Asian Financial Crisis of 1997: This crisis was triggered by a     devaluation of the Thai baht and quickly spread throughout the     region, causing a number of major banks to fail. The crisis was     largely a result of a lack of transparency and poor regulation in     the banking industry.     The 2008 financial crisis in Iceland: This crisis was caused by the     collapse of the country's three largest banks, which had been     engaging in risky lending practices and had accumulated large     amounts of debt. The crisis had a devastating impact on the     Icelandic economy and resulted in a severe recession.     The Global Financial Crisis of 2007-2009: This was a result of a     widespread failure of the global banking system, caused by a     combination of factors including the housing market collapse, risky     lending practices, and a lack of regulation.     The collapse of Banco Popular in Spain in 2017: This was one of the     largest bank failures in European history, and was caused by a     combination of factors including a large amount of bad debt and a     declining real estate market.     There were many bank runs on smaller rural banks in China     during 2022. The financial conditions of Chinese banks are somewhat     reminiscent of the 2008 American landscape.
In response to the Global Financial Crisis, many measures have been taken to shore up the banking system, including the creation of new regulatory bodies, the implementation of new regulations, such as the Dodd-Frank Wall Street Reform and Consumer Protection Act, which increased the regulatory oversight of the banking industry. The introduction of stress testing for banks, to ensure that they have enough capital to withstand financial shocks, globally, has radically deleveraged banks from around 1:40 fractional reserve, to around 1:10.
There is increased political pressure to regulate the banking industry and prevent another financial crisis. However, there is also political opposition to excessive regulation, as some argue that it may stifle economic growth. There are concerns about rising levels of debt and the potential for another financial crisis.
It's interesting that Brett, a former FDIC regulator [believes that](https://blog.orchid.com/exfdic-regulator-on-trust-and-the-battle-of-the-social-media-videos/) the 2008 US bank run was sparked by youtube posts of queues forming at banks. He says those that formed the initial lines carried memories of the great depression, but that once youtube started showing the footage more broadly the contagion struck. In the world of instant messaging media today we can perhaps see how this might happen again. More recently, the 2023 'wobble' in global banking caused by the collapse of America's 5th largest [bank SVB](https://theconversation.com/why-svb-and-signature-bank-failed-so-fast-and-the-us-banking-crisis-isnt-over-yet-201737) has precipitated strong intervention by the federal government, who have opted to 'backstop' investor deposits. In the midst of this potential crisis it it notable that TikTok (now arguably the world's [most popular search engine](https://blog.cloudflare.com/popular-domains-year-in-review-2021/)) is carrying millions of hashtag references to [bankruns](https://www.tiktok.com/tag/bankrun?lang=en). Senator Kelly in the USA [allegedly inquired](https://public.substack.com/p/exclusive-senator-mark-kelly-called) about the potential for limiting such references on social media, and a UK minister is [asking for security services](https://news.sky.com/story/tiktok-ban-minister-asks-national-cyber-security-centre-to-look-into-safety-of-app-12833371) to examine the risks of the Chinese application. The perhaps reflects concern about algorithmically driven geopolitically motivated threats to the banking system.
There is a growing awareness of the role of banks in the economy, and a growing desire for greater transparency and accountability. There is also a growing mistrust of banks, particularly in light of the Global Financial Crisis. As we have seen, the advent of new technologies, such as blockchain CBDC, and fintech, is changing the way that banks operate and interact with customers. This presents both opportunities and challenges for the banking industry. As a final controversial aside, there is [industry suspicion](https://apnews.com/article/signature-bank-fdic-barney-frank-silicon-valley-6ad86262d9945675a42d735b66ace4f2) that the collapse of SVB has been used as cover to close the final US bank servicing crypto, effectively decapitating the banking rails of the industry, and forcing it overseas. Were it not for the credibility of the people making these claims, this would seem pretty wild, but the prevailing winds are surely blowing against the disruptive potential of a money system which is beyond the control of legislators.
- #### 9.1.8 Surveillance Capitalism
Surveillance capitalism is a term coined by Harvard Business School professor Shoshana Zuboff to describe the business model of using data collected from individuals to target advertising and influence behavior. The concept of surveillance capitalism emerged in the late 20th and early 21st centuries with the rise of technology companies that specialize in gathering and analyzing personal data.
The history of surveillance capitalism can be traced back to the early days of the internet. In the 1990s, companies such as DoubleClick and Omniture began collecting data on internet users' browsing habits in order to target advertising. As the internet grew in popularity, these companies were able to gather an increasing amount of data on individuals, allowing them to more effectively target advertising and increase profits.
The advent of smart phones and mobile technology in the 2000s further expanded the reach of surveillance capitalism. With the widespread adoption of smart phones and mobile apps, companies were able to collect even more data on individuals, including location data and information about their physical activity. This data was used to target advertising and influence behavior, leading to the rise of companies such as Google and Facebook, which have become dominant players in the digital advertising market.
The use of data collected from individuals to influence behaviour has also been used to influence political campaigns. In the 2016 US presidential election, Cambridge Analytica, a data analytics firm, used data collected from Facebook users to influence voter behaviour. The firm used the data to target advertising and create psychological profiles of individuals, allowing them to more effectively influence voter behaviour.
The business model of surveillance capitalism has been widely criticized for its ethical implications. Critics argue that the collection and use of personal data without consent is a violation of individuals' privacy and that the use of data to influence behaviour is manipulative and unethical. In recent years, there have been calls for greater regulation of the tech industry to address these concerns.
Surveillance capitalism has led to significant compliance overheads for companies that collect and use personal data. There are a number of laws and regulations that have been put in place to protect individuals' privacy, such as the General Data Protection Regulation (GDPR) in the European Union and the California Consumer Privacy Act (CCPA) in California, USA. These laws require companies to obtain consent from individuals before collecting and using their data, and to provide individuals with the right to access, correct, and delete their data.
Complying with these laws can be costly and time-consuming for companies. They may need to hire additional staff to handle data privacy compliance, and may also need to invest in new technology to manage and protect personal data. In addition, companies are at risk of significant fines if they fail to comply with these laws.
In terms of who profits from surveillance capitalism, the primary beneficiaries are technology companies such as Google and Facebook, which have become dominant players in the digital advertising market. These companies collect and analyse large amounts of personal data, which they use to target advertising and influence behavior. This allows them to generate significant profits from advertising revenue.
On the other hand, those who suffer the most negative impact from surveillance capitalism are individuals, whose personal data is collected and used without their consent. They are also at risk of their privacy being violated, and their personal data being misused. Additionally, the collection and use of personal data can lead to the manipulation of individuals' behaviour and decision-making, which can have negative consequences for their lives and society at large.
Moreover, the business model of surveillance capitalism has also been criticized for creating a power imbalance between companies and individuals. Companies have access to vast amounts of personal data, which they can use to influence behavior and make decisions that affect individuals' lives. This can lead to a lack of privacy and autonomy for individuals, and can also lead to discrimination and bias in decision-making.
This is collectively an erosion of the demarcation between data, state surveillance, banking, and political leadership globally.
The term \"surveillance state\" refers to a state in which government agencies have the power to collect and analyze large amounts of personal data, often without the consent of individuals. The rise of surveillance capitalism has led to concerns about the potential for the creation of a surveillance state, as government agencies may use the data collected by companies for surveillance purposes.
There have been instances where government agencies have used data collected by companies for surveillance purposes. For example, in the United States, the National Security Agency (NSA) has been accused of using data collected by companies such as Google and Facebook for surveillance purposes. The agency's PRISM program, which was revealed by Edward Snowden in 2013, was designed to collect and analyze data from internet companies in order to identify and track individuals. Europe is [clear about it's intentions](https://www.patrick-breyer.de/en/posts/chat-control/) to mandate their complete access to all encrypted personal communications in forthcoming legislation.
The use of data collected by companies for surveillance purposes can have significant implications for individuals' privacy and civil liberties. It can also lead to a lack of transparency and accountability, as government agencies may use the data without the knowledge or consent of individuals. In addition, the use of data for surveillance purposes can lead to discrimination and bias in decision-making, as well as a chilling effect on free speech and the exercise of other rights.
Akten has been [talking about](https://memoakten.medium.com/all-watched-over-by-machines-of-loving-grace-8c2464aa6fda) the phase transition from digital surveillance to pernicious corporate AI in terms of a modern 'religion' for many years \[[142](https://arxiv.org/html/2207.09460v11/#bib.bibx142)\]. He feels that despite public awareness of privacy invasion, there has been no significant outcry or unanimous demand for privacy. Instead, most individuals seem to find comfort in the belief that a higher force is watching and protecting the virtuous, while punishing wrongdoers. The concept of a 'digital deity' emerge from his thinking in this context, reflecting the role that religion and traditional gods have played in providing ethical frameworks, security, discipline, power, and other societal functions. More recently O'Gieblyn has been drawing the same conclusions \[[143](https://arxiv.org/html/2207.09460v11/#bib.bibx143)\], explicitly linking religiosity to the imperative to 'create a godhead' simply because it can be done, not pausing to discuss if it should be. Rosenberg calls this 'a threat to Epistemic Agency' \[[144](https://arxiv.org/html/2207.09460v11/#bib.bibx144)\]More recently the Harari, author of Sapiens \[[145](https://arxiv.org/html/2207.09460v11/#bib.bibx145)\] [said of AI](https://forumlive.frontiersin.org/agenda/speakers/2977577): ["For thousands of years, prophets and poets and politicians have used language and storytelling in order to manipulate and to control people and to reshape society. Now AI is likely to be able to do it. And once it can... it doesn't need to send killer robots to shoot us. It can get humans to pull the trigger. We need to act quickly before AI gets out of our control. Drug companies cannot sell people new medicines without first subjecting these products to rigorous safety checks."]  (AI will be discussed in detail in a later chapter).
Klein at the New York Times has been [writing against this point](https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html) for some time. His well articulated fear, is that the current model where three major Western companies, with similar highly competitive capitalist origins and values, should certainly not be in charge of racing to monetise the most compelling and innately unknowable chat bot experience. As societies shift towards materialism and technological dependence, traditional gods lose their relevance, and the need for a new form of overseer arises. This digital deity, existing within the realm of technology and the cloud, perhaps represents an adaptation of primal human belief systems. This will be explored further in the AI/ML chapter later.
In conclusion, the rise of surveillance capitalism has led to concerns about the potential for the creation of a surveillance state, or worse, a new kind of omnipresent digital culturla authority. Corporations and government agencies may use the data collected by companies for surveillance purposes. This can have significant implications for individuals' privacy and civil liberties. It's important for laws and regulations to be in place to safeguard citizens' rights and privacy in regards to the use of data by government agencies, and to hold them accountable for any misuse of data, and yet it seems the reality of the situation in 'post Snowden' seems far from that.
Surveillance Capitalism. As a quick round-up of this area, which is best researched elsewhere:
    The global digital advertising market is expected to reach \$335     billion by 2023.     In 2020, Google and Facebook accounted for 60% of the global digital     advertising market.     The data brokerage industry, which includes companies that collect     and sell personal data, is estimated to be worth \$200 billion.     In 2020, Google and Facebook were reported to have data on over 4     billion active users.     As of 2021, the number of data breaches reported worldwide has grown     from 4.1 billion in 2018 to 4.9 billion in 2020.     In 2013, it was revealed that the US National Security Agency (NSA)     had been collecting the phone records of millions of Americans under     its PRISM program.     In 2013, Edward Snowden leaked classified documents that revealed     the scale of the NSA's surveillance programs.     In the US, the Foreign Intelligence Surveillance Act (FISA) allows     the government to conduct surveillance on non-US citizens outside     the US without a warrant.     The UK's Investigatory Powers Act 2016, also known as the     \"snooper's charter,\" gives government agencies wide-ranging powers     to collect and analyze personal data.     In 2021, it was reported that the Chinese government has been     collecting and analyzing the data of its citizens through a system     of \"social credit\" scores, which are used to monitor and control     individuals' behaviour.     Surveillance capitalism refers to the business model of collecting     and analyzing personal data for the purpose of targeted advertising     and other forms of monetization.     A recent study by the Center for Digital Democracy found that the     top 100 global digital media companies are projected to generate     over \$1 trillion in revenue by 2020, much of which is derived from     surveillance-based advertising.     The number of surveillance cameras in use worldwide is estimated to     be over 1 billion, with the majority located in China.     A 2018 study by Comparitech found that the average person in the UK     is captured on CCTV cameras over 300 times per day.     According to a report by the American Civil Liberties Union (ACLU),     the FBI has access to over 640 million photographs for facial     recognition searches, including driver's license and passport     photos.     The U.S. government's use of surveillance technologies, such as     drones and mass data collection, has been a subject of ongoing     controversy and debate.     Some experts warn that the increasing use of surveillance     technologies by governments and private companies could lead to the     erosion of privacy rights and the creation of a \"surveillance     state.\"     In the USA senate hearing following the collapse of FTX Rep. Jesus     Garcia described bitcoin and crypto as an industry that operates     outside of the law and relies on hype, implying that the communities     that have adopted bitcoin are ill-informed and vulnerable.     Bitcoin has been adopted by a variety of communities worldwide,     particularly in countries such as Vietnam, the Philippines, Ukraine,     India, Pakistan, Brazil, Thailand, Russia, and China.     There is an outsized level of adoption among Black Americans in the     United States. This trend is not a result of targeted advertising by     companies such as FTX, but rather a response to a legacy financial     system that has limited individuals' potential.     Marginalized early adopters of bitcoin still constitute a minority     in their communities, but the worldwide adoption trend among these     groups is on the rise.     The solutions that outsiders build in bitcoin will ultimately be the     source of the technology's promised revolution. Adoption in Africa     and possibly India seems likely to be capable of driving this.     The paradigm shift will come from those who bring local, real-world     focused use cases to their communities, separating bitcoin from the     empty hype of speculation.     Marginalized communities will lead the industry's recovery and     redefine the purpose of bitcoin in the future.
Much of the following text is paraphrased from the work of Guy Turner of 'The Coin Bureau', and Lawyer and academic Eden Moglen, and needs more work because of it's critical importance to the book.
The adoption of printing by Europeans in the 15th century led to concerns around access to printed material. The right to read and the right to publish were central subjects in the struggle for freedom of thought for most of the last half millennium. The basic concern was for the right to read in private and to think, speak, and act based on a free and uncensored will. The primary antagonist for freedom of thought at the beginning of this struggle was the universal Catholic Church, an institution aimed at controlling thought in the European world through weekly surveillance of individuals, censorship of all reading material, and the ability to predict and punish unorthodox thought. In early modern Europe, the tools available for thought control were limited, but they were effective. For hundreds of years, the struggle centered around the book as a mass-manufactured article in Western culture, and whether individuals could print, possess, traffic, read, or teach from books without the permission or control of an entity empowered to punish thought. By the end of the 17th century, censorship of written material in Europe began to break down in waves throughout the European world, and the book became an article of subversive commerce, undermining the control of thought.
Currently, a new phase in human history is beginning as we are building a single extraneous digital nervous system, that will connect every human mind. Within two generations, every single human being will be connected to this network, in which all thoughts, plans, dreams, and actions will flow as nervous impulses. The fate of freedom of thought and human freedom as a whole will depend upon the organization of this network. Our current generation is the last in which human brains will be formed without contact with this network, and from now on, every human brain will be formed from early life in direct connection to the network, with input from generative AI/ML systems. This possibly results in humanity becoming a super organism of a sort, where each of us is but a neuron in the brain. Unfortunately, this generation has been raised to be consumers of media, which is now consuming us.
Anonymous reading is being determined against. Efforts discussed throughout the book to ensure privacy, from Zimmerman and the cypherpunks onward, have been met with resistance from government efforts to monitor and control information flow. The outcome of the organization of this network, and the freedom it allows, is currently being decided by this generation.
It is not solely the ease of surveillance, nor solely the permanence of data, that is concerning, it is the relentless nature of living after the "end of forgetting". Today's encrypted traffic, which is used with relative security, will eventually be decrypted as more data becomes available for crypto analysis. This means that security protocols will need to be constantly updated and redone. Furthermore, no information is ever truly lost, and every piece of information can be retained and eventually linked to other information. This is the rationale behind government officials who argue that a robust social graph of the United States is needed. The primary form of data collection that should be of most concern is media that is used to spy on us, such as books that watch us read them and search boxes that report our searches to unknown parties. There is a lot of discussion about data coming out of Meta/Facebook, but the true threat is code going in. For the past 15 years, enterprise computing has been adding a layer of analytics on top of data warehouses, which is known as business intelligence. This allows for the vast amount of data in a company's possession to be analyzed and used to answer questions the company did not know it had. The real threat of Facebook is the business intelligence layer on top of the Facebook data warehouse, which contains the behaviour of nearly a billion people. Intelligence agencies from around the world want to access this layer in order to find specific classes of people, such as potential agents, sources, and individuals that can be influenced or tortured. The goal is to run code within Facebook to extract this information, instead of obtaining data from Facebook, which would be dead data once extracted. Facebook wants to be a media company and control the web, but the reality is the true value of Facebook is the information and behavior of it's users, and the ability to mine that data. Distributed internet protocols are important in the context of government overreach into digital society and people's private lives because they provide a level of decentralization and resilience that can help protect against censorship and surveillance.
For example, if a government were to attempt to censor or block access to a centralized internet service, it could potentially do so with relative ease. However, if that same service were distributed across a network of nodes, it would be much more difficult for the government to effectively censor or block access to it.
Another advantage of distributed protocols is that they are typically more resilient to attacks or failures. If one node in the network goes offline or is compromised, the others can continue to operate, ensuring that the service remains available. This can be especially important in situations where the internet is being used for critical communication, such as during a natural disaster or political crisis.
In addition to their benefits for censorship resistance and resilience, distributed protocols can also help protect people's privacy. Because they do not rely on centralized servers or infrastructure, they can be more difficult for governments or other entities to monitor or track. This can be especially important in countries where government surveillance is prevalent or where individuals may be at risk of persecution for their online activities.
There are a number of distributed protocols that have been developed specifically to address issues of censorship and privacy, and these will be covered in more detail later.
It is important to note that distributed protocols are not a silver bullet for censorship or privacy concerns. They can be vulnerable to certain types of attacks, such as those that target the nodes of the network, and they may not always be practical for certain types of applications. However, they do provide an important tool for those seeking to protect their freedom of expression and privacy online. They offer a valuable tool for those seeking to protect their freedom of expression and privacy online, and they will likely continue to play a critical role in the future of the internet.
In recent years, several countries have proposed or passed bills that would result in unprecedented levels of online censorship. One such example is Canada's Bill C-11, also known as the Online Streaming Act. This bill was first proposed in November 2020 as Bill C-10, but failed to pass due to its controversial provisions. It was reintroduced in February 2021 as Bill C-11 and was approved by the Canadian House of Commons, the first step in the process of becoming law. If passed, the bill would give the Canadian Radio, Television and Telecommunications Commission (CRTC) the power to decide what content Canadians can view on YouTube and other social media platforms. The CRTC would also have the power to dictate what content creators can produce, with a focus on promoting \"Canadian content.\" Additionally, the bill would require certain broadcasters to contribute to the Canada Media Fund, which is used to fund mainstream media in Canada. The bill is currently being considered by the Canadian Senate, which will vote on it in February. If passed, it will then be debated by the Canadian Parliament. Tech companies such as YouTube have reportedly failed to convince the Senate to exclude user-generated content from the bill, indicating a high likelihood of it becoming law. The potential impact on the internet and free expression in Canada is significant, as the bill would give the government significant control over online content and restrict the ability of individuals to share their views and perspectives.
In a similar vein the forthcoming RESTRICT act in the USA gives huge powers without oversight to a single branch of the US government.
    The bill is called the "Restricting the Emergence of Security     Threats that Risk Information and Communications Technology Act"     It was initially thought to be about banning TikTok due to its     connections to the Chinese government and the data it collects on     its users.     The RESTRICT Act has very little to do with banning TikTok and     instead grants the US Secretary of Commerce significant powers to     determine which entities are foreign adversaries and what technology     poses a risk to national security.     The bill defines critical infrastructure broadly, which means it     could apply to almost anything the government deems necessary.     Lobbyists will be allowed to advise the Secretary of Commerce on     which products and services should be labeled as foreign     adversaries, potentially leading to monopolies.     Fines and jail time for interacting with foreign adversaries or     posing a risk to national security could reach up to \$1 million, 20     years in prison, and asset seizures.     The bill aims to crack down on VPNs (Virtual Private Networks),     which provide privacy and access to foreign websites.     There is no oversight for the actions taken by the Secretary of     Commerce under this act, and neither Congress nor the courts can     request information on these decisions.
The European Union (EU) has separated its online censorship efforts into two separate bills: the Digital Markets Act and the Digital Services Act. These bills were introduced in December 2020 and are part of the EU's Digital Services package, which aims to be completed by 2030. The Digital Services package is the second phase of the EU's digital agenda, which is being enforced through regulation in the public sector and through ESG investing in the private sector. Both the Digital Markets Act and the Digital Services Act were passed in spring 2022 and went into force in autumn 2022, but will not be enforced until later this year and early next year, depending on the size of the relevant entity. The Digital Markets Act aims to increase the EU's competitiveness in the tech space by imposing massive fines on \"gatekeepers,\" or companies that maintain monopolies by giving preference to their own products and services. This could open the door to innovation in cryptocurrency in the EU, but also requires gatekeepers to provide detailed data about the individuals and institutions using their products and services to the EU. The Digital Services Act, on the other hand, aims to regulate the content that is available online, including user-generated content. It does this by requiring companies to remove illegal content within one hour of it being reported and by imposing fines for non-compliance. The act also requires companies to implement measures to protect users from illegal content and from \"other forms of harm,\" which is defined broadly and could include a wide range of content. The EU is also in the process of passing the Artificial Intelligence Regulation Act, which will be discussed later this year and is reportedly the first of its kind. All five bills in the EU's Digital Services package are regulations, meaning they will override the national laws of EU countries. The potential impact on the internet and free expression in the EU is significant, as the Digital Services Act would give the government significant control over online content and restrict the ability of individuals to share their views and perspectives.
In the United States, two significant documents related to online censorship are the Kids Online Safety Act and the Supreme Court case Gonzalez v. Google. The Kids Online Safety Act was introduced in February 2021 and is expected to pass later this year due to bipartisan support. The act requires online services to collect Know Your Customer (KYC) information to ensure that they are not showing harmful content to minors. It also gives the Federal Trade Commission (FTC) the power to decide when children have been made unsafe online and allows parents to sue tech companies if their children have been harmed online. The act has received criticism from both sides of the political spectrum and entities outside of Congress, as it is seen as giving too much power to the government to regulate online content and could lead to increased censorship by tech companies.
The Supreme Court case Gonzalez v. Google involves the question of whether Google's algorithmic recommendations supported terrorism and contributed to the 2015 terrorist attacks in Paris. The case has been picked up by the Supreme Court after being passed up by various courts of appeal. It is being heard alongside another case, Twitter v. Tumne, involving the role of Twitter's algorithms in a terrorist attack in Istanbul. There are two potential outcomes for the case. If the Supreme Court sides with Gonzalez, it could increase the liability of social media companies under Section 230 of the Communications Decency Act, which allows them to moderate content to a limited extent without violating the First Amendment. Alternatively, the Supreme Court could declare Section 230 unconstitutional, which would make online censorship illegal but also hinder the use of algorithms on the internet. The ideal outcome, in theory, would be for the Supreme Court to side with Google and for Congress to change Section 230. However, giving Congress the power to change the law could lead to increased censorship and the potential for abuse of power.
In the UK forthcoming legislation will see tech company leaders liable for [prison sentences](https://www.independent.co.uk/news/uk/politics/bill-mps-iain-duncan-smith-molly-russell-rishi-sunak-b2263353.html) if they fail in their duty to protect minors. This will doubtless lead to both stringent universal requirements for identity proof (KYC), and significantly muted and controlled content on the platforms.
Our research focuses on business to business use cases for distributed technologies, and will provide mechanisms for verifying who is communicating with whom, to avoid falling foul of these swinging global infringements on privacy.
It is the opinion of this book that information should be free \[[146](https://arxiv.org/html/2207.09460v11/#bib.bibx146)\]
- ### 9.2 Government over-reach through bureaucracy
As an contextual example of the soft power which political apparatus uses to influence emergent human behaviour and their markets it is useful to look again to the USA. In 2013, the Obama Administration, faced with a divided Congress, resorted to using the banking system as a means to implement policy through non-traditional channels. This effort, known as Operation Choke Point, was a continuation of their success in cutting off the offshore online poker industry from banking services. Initially, the crackdown was aimed at the payday lending industry, but it soon expanded to include gun sales and adult entertainment, and eventually up to 30 different industries.
The rationale behind Operation Choke Point was to target banks that facilitated fraud, as indicated by a high ratio of fraud and disputes. However, the operation soon evolved into a redlining of industries based on nothing more than the perceived risk of reputational harm. Financial institutions were investigated without any evidence of losses. Throughout the entire operation, there was no new legislation or written guidance issued. Banks were simply warned of increased regulatory scrutiny if they did not comply.
Major banks continue to deny services to industries such as firearms and fossil fuels, and they continue to assign higher risk ratings to industries that may face government criticism, even in the absence of any official guidance. This utilisation of the financial system as a means of driving change is seen by some as a legitimate, if not ideal, mechanism; as just one more type of market actor. Regardless of one's political perspective, it is important to consider the moral hazard of bypassing traditional political channels and using bureaucratic mechanisms as a means of affecting change in the free market. It is important to consider how the power of these tactics might be used in the future by opposing political groups. For example, supporters of Operation Choke Point who were in favour of increased financial pressure on the oil and gas industry may not feel the same if the same techniques were applied to organizations like Planned Parenthood. From this perspective, the tactics used by Operation Choke Point can be seen as undemocratic, regardless of who is deploying them. Bringing this back to our study of new financial tooling in crypto we can look to recent events:
    January: Some banks start to wind down activity in the crypto     industry     January 21st: Binance announces its banking partner, Signature Bank,     refuses to process Swift payments for less than \$100,000     January 27th: Federal Reserve denies Custodia Bank's application to     access Federal Reserve System     January 27th: Federal Reserve denies Custodia Bank's application for     a master account     January 27th: Federal Reserve releases statement discouraging banks     from holding crypto assets or issuing stable coins     January 27th: National Economic Council issues policy statement     discouraging banks from transacting with crypto assets or     maintaining exposure to crypto depositors     February 2nd: DOJ announces investigation into Silvergate Bank over     dealings with FTX and Alameda Research     February 6th: Binance announces suspension of USD bank transfers to     and from offshore exchange     February 8th: Binance announces search for another banking partner     February 7th: Fed's policy statement enters Federal Register as a     final rule     Two outstanding applications for National Trust Bank licenses from     Anchorage and Paxos likely to be rejected by the OCC     Banking services becoming increasingly difficult for crypto firms,     some startups will likely now not make the attempt
It seems that in the absence of democratic the SEC is attempting to use their tools to control and centralise the 'ramps' into and out of digital assets, and the rules around holding them for investors. The SEC has proposed a new rule that would require registered investment advisors to use qualified custodians for all assets, including cryptocurrencies. The intention behind this proposal is to improve investor protection by mandating that custodians hold customer assets in segregated and identifiable accounts. However, critics argue that this proposal would limit the number of qualified cryptocustodians and deter investment advisors from advising their clients on crypto. The few banks with the necessary technical capabilities and regulatory approvals will have a monopoly on crypto custodial services, while exchanges without a banking license or trust bank will likely lose out. The proposal assumes that crypto assets are securities without going through a process to determine that. The outcome of the proposal will depend on the stringency of the SEC's qualified custodian registrations. The proposal is currently in a 60-day public comment period before the Commissioners hold another vote on whether to pass the rule.
Caitlyn Long explains that the proposed rule would not necessarily kill crypto custody, but would be a move against State Charter trust companies. She points out the big issue with the proposal, which is the requirement for custodians to indemnify for negligence, recklessness, or willful misconduct. This would apply to all asset classes, including commodities and crypto, which could kill the custody business broadly. The SEC proposal would apply the custody rule to all asset classes, including commodities and crypto, which is okay, but the SEC also wants custodians to indemnify the full asset value for losses in which the custodian played any role, even for physical assets like oil, cattle, and wheat. This would upset long-standing insurance terms and could cause huge pushback from the banking, Wall Street, commodities, and crypto industries. Sarah Brennan believes that the proposal represents continued governmental efforts at denial of service attacks on crypto, and that the SEC's approach only seeks to chill digital asset markets. She and the Republicans on the House Financial Services Committee are urging stakeholders to submit public comments on the proposed amendments to ensure the custody rule for investment advisors is modernized appropriately. The U.S. Internal Revenue Service plans to hire nearly 30k new staff and technology over the next two years, spending \$80 billion to improve tax enforcement, much of it focussing on crypto markets. It might be that the industry follows the prevailing winds [and pivots to the East](https://noelleacheson.substack.com/p/weekly-feb-25-2023). As usual, none of this particularly impacts our use case and thesis.
- ### 9.3 Global monetary policy
The term "don't fight the Fed" has been used in trading circles for many years. Owing to the pre-eminent role of the dollar in global markets actions of the political and central banking bodies which impact the dollar always have global reach. It is worth knowing that these decisions are usually contested, and worse, the power of the decision makers seems rooted in their narrative impact. It's a pretty terrible system given the impact on billions of lives. The Federal Reserve System, which is comprised of a Board of Governors, 12 regional banks, and an Open Market Committee, is a privately-owned central banking system in the United States. The member banks of each Federal Reserve Bank vote on the majority of the Reserve Bank's directors and the directors vote on members to serve on the Open Market Committee, which determines monetary policy. The president of the New York Federal Reserve Bank is traditionally given the vice chairmanship of the Open Market Committee and is a permanent committee member. This means that private banks are the key determinants in the composition of the Open Market Committee, which regulates the entire economy. The Federal Reserve is an independent agency and its monetary policy decisions do not have to be approved by the President or anyone else in the executive or legislative branches of government. The Fed's profits are returned to the Treasury each year, but the member banks' shares of the Fed earn them a 6% dividend. The 2008 financial crisis and subsequent bailouts exposed the fundamental conflicts of interest at the heart of the Federal Reserve System, where the very banks that caused the crisis were the recipients of the trillions of dollars in bailout money. These conflicts of interest were baked into the Federal Reserve Act over 100 years ago and are a structural feature of the institution. The concentration of power within this group is staggering.
- ### 9.4 Opportunities in Africa
- #### 9.4.1 Gridless
In the course of researching this book we see most opportunity for change in Africa. As an example the company 'Gridless' began by examining different energy sources in Africa and exploring opportunities for larger energy generation and grid-connected energy. However, they found that the real benefit of gridless energy was in providing energy to places that were not well connected and did not have a good grid. They contacted mini-grid providers all over East and Southern Africa to learn about their problems. A mini-grid is defined as a project that generates energy under 2 megawatts, often under 1 megawatt. They discovered that these providers had to overbuild for the community, resulting in stranded energy. The company found a way to utilize this stranded energy by placing Bitcoin miners on it and paying the mini-grid providers for it. They tested this method and found it to be successful. Additionally, they implemented a system to automate and remotely turn off the power during periods of high usage to make the grid more efficient and sustainable. This solution provided a win-win-win situation for the company, the mini-grid providers, and the communities they served.
The company utilizes Bitcoin miners to create space for other activities and to increase access to affordable energy for communities and small businesses. As energy usage increases in the community, the company decreases their usage of miners and moves them to other locations. This is outlined in their contracts with partners. The company is currently testing this method and has encountered some challenges, such as losing internet connection at one of their sites and poor rainfall affecting the amount of water flowing into turbines. They have found that building a lean operation with flexible and adaptable staff is crucial, as well as creating processes and systems to manage variables. The company also faces unique environmental factors such as lightning strikes, which require them to turn off their operations temporarily.
Gridless suggest that those who are critical of opportunities like this often come from a place of privilege and do not understand the consequences of their actions in places like Africa where access to electricity and other resources is limited. They argue that these critics, who are often from the West, have blinders on and cannot see the impact of their actions on a global scale. They suggest that more people need to travel and have diverse experiences in order to change their perspective on Bitcoin and its potential to support human flourishing in underprivileged areas. They also mention that gridless plans may become a case study for the positive impact of Bitcoin mining on economic opportunities, particularly in rural Africa.
- #### 9.4.2 Machankura
Mobile phone users in Nigeria, Tanzania, South Africa, Kenya and five other African countries can now [send and receive bitcoin](https://www.forbes.com/sites/digital-assets/2023/03/15/how-africans-are-using-bitcoin-without-internet-access/?sh=434df18b7428) without a smartphone or Internet connection. Just a basic feature phone and text code will suffice, thanks to a digital wallet from software developer Ngako. No internet connection and low power handsets means using SMS and the Lightning network, with the phones SIM acting as the wallet private keys.
- ### 9.5 El Salvador as a case study
El Salvador became the first country in the world to adopt Bitcoin as legal tender. El Salvador's adoption of Bitcoin was a historic moment in the world of Bitcoin and was met with a mix of excitement and scepticism. On June 9, 2021, the country's Legislative Assembly approved a bill introduced by President Nayib Bukele to make Bitcoin a legal tender alongside the US dollar, which has been used as the country's official currency since 2001.
President Bukele, who has been a vocal proponent of Bitcoin, stated that the adoption of Bitcoin was a way to promote financial inclusion and stability in the country, where more than 70% of the population is unbanked or underbanked. In a tweet, he stated, "Bitcoin will have the same value as the US dollar. We will support both. They will have the same power of purchase and will be accepted in the same way."
The move was met with a lot of media attention and reaction, with some praising it as a bold and innovative step, while others raised concerns about the volatility of Bitcoin and their potential impact on the economy. President Nayib Bukele himself has faced criticism for his handling of political power and some of his actions have raised concerns about the potential for abuses of power. In 2021, President Bukele faced widespread criticism for his handling of the legislative process and his use of the military to secure the Legislative Assembly building during a political standoff with lawmakers. This led to allegations of intimidation and a violation of democratic norms, and raised concerns about his willingness to use force to achieve his political goals. Additionally, President Bukele has faced criticism for his use of social media to communicate with the public and his tendency to bypass traditional media outlets, which has raised concerns about the potential for censorship and the manipulation of information. With that said he seems much loved in the country, and the previously appalling safety statistics of the nation have radically improved.
In addition to the adoption of Bitcoin as legal tender, El Salvador has also proposed the issuance of a Bitcoin-backed bond to finance various public works projects and promote the use of Bitcoin. The bond would be denominated in Bitcoin and would allow investors to directly participate in the country's development while also supporting the growth and adoption of Bitcoin.
Another ambitious project that has been proposed by President Bukele and his administration is the creation of "Bitcoin City", a new city that would mine Bitcoin at the base of a dormant Volcano, and offer considerable tax benefits to holders. The city would serve as a hub for innovation and a showcase for the potential of Bitcoin, and would offer a wide range of services, including housing, healthcare, education, and entertainment.
There has been a significant increase in the adoption of Bitcoin in El Salvador, and apparently increased inward investment to the country. Many businesses, both small and large, have started accepting Bitcoin as a form of payment, and there has been a growing interest in Bitcoin among the general population. Additionally, the government has been actively promoting the use of Bitcoin through various initiatives. There have also been efforts to educate the public about Bitcoin and its potential benefits, including increased financial security and reduced transaction fees compared to traditional banking systems.
Overall, the adoption of Bitcoin in El Salvador has been positive, far outstripping the number of people in the country with traditional bank accounts, and has the potential to greatly impact the country's economy and financial sector. However, it is important to note that there are still challenges to overcome, such as regulatory and infrastructure limitations, as well as ongoing concerns about the volatility and stability of Bitcoin.
Somewhat surprisingly the IMF have de-escalated their previously highly critical assessment of the move, toward a more [concerned and conciliatory tone](https://www.imf.org/en/News/Articles/2023/02/10/el-salvador-staff-concluding-statement-of-the-2023-article-iv-mission): ["Bitcoin's risks should be addressed. While risks have not materialized due to the limited Bitcoin use so far---as suggested by survey and remittances data---its use could grow given its legal tender status and new legislative reforms to encourage the use of crypto assets, including tokenized bonds (Digital Assets Law). In this context, underlying risks to financial integrity and stability, fiscal sustainability, and consumer protection persist, and the recommendations of the 2021 Article IV remain valid. Greater transparency over the government's transactions in Bitcoin and the financial situation of the state-owned Bitcoin-wallet (Chivo) remains essential, especially to assess the underlying fiscal contingencies and counterparty risks."] 
In terms of economic impact, it is still too early to determine the full effects of the adoption of Bitcoin in El Salvador. However, it is expected to have a positive impact on financial inclusion and stability, as well as reducing the reliance on traditional banking systems. The use of Bitcoin has the potential to lower transaction fees and increase financial security, which could be particularly beneficial for those who do not have access to traditional banking services.
Overall, the adoption of Bitcoin in El Salvador marks a significant step forward in the mainstream acceptance and adoption of Bitcoin and has the potential to set a precedent for other countries to follow. However, it is important to monitor the situation and assess the long-term impacts on the economy and financial sector.
The swift rise of digital walled gardens, moving towards a less transparent internet, reveals both a need for user data protection and a corporate push for greater control and profit. Tech giants like Google, Reddit, and Twitter are increasingly controlling their platforms, adjusting data flows for revenue growth. Google's new privacy policy, which allows data collection for AI model training, increases public concerns over user consent and privacy rights. The wide-ranging language of the policy gives Google considerable power in using user-generated content, fueling debates on data usage ethics. Simultaneously, the social web's shift towards an entertainment-focused business model prioritizes revenue over human connection. Platforms target ad revenue through vertically scrolling videos, risking reduced content diversity and creating echo chambers.
Entertainment unions like the International Alliance of Theatrical Stage Employees (IATSE) are grappling with AI's impact on employment. Their approach includes research, collaboration, education, political advocacy, organizing, and collective bargaining to protect members' interests, including upskilling initiatives. Upskilling is gaining industry attention. Companies like Tata Consultancy highlight the need to equip engineers with AI skills. Recognizing AI technologies' potential, they invest in reskilling programs to stay competitive and effectively use AI tools.
The rise of generative AI and the declining open web raises concerns about maintaining digital commons and encouraging diverse perspectives. AI-generated content could overshadow human contributions, making meaningful information harder to find and increasing misinformation risks. This situation highlights the need for balance between AI-generated and human-generated content.
Data control battles between platforms and users fuel debates on data ownership and profit sharing. Users demand more control over their data use and potentially a share in the resulting profits. This issue emphasizes the need for transparent data policies and fair user compensation models.
The influence of AI on the job market and the future of work is a significant concern. As AI technology progresses, the need for upskilling and reskilling programs grows to ensure workers can adapt to changing job requirements. Collaboration between industries, governments, and educational institutions is essential to address AI-induced disruptions and ensure a smooth workforce transition.
Finally, the importance of AI ethics and governance grows as AI technologies become more prevalent. The development and deployment of AI systems require ethical frameworks, transparency, and accountability. Collaboration between AI researchers, policymakers, and ethicists is critical to address potential risks and societal implications of AI technology.
- ### 9.6 Artificial Intelligence in a global context
This currently borrows heavily from [the AI breakdown podcast](https://www.youtube.com/watch?v=5clOHBo8HP8), is an AI generated placeholder, and needs considerably more more.
- #### 9.6.1 Perception of AI and Society
The examination of AI's implications on societal structures should undoubtedly receive the necessary attention. Soros's language and perception of reality seem particularly interesting, especially in the era of AI. He emphasizes his belief in reality and its importance in providing moral guidance, a concept that seems increasingly challenged in the age of AI.
- #### 9.6.2 AI, Propaganda, and Authoritarianism
In an opinion piece for The Hill by Bill Drexel and Caleb Withers, titled \"Generative AI could be an authoritarian breakthrough in brainwashing,\" the authors argue that the concern isn't just external attempts to influence U.S. elections, but the impact on the populations within authoritarian countries. They posit that foreign disinformation efforts by Chinese and Russian entities are only the tip of the iceberg, with Beijing and Moscow disseminating massive amounts of propaganda to their own populations. The authors also cite instances of AI-enabled propaganda and misinformation campaigns, both in the context of undermining democracies and consolidating control within authoritarian states.
- #### 9.6.3 Increased Surveillance Through AI
Another critical concern around AI and authoritarianism is the potential for increased surveillance. With the integration of AI and data scraping techniques, governments can employ extensive teams to facilitate unprecedented levels of surveillance, compromising privacy. Such concerns are raised in the works of authors like Daniel Oberhaus, who posits that authoritarian regimes may have an advantage in AI due to their willingness to exploit data, such as advanced facial recognition data, in ways that open societies might not.
- #### 9.6.4 Worker Surveillance and Remote Work
Furthermore, the issue of worker surveillance, especially with the rise of remote work regimes, has garnered the attention of various entities, including the White House. This is due to concerns over automated systems that employers are using to monitor their remote workers, highlighting a less benign context of surveillance.
- #### 9.6.5 AI and Ideology
One way AI might foster authoritarianism is by supporting the ideology of closed societies or authoritarian regimes, such as China. These societies may leverage their global influence to disseminate their particular AI model, aligning it with their motivations and goals. The Carnegie Endowment for International Peace points out that for most countries, AI technology is viewed as an economic development factor that determines their standing in the global technology race, rather than as an ideological preference.
- #### 9.6.6 AI and Central Planning
Another concern is the fear that AI will make centrally planned economies seem viable, where past attempts failed due to the lack of data. This idea was discussed in a conversation between Peter Thiel and Reed Hoffman hosted by Neil Ferguson at Stanford in 2018. Thiel posited that AI appears to favor centralization, an aspect that supports the principles of central planning.
- #### 9.6.7 Uncontrolled AGI Creation
On the other hand, some suggest that capitalist competition could result in the creation of AGI that cannot be controlled. Dr. Jeffrey Hinton, a vocal advocate of this view, argues that AI's potential to disrupt business models could drive companies to recklessly pursue advancements in AI to stay competitive. This could lead to increased state power as people become more reliant on the state in an AI-dominated economy, potentially resulting in increased authoritarianism.
- #### 9.6.8 AI Promoting Freedom
However, AI could also promote freedom in several ways. For instance, AI tools like Altana have been used to identify goods made using forced labor, helping companies make informed supply chain decisions. AI could also serve as a new interface for disseminating information, such as a chatbot that aids detainees in requesting legal assistance.
- #### 9.6.9 AI, Integrity, and Accessibility
Yet, for AI to achieve its full potential in promoting freedom, the integrity of the information it disseminates must be uncompromised, and its accessibility must be ensured despite potential firewalls.
- #### 9.6.10 AI's Impact on Societal Organization
Given these diverse viewpoints, it seems that the potential of AI to either aid authoritarianism or promote freedom is yet to be fully explored. However, the inherent ability of democracies to encourage disagreement and diverse perspectives may serve as a counterbalance to the potential of AI for authoritarian control. Moreover, AI's capacity as a catalytic force in societal organization should not be underestimated. The increasing discourse around AI and its implications for labor and technology usage suggests that AI technology is reshaping the world in ways that were unimaginable just a few years ago. Its capabilities in data analysis, decision making, and automation are transforming industries and redefining the scope of what's possible.
- #### 9.6.11 Democratization of AI Technology
An argument often made in favor of democratization of AI technology is that it should be made open-source and freely available, thus creating a challenging framework for global political incumbents. This perspective is grounded on the belief that technology - and its underlying power - must be accessible to everyone to mitigate the risks of misuse and ensure fair benefits distribution.
- #### 9.6.12 Open-source AI and Innovation
Open-source AI can be a vehicle for widespread innovation. It can spur creativity, leading to breakthroughs in various sectors, from healthcare and education to energy and transportation. Open-source technologies facilitate collaboration, accelerate the pace of research, and democratize access, enabling researchers and developers across the globe to contribute to the expansion of AI's capabilities. It opens the possibility for rapid iteration and innovation, reducing the likelihood that a few powerful entities monopolize control over these transformative technologies.
- #### 9.6.13 Open-source AI and Global Politics
However, as beneficial as open-source AI may appear, the complexity of global politics can make the transition challenging. A landscape where AI technologies are open-source and freely available brings about potential dilemmas in various areas including national security, economic competitiveness, intellectual property rights, and data privacy.
- #### 9.6.14 National Security and Open-source AI
To start, national security is a primary concern. AI has a myriad of applications in defense and security sectors, many of which could potentially be exploited by adversarial entities. As such, unrestricted access to AI technologies could pose a risk to nations' security. Nevertheless, it is crucial to note that security risks also stem from concentrated AI power. A handful of nations or corporations owning the majority of AI developments may lead to destabilization, power imbalance, and heightened global tensions.
- #### 9.6.15 Economic Competitiveness and Open-source AI
Economic competitiveness is another intricate aspect. Countries and corporations are engaged in a fiercely competitive race to advance in AI technologies, recognizing the economic gains and strategic advantages tied to AI leadership. Open-source AI might challenge this dynamic, disrupting traditional models of competition. However, it could also create an environment of shared growth, leading to a more balanced global AI landscape.
- #### 9.6.16 Intellectual Property Rights and Open-source AI
Intellectual property rights form another complex dimension in the discussion. Open-source AI challenges traditional notions of ownership and patents, potentially undermining the incentives for companies and individuals to invest in AI research and development. Balancing the need for innovation with the necessity to protect inventors' rights becomes critical in an open-source framework.
- #### 9.6.17 Data Privacy and Open-source AI
Data privacy is a further point of contention. Open-source AI, coupled with increasingly ubiquitous data collection methods, raises concerns about individuals' privacy. However, it also provides an opportunity to develop robust, decentralized, and transparent AI systems that respect user privacy.
- #### 9.6.18 A New Social Contract for AI
Thus, navigating the intersection of AI and global politics necessitates careful consideration. It requires establishing a new social contract for AI---one that respects human rights, promotes equitable economic growth, and protects national security.
- #### 9.6.19 Conclusion
In conclusion, making AI open-source and freely available represents a shift from the status quo, with both promising potentials and daunting challenges. A global AI framework that upholds democratic principles and values, promotes shared prosperity, and safeguards security and privacy is the aspiration. To achieve this, an inclusive and multidimensional discourse is essential, involving governments, corporations, civil society, academia, and individual citizens. It is through this collective effort that AI's true potential can be harnessed for the global good.
There is skepticism the idea of artificial general intelligence (AGI) leading to superintelligent machines that threaten humanity in the near future. This supposed risk of AGI is described as a \"red herring\" - an unfounded fear. The reasons given are:
    We do not have a clear understanding or definition of general     intelligence or consciousness.     Current AI like large language models are limited in scope. They are     good at statistical pattern matching in language, not generally     intelligent.     The hypothesis that intelligence and consciousness emerge simply     from increasing computational power is unproven. There are likely     other components we don't understand.
The real risk is perhaps government control and regulation of AI development and applications, justified by arguing it is needed for safety and responsible AI. This could impose limits on acceptable speech and thought. Centralised entities could become gatekeepers for how people access and interpret information about the world. Mandating allowable language could narrow ideas and speech to fit an official narrative. Fears of AGI, even if exaggerated, open the door for regulators and bureaucrats to intervene in the name of safety. The risk is not AGI itself but the government control that hype about it enables.
There is speculation that AI will automate many white collar cognitive jobs, similar to how industrial machinery automated manual labor. This may \"chase humans up the value stack\" as lower value work is handled by AI, freeing people to focus on higher value creative activities.
- ## Chapter 10 Our proposition
- ### 10.1 Introduction and Problem Definition
This chapter identifies an intersectional space across the described technologies, and proposes a valuable and novel software stack, which can enable exploration and product development. It is useful to briefly look at the Venn disgram we began with, and recap the book and the conclusions we have drawn so far.
![Figure 10.1: Another look at the diagram of intersections.](../assets/landscapevenn.png) 
- #### 10.1.1 Overview of the Metaverse and Digital Society
The concept of the Metaverse has gained significant attention, with various stakeholders positioning themselves to capitalize on its potential. While it remains unclear exactly what form the Metaverse will take or whether people truly desire it, it is evident that digital society holds considerable promise. We see advantage less in social metaverse, and more in solving business to business technical use cases where professionals with visual technical problems, or training requirements, gather in collaborative spaces.
- #### 10.1.2 Trust, Accessibility, Governance, and Safeguarding
The Metaverse faces numerous challenges, including poor adoption rates, overstated market need, and a lack of genuine digital society use cases. Meanwhile trust abuses by incumbent providers have led to potential inflection points in the organization of the wider internet. Moreover, emerging markets and less developed nations face barriers to entry due to inadequate identification, banking infrastructure, and computing power. There is an opportunity to build pervasive digital spaces with a different and more open foundation, learning from these lessons.
- #### 10.1.3 The Need for Modular Open-Source Solutions
Developing a topologically flat, inclusive, permissionless, federated, and open Metaverse is essential to address these challenges. By using open-source AI tooling and large language models, it is possible to improve creativity, safeguarding, and governance, while breaking down language barriers and accessibility challenges. Implementing secure, trusted, and task-appropriate solutions can promote collaboration and innovation across various industries.
- #### 10.1.4 Technical problem definition
Problems are
    evergreen telecollaboration around technical issues     exchange of good, services, money within systems, without friction     identity management within virtual spaces     access to information in the extrinsic world from within the tool     federation of instances without overhead (scaling)     seamless access to personal information within and without the     collaborative system     ability to take advantage of supporting smart support agents (bots,     etc) throughout     governance, trust, safeguarding
- ### 10.2 Lean canvas business model
-   [Problem]      Existing large-scale telecollaboration solutions suffer from poor     adoption, limited accessibility, and trust issues. Meanwhile,     emerging markets struggle to participate in the growing digital     society due to the lack of inclusive tools and infrastructure,     limiting access to global talent and new pools of ideas. There is     insufficient provision of global talent pipelines for highly     technical workflows. -   [Solution]      Develop a secure, accessible, and inclusive platform for specialized     telecollaboration spaces that seamlessly integrate advanced AI, ML,     highly scalable and proven distributed systems, and open-source     principles to create a digital society that caters to diverse     industries, users globally, and captures global talent and     innovative ideas. -   [Unique Value Prop]      Ultra low cost training spaces, accessible 24/7 through very low end     hardware. Interact with highly customizable, task-appropriate, and     user-friendly specialized telecollaboration spaces supported by     specially trained and optimised supportive large language AI models.     Multi-ligual for emerging markets, enabling access to untapped     global talent and fostering the exchange of diverse ideas. -   [Target Market]      We will cater to the global training, research, biomedical, and     creative industries, with a special focus on empowering users in     emerging markets such as Africa and India, and connecting them with     worldwide opportunities and resources. In the first instance we     would leverage UK academic institutions and their problems, and     networks. -   [Channels]      Initially Universities, but this will scale to be sector specific. -   [Revenue Streams]      We will offer tiered subscription plans to accommodate various user     needs and budgets, as well as tailored enterprise solutions for     large-scale clients. Bespoke consulting and support trending toward     software as a service at scale. -   [Cost Structure]      Platform development, AI/ML tool integration, training for LLMs,     market research and awareness, and ongoing maintenance and support. -   [Key Metrics]      We will track user growth, engagement, and retention, successful     collaborations across industries, the platform's positive impact on     users in emerging markets, and the effectiveness of global talent     capture and idea exchange. -   [Unfair Advantage]      Our team's extensive experience in telecollaboration research, AI,     ML, and a deep understanding of the complex landscape of emerging     technologies, including highly scalable and proven distributed     systems, provide us with a unique edge in creating a game-changing     platform for specialized telecollaboration spaces that are secure,     trusted, and tailored to diverse user needs while enabling access to     global talent and innovative ideas.
- ### 10.3 Proposed Layered Framework
- #### 10.3.1 Layer 1: Bitcoin, Lightning, and Nostr protocols
Distributed financial tooling and digital assets, have ignited imagination and adoption within and outside of the Metaverse context. A global ledger could unite isolated digital ecosystems and enable the transfer of portable 'goods' across digital society. An open-source Metaverse should emphasize the development and adoption of open protocols and data formats. The Nostr protocol, for instance, might link and federate mixed reality spaces, providing identity assurances and mediating data synchronization while maintaining reasonably strong cryptography. This also allows integration with the legacy web through ubiquitous web sockets. Bitcoin and associated technologie, despite their issues, have the potential to revolutionize the way digital society operates by enabling "money-like networks" which are a cornerstone of human interaction. Representations of traditional currencies can ride securely on top of these networks as stablecoins, opening up global collaborative working practices, especially for emerging markets. Streaming micropayments and machine to machines (AI to AI) are crucially and under-considered in this context.
- #### 10.3.2 Layer 2: Modular human computer interface
Collaborative global networks for training, research, biomedical, and creative industries can be developed using immersive and accessible environments. Engaging with ideas from diverse cultural backgrounds can enrich the overall user experience.
Industry players have noted the risk and failures associated with closed systems like Meta and are embracing the \"open Metaverse\" narrative to de-risk their interests. To enable a truly open and interoperable Metaverse, it is crucial to develop open-source APIs, SDKs, and data standards that allow different platforms to communicate and exchange information. While we wish initially to build around a simpler open source engine we aim to link across standards such as Unity, Unreal, and Omniverse as we develop. This can be accomplished using our federation layer.
- #### 10.3.3 Layer 3: LLM and Generative ML Integration
Integrating AI and machine learning into the Metaverse can promote supported creativity and augmented intelligence. By incorporating generative ML technologies, users can ideate in simple immersive spaces while instantly creating scenes that can be stylized using verbal commands in real-time.
To create a more inclusive and accessible Metaverse, user experience components like UI/UX design, AI assistants, and generative content creation should be tailored to a wide range of users. The integration of AI and machine learning technologies, such as GPT-4, can facilitate more seamless interactions and creative content generation, fostering a more engaging and immersive experience.
- ##### Bots and AI agents
Autonomous AI agents, bonded to, but not bounded by, each federated mixed reality instance, can to be self-governing entities that operate within their federated virtual social spaces, drawing upon private Bitcoin and Lightning wallets to perform and mediate economic exchanges within the spaces. They could also trivially operate outside the virtual space, and within other spaces on the same metaverse federation. They would accomplish this by drawing on their 'home' GPU/TPU processors where appropriate, or else using distributed large language model (LLM) processing to accomplish tasks assigned by their instructors. They can interact with the 'web2' world using open-source software called auto-gpt and have constraints, such as "time to live" and limited access to funds through their Bitcoin Lightning wallets.
    Resource Management: These AI agents have access to dedicated LLM     resources within their home instances in the federated virtual     social spaces. If such resources are unavailable, they can resort to     using slower, distributed open-source LLMs like Horde. This     flexibility ensures that the agents can continue to function and     complete tasks even if faced with limited LLM interpretive     resources.     Financial Autonomy: The AI agents have their own private Bitcoin and     Lightning wallets, which enable them to manage and utilize funds     independently. They can use these funds to pay for services, acquire     resources, or even trade with other agents or users within the     virtual social spaces.     Interaction with Web2: By using open-source software like auto-gpt,     the AI agents can interact with the web2 world, which includes     browsing websites, retrieving information, and communicating with     web services. This allows them to gather data, analyze trends, and     perform other tasks that may require access to the broader internet.     Task Execution: The AI agents can be assigned tasks by their     instructors (or a hierarchy of AI actors), such as data analysis,     research, content creation, or other complex tasks that require LLM     processing. They can use their dedicated LLM resources or     distributed LLMs like Horde to process and analyze large datasets,     generate insights, and produce desired outputs, up to and including     those which require finance systems. This would be bridged in the     first instance using Bitrefil gift card infrastructure.     Social Interactions: Within the federated virtual social spaces, AI     agents can communicate and collaborate with other agents or human     users. They can participate in discussions, provide assistance, or     even learn from the interactions, thereby improving their     capabilities over time. Language translation, governance, and     safeguarding could also be developed. Safeguarding would be handled     by threshold risk triggers and transmission of data in a sovereign     way to all parties, allowing external action by authorities     appropriate to any abuse.     Time-to-Live Constraint: The AI agents have a predetermined "time to     live", which means they exist for a specific duration before     expiring. This constraint ensures that agents do not consume     resources indefinitely and allows for the creation of new agents     with updated capabilities. Any agents which deplete their financial     resource would also expire.     Adaptive Learning: As AI agents interact with their environment,     other agents, and users, they can learn and adapt their behaviour.     This enables them to improve their performance, better understand     their assigned tasks, and become more effective at achieving their     goals.
- ### 10.4 Application case studies
As we have seen in the 'collaborative mixed reality' chapter, these tools are best deployed where some human conversational cues (pointing, looking etc) are required in the context of a shared task, which is mostly visual in nature. This is a surprisingly small amount of tasks, though we have seen that the emergence of AI means that increasingly natural language AI can streamline communication, while visual generative ML can suggest design alternatives or improvements based on existing data and user preferences. This is very likely to expand the use space and this section will attempt to explain how as the case studies are explained.
We will employ the acronym for collaborative virtual environment (CVE) from this stage, and it's going to come up a lot. There will be far less references in this section for brevity.
- #### 10.4.1 Classic use cases
Small teams working on product, architectural, or industrial design can benefit from CVEs that allow them to visualize, modify, and iterate on 3D models in real-time.
- #### 10.4.2 Virtual training and simulation
CVEs can facilitate skill development and training in various industries, such as healthcare, military, aviation, and emergency response. Trainees can practice procedures in a virtual environment, with natural language AI providing instructions, explanations, or feedback, and visual generative ML potentially customizing scenarios to adapt to each user's learning curve.
- #### 10.4.3 Remote teleconferencing
In situations where face-to-face communication is not feasible, CVEs can enable remote teams to work together on shared visual tasks like planning events, brainstorming ideas, or reviewing documents. Natural language AI can transcribe and analyse spoken conversations, providing real-time translations or summaries, while visual generative ML can create visual aids or dynamically update shared documents. This may especially be useful in complex multinational legal and/or negotiation applications, though very clearly the risks of using assisting ML tooling increases.
- #### 10.4.4 Virtual art & media collaboration
Artists, animators, and multimedia professionals can collaborate in CVEs to create and develop their projects, such as films, animations, or video games. Natural language AI can help in storyboarding, scriptwriting, or character development, while visual generative ML can generate new visuals or adapt existing assets based on user input and style preferences.
- #### 10.4.5 Data visualization and analysis
Small teams working with large datasets can use CVEs to visually explore and analyze data in a more intuitive and engaging way. Natural language AI can help users query and interact with the data using conversational interfaces, while visual generative ML can generate new visualizations based on patterns and trends identified in the data.
- #### 10.4.6 Education and virtual classrooms
Educators can leverage CVEs to create immersive learning experiences that engage students in collaborative activities, such as group projects, problem-solving, or scientific experiments. Natural language AI can facilitate communication, provide personalized tutoring, or assess student progress, while visual generative ML can create customized educational content based on individual needs and interests.
- #### 10.4.7 Virtual labs and scientific research
Researchers can use CVEs to conduct experiments, visualize complex data, or simulate real-world conditions in a controlled environment. Natural language AI can assist in interpreting results, automating lab protocols, or identifying research gaps, while visual generative ML can generate predictions or models based on existing data to support hypothesis testing and decision-making.
- #### 10.4.8 Media and entertainment
- #### 10.4.9 Biomedical
Collaborative Virtual Environments (CVEs) have immense potential in the fields of chemical and medical molecular modeling. By incorporating natural language AI and visual generative machine learning, these environments can revolutionize the way scientists and researchers approach complex chemical and biological problems. Here are some specific use cases:
Drug design and discovery: CVEs can enable researchers to collaboratively visualize and manipulate 3D molecular structures in real-time, identifying potential drug candidates and understanding protein-ligand interactions. Natural language AI can help users interact with the molecular data, while visual generative ML can predict potential binding sites, energetics, or toxicity profiles based on existing knowledge.
Protein structure prediction and modeling: Small teams can work together to predict protein structures, visualize folding patterns, and model protein-protein or protein-nucleic acid interactions. Natural language AI can assist in annotating and explaining the structural features, while visual generative ML can generate new structural hypotheses based on sequence alignments, homology modeling, and experimental data.
Molecular dynamics simulations: CVEs can facilitate collaboration on complex molecular dynamics simulations, allowing researchers to analyze and visualize trajectories, energetics, and conformational changes. Natural language AI can help users navigate through simulation data and identify relevant patterns, while visual generative ML can create new conformations or predict the effects of mutations on protein stability and function.
Cheminformatics and QSAR modeling: Researchers can leverage CVEs to develop and validate Quantitative Structure-Activity Relationship (QSAR) models, which predict the biological activity of chemical compounds based on their structural properties. Natural language AI can facilitate the exploration and interpretation of chemical descriptors, while visual generative ML can suggest new compounds with desired properties or optimize existing molecular scaffolds.
Metabolic pathway modeling: Small teams can work together to build and analyze metabolic pathways, integrating experimental data and computational models to understand the underlying mechanisms and predict metabolic fluxes. Natural language AI can assist in annotating and explaining pathway components, while visual generative ML can generate new pathway hypotheses or predict the effects of genetic or environmental perturbations.
Biomolecular visualization and virtual reality: CVEs can offer immersive, interactive experiences for exploring biomolecular structures and dynamics, enhancing researchers' understanding of complex biological systems. Natural language AI can provide contextual information or guide users through molecular landscapes, while visual generative ML can create new visualizations or adapt existing ones based on user preferences and insights.
Collaborative molecular docking and virtual screening: Small teams can use CVEs to perform collaborative molecular docking and virtual screening, which involve predicting the binding of small molecules to target proteins. Natural language AI can help users refine docking parameters and analyze results, while visual generative ML can generate alternative poses or suggest new compounds for screening based on user feedback and existing data. Choose a suitable mixed reality platform: Select a platform that allows the creation of simple, accessible shared mixed reality environments. Consider open-source options like Mozilla Hubs or JanusVR, which offer customizable and collaborative virtual spaces.
Integrate open-source biomed software: Incorporate open-source biomed software such as PyMOL, Chimera, or VMD for molecular visualization and analysis. These tools can be integrated into the mixed reality environment for real-time interaction, allowing students and instructors to collaboratively visualize and manipulate molecular structures.
Leverage AI and machine learning: Integrate AI and ML algorithms like those found in DeepChem, RDKit, or Open Babel to aid in the discovery and optimization of novel compounds. These tools can help predict molecular properties, perform virtual screening, and optimize lead compounds for drug development. By incorporating AI and ML, students can learn how to apply these cutting-edge techniques to real-world problems in biomedicine.
Establish a distributed proof system: Utilize a distributed proof system like the Nostr protocol to federate the small virtual classroom environments. This will allow for seamless collaboration among students and faculty while maintaining security and data integrity.
Create digital objects for interaction: Use digital objects such as 3D molecular models, virtual lab equipment, and interactive simulations to create an immersive learning experience. These digital objects can be shared and manipulated in real-time, promoting collaborative learning and problem-solving.
Implement accessible interfaces: Ensure that the virtual classroom environment is accessible to all students, including those with disabilities. Utilize AI-driven tools like StabilityAI to help with language barriers, safeguarding, and governance, enabling a more inclusive learning experience.
Foster collaboration and communication: Encourage students and faculty to collaborate on projects, share ideas, and ask questions in real-time using voice chat, text chat, or other communication tools integrated into the mixed reality environment.
Provide training and support: Offer training sessions and support materials to help students and faculty become familiar with the mixed reality environment, the integrated biomed software, and AI/ML tools.
Monitor progress and adjust as needed: Regularly review student progress, gather feedback, and adjust the virtual classroom environment as needed to ensure an effective and engaging learning experience.
- #### 10.4.10 Collaborative Design and Prototyping
Utilizing open-source systems and AI-assisted tools can enable more efficient and creative collaboration in design and prototyping processes. Teams from diverse cultural backgrounds can work together seamlessly, creating a rich pool of ideas and innovations.
- #### 10.4.11 Training, Simulation, and Education
The modular open-source system can be applied to various training, simulation, and education scenarios. By integrating AI and generative ML technologies, these tools can provide personalized learning experiences and create realistic simulations that cater to different learning styles and requirements.
- #### 10.4.12 Remote Collaboration and Teleconferencing
As remote work becomes more prevalent, the Metaverse can provide a more engaging and immersive platform for collaboration and teleconferencing. The open-source system can be adapted to serve various industries, making remote collaboration more efficient and inclusive.
- #### 10.4.13 Chemical and Medical Molecular Modeling
In fields like chemical and medical molecular modeling, the integration of AI and generative ML technologies can significantly improve collaboration and innovation. Teams can work together in immersive environments to visualize complex molecular structures, benefiting from real-time AI-generated visuals and natural language processing.
- #### 10.4.14 Creative Industries and Generative Art
The combination of AI, ML, and open-source systems can revolutionize the creative industries by offering new avenues for generative art, content creation, and collaboration. Supported creativity and augmented intelligence can break down barriers and enable artists to explore new ideas and techniques, enriching the creative landscape.
- #### 10.4.15 Case Study: Biodiversity Monitoring and Data Exchange with Isolated Communities
Biodiversity monitoring in and around isolated communities is challenging due to limited access and resources. Traditional methods rely on sporadic visits by grant-funded academics, which can introduce biases and lack regular follow-up. Engaging local communities may also introduce incentive structures and biases and may not be sustainable without continuous investment.
We propose an open-source collaboration infrastructure that leverages advanced technologies such as multi-modal large language models (LLMs), satellite communication, and cryptocurrency networks to facilitate sustainable and reliable biodiversity monitoring and data exchange in isolated communities.
- ##### Language Model and Voice Interface
A specialized multi-modal LLM can be trained on local language, culture, customs, and environmental data such as flora, fauna, biotica, soil pH, and rainfall. This LLM can be accessed through a voice interface by the local community, enabling data entry and knowledge exchange in the local language. The voice interface can help overcome literacy barriers and make the system more accessible to a diverse range of community members.
- ##### Data Collection and Storage
Photographs and metadata can be logged and collected by a remote team at a later date or uploaded regularly through a satellite link (e.g., Starlink). The data storage system can be designed to be both secure and resilient, ensuring that the collected data remains available and accessible for future analysis and decision-making.
- ##### Live Connection and Model Tuning
A live connection with the academic team allows for model tuning through prompt engineering, vector database updates, and efficient Lora models, potentially offering timely advice for ecosystem interventions. Real-time communication between the community and academic teams can help identify areas of concern and rapidly adapt the LLM to address emerging challenges.
- ##### Ecosystem Interventions
The proposed infrastructure would be particularly valuable in areas facing novel disease encroachment, invasive species, active hydrology, shifting aquatic conditions, microplastic hotspots, changing microclimates, or volcanic activity. By providing real-time advice and guidance, the LLM can help communities make informed decisions about ecosystem management and conservation efforts.
- ##### Incentives and Education
Incentivizing community engagement could be achieved by providing access to the LLM for educational purposes, as demonstrated by the refugee camp e-prize (ref). Local schools and community centers can leverage the LLM as a resource for teaching environmental stewardship and ecological awareness, while also promoting digital literacy and technology skills.
- ##### Monetization and Blockchain Integration
Monetizing these systems could involve using chaumian mints such as Cashu or Fedimint, under the control of local community leaders, mediated through the global Bitcoin satellite network (Blockstream), enabling digital dollar payments to communities via low-end mobile handsets. By integrating blockchain technology, the proposed infrastructure can ensure secure, transparent, and efficient financial transactions, while also opening up new economic opportunities for isolated communities.
- ##### Visual Training Support Systems
The infrastructure could be further extended to visual training support systems using low-cost, low-power components. These systems could provide interactive, immersive learning experiences for community members, helping them better understand the local ecosystem and develop skills in environmental monitoring and management.
- ##### Solar Infrastructure
To minimize the environmental impact and ensure energy sustainability, the proposed infrastructure can be powered by solar energy. This approach will enable the system to operate independently of local power grids, reducing the overall operational costs and maintenance requirements.
- ##### Open-Source Collaboration
By linking this case study to the open-source collaboration infrastructure discussed earlier, we can create an inclusive, permissionless, federated, and economically empowered system that addresses the challenges of biodiversity monitoring while promoting digital society values such as trust, accessibility, and governance. This collaborative approach can help drive innovation and ensure that the proposed solutions are both scalable and adaptable to the unique needs of different communities and ecosystems.
- ##### Risk Mitigation and Ethical Considerations
While implementing such an infrastructure, care must be taken to address potential unintended consequences of embedding these inference systems in communities. It is essential to involve the local communities in the development and deployment process, ensuring that their perspectives, values, and traditions are respected and preserved.
Moreover, it is crucial to establish a robust ethical framework for the use of AI technologies, considering potential issues related to privacy, data security, and cultural sensitivity. Regular audits and monitoring can be implemented to ensure that the infrastructure remains transparent, accountable, and aligned with the needs and expectations of the communities it serves.
- ##### Capacity Building and Local Empowerment
An essential aspect of this initiative is building capacity and empowering local communities to take ownership of their environment and resources. By providing training, resources, and support, the proposed infrastructure can help communities develop the skills and knowledge needed to manage their ecosystems effectively.
Furthermore, the integration of digital tools and technologies can promote digital inclusion and bridge the digital divide, giving isolated communities access to valuable information and resources while fostering a sense of global connectedness and collaboration.
- ##### Future Outlook and Potential Impact
The proposed open-source collaboration infrastructure for biodiversity monitoring and data exchange has the potential to transform how isolated communities interact with their environment, enabling them to make informed decisions about conservation and ecosystem management.
By leveraging cutting-edge technologies such as LLMs, satellite communication, and blockchain networks, this approach can create a more inclusive, transparent, and accessible system for environmental monitoring and stewardship. The successful implementation of this infrastructure could pave the way for similar initiatives in other regions and ecosystems, promoting global collaboration and innovation in the pursuit of a more sustainable and equitable world.
- ### 10.5 Overcoming Challenges and Barriers
- #### 10.5.1 Trust, Accessibility, and Governance
To create a successful open-source Metaverse, it is crucial to address trust, accessibility, and governance challenges. By integrating decentralized and secure technologies such as blockchain and distributed ledger systems, a more transparent and trustworthy infrastructure can be established.
- #### 10.5.2 Ensuring Safeguarding and Privacy Compliance
Protecting user privacy and ensuring safeguarding is vital for any digital society platform. The open-source system must be developed in compliance with legislative and cultural norms while maintaining the balance between user privacy and the need for identity verification and data management. The evidence that social media is damaging youth mental health is very compelling \[[405](https://arxiv.org/html/2207.09460v11/#bib.bibx405)\]. The Centre for Humane Technology call social media the '[first contact point](https://www.youtube.com/watch?v=xoVJKj8lcNQ) with AI'. They explains that new technologies often create an arms race. They list the negative impacts of this contact as including "information overload, addiction, doom scrolling, sexualization of kids, shortened attention spans, polarization, fake news, and breakdown of democracy". These were not the intended consequence of engineers who aimed to maximize engagement. The underlying arms race for attention led to what they call 'an engagement monster' that rewrote the rules of society.
These lessons should be learnt and the problems should be pro-actively mitigated. This proposal is [not]  a social metaverse, and deliberately limits both numbers of participants and avatar optionality.
- #### 10.5.3 Managing Scalability, Performance, and Latency
As the Metaverse continues to grow, it is crucial to ensure that the open-source system can scale effectively and maintain optimal performance. By using distributed and federated networks, the system can better manage latency and performance issues, ensuring a seamless user experience.
- #### 10.5.4 Promoting Open Standards and Interoperability
For the Metaverse to truly thrive, it is essential to promote open standards and interoperability among various platforms and systems. This can be achieved by fostering collaboration between industry stakeholders, encouraging the development of open protocols, APIs, and data standards, and actively supporting the open-source community.
- ### 10.6 Future Outlook and Potential Developments
- #### 10.6.1 AI and Generative ML Technologies
As AI and generative ML technologies continue to evolve, their integration into the Metaverse will further enhance user experiences and create new opportunities for innovation. The release of models like GPT-4 have already prompted debate about general AI \[[397](https://arxiv.org/html/2207.09460v11/#bib.bibx397), [406](https://arxiv.org/html/2207.09460v11/#bib.bibx406)\] (Figure [10.2](https://arxiv.org/html/2207.09460v11/#Ch10.F2 "Figure 10.2 ‣ 10.6.1 AI and Generative ML Technologies ‣ 10.6 Future Outlook and Potential Developments ‣ Chapter 10 Our proposition ‣ Part I State of the art")). It seems unavoidable that this will all impact on the Metaverse and digital society.
![Figure 10.2: Models exhibit uncanny behaviours.](../assets/rlhf.png) 
- #### 10.6.2 Inclusive Digital Society
By overcoming barriers to entry for emerging markets and less developed nations, a more inclusive digital society can be fostered. This inclusivity will empower new ideas and perspectives, leading to a richer and more diverse digital landscape.
- #### 10.6.3 Spatial and Augmented Reality Technologies
The incorporation of spatial and augmented reality technologies can expand the possibilities within the Metaverse, allowing for more immersive and interactive experiences. These technologies have the potential to reshape digital society and redefine the ways in which people interact with digital environments.
- #### 10.6.4 Economic Empowerment AI Actors
The creation of an open and economically empowered Metaverse, in which AI actors can mediate governance issues and participate in economic transactions, can lead to a more efficient and dynamic digital ecosystem. This integration will enable new business models and opportunities for all users, both human and AI.
- #### 10.6.5 Continuous Evolution and Adaptation
As the digital landscape continues to evolve, the open-source Metaverse system must be flexible and adaptable to meet changing needs and expectations. Continuous innovation and collaboration within the industry will be crucial for the success and longevity of the Metaverse as a transformative digital society platform.
- ### 10.7 Conclusion and Final Thoughts
- #### 10.7.1 Embracing the Open-Source Metaverse Vision
To create a truly transformative and inclusive digital society, it is essential to embrace the vision of an open-source Metaverse. By fostering collaboration, promoting open standards, and integrating advanced AI and ML technologies, the Metaverse can become a platform that serves societal and business needs.
- #### 10.7.2 Learning from Past Failures
Learning from past failures and addressing challenges head-on will be critical to the successful development of an open-source Metaverse. Trust, accessibility, governance, and safeguarding issues must be thoughtfully considered and addressed to build a secure and user-friendly platform.
- #### 10.7.3 Unlocking New Opportunities and Use Cases
The integration of AI, ML, and cutting-edge technologies within the Metaverse can unlock new opportunities and use cases across various industries, including education, research, biomedical, and creative fields. By building on a modular open-source system, these opportunities can be explored and realized to their full potential.
- #### 10.7.4 Fostering Collaboration and Inclusivity
Creating an inclusive digital society is a key goal for the open-source Metaverse. By breaking down barriers and making the platform accessible to a wider audience, new ideas and perspectives will enrich the digital landscape and drive innovation.
- #### 10.7.5 Shaping the Future of Digital Society
As the Metaverse continues to evolve and grow, it will play an increasingly important role in shaping the future of digital society. By embracing an open-source vision, overcoming challenges, and unlocking new opportunities, the Metaverse can become a powerful platform that transforms how people live, work, and interact in the digital world.
- #### 10.7.6 Industry Conversations
Continued dialogue and collaboration among industry stakeholders are vital to ensuring the successful development of the open-source Metaverse. By engaging in conversations and understanding the cautious appetite for the ideas presented, the community can work together to shape the future of digital society and overcome the challenges that lie ahead.
- ### 10.8 Software stack
This section needs building out to describe the stack and the choices made, but can be seen in Figure [10.3](https://arxiv.org/html/2207.09460v11/#Ch10.F3 "Figure 10.3 ‣ 10.8 Software stack ‣ Chapter 10 Our proposition ‣ Part I State of the art") and Figure [10.4](https://arxiv.org/html/2207.09460v11/#Ch10.F4 "Figure 10.4 ‣ 10.8 Software stack ‣ Chapter 10 Our proposition ‣ Part I State of the art").
![Figure 10.3: Pyramid showing the components for sats, stablecoins on lightning, asssets, and trust](../assets/pyramid.jpg) 
![Figure 10.4: High level overview showing the components for sats, stablecoins on lightning, asssets, and trust](../assets/HighLevelStack.png) 
At this time we favour the following component units, with alternatives in brackets.
    Collaborative space - Vircadia \[Omniverse, Open3D foundation,     Unreal\]     Distributed truth - Bitcoin testnet \[Main net\]     Digital Objects - Fedimint \[Ordinals, Pear credits, RGB\]     Messaging and sync - Nostr     Identity - Nostr \[Bluesky ION, Slashtags\]     Fiat money xfer - Fedimint \[Pear credits, RGB, Taro main net\]     Hardware signing - Seed signer \[any hardware wallet\]     Small group banking - Fediment \[chaumian ecash\]     Local wallet - [Mutiny](https://app.mutinywallet.com/) \[bitkit, and     lightning wallet\]     Machine learning text - Alpaca \[ChatGPT etc\]     Machine learning image - Stable diffusion \[midjourney, Dall-E\]     Object tracking - Nostr \[LnBits accounts\]
- ### 10.9 In camera VFX & telepresence
Designing open federated metaverse from a 25 year research foundation There are serious and under discussed natural social constraints on group behaviours, and these translate into social VR. For instance the ideal meeting size is 6, and this is naturally established in work settings. This has not translated into a metaverse setting where dozens of people routinely crash across one another. In the context of supporting a creative "backstage" world where set planning, production shots, etc can be discussed we believe we have solutions which will get the best out of distributed teams of film-makers. Leveraging the world's most powerful decentralised computing network to create scale and security without high cost The Bitcoin network is more than just a speculative money like asset, it is the most secure distributed computing system ever built. We can jump on the back of this at almost no cost to enable scale for transfer of value, trust, and digital assets of provenance. Cryptographically assured end points With the cryptography tools provided through integration of the Bitcoin network we can also use non-blockchain based secure messaging, and identity proofs. Micro transactions in collaborative spaces New tooling the space allows fractions of a pound or dollar to be exchanged between parties across the world. This means that work can be paid "by the second" both inside and outside of the metaverse. This radically improves creative microtask workflows. World leading open source machine learning and bot architectures By integrating Stablity AI tools for image generation, video processing, natural language, and speech to text / text to speech we hope to reduce friction within the backstage worlds. Creating a narrative arrow from a remote director/producer/DP, through a VP screen into a shoot, and back into a persistent metaverse shared with the public By linking across these new systems with world class telepresence research we hope to use a single digital context to support senior stakeholders, creatives, technical teams, and the wider public. New paths to monetisation and digital ownership This unified digital back end is optimised for flows of money, trust, and digital objects. This is a new area for VP. Current workstreams:
    Storyboarding with text2img and dreambooth to add talent and costume     ideas before meeting up, as demonstrated in this document     \[[407](https://arxiv.org/html/2207.09460v11/#bib.bibx407)\].     Collaborative, self hosted, high speed, low detail, economically and     cryptographically enabled set design spaces, with near instant     language translation (speech to text an speech to speech).     Micropayment for cheap international labour. Technology agnostic.     Use the screen, audio only, compressed video dial-in, headsets,     tablet rendering: (this book).     High end telepresence     \[[289](https://arxiv.org/html/2207.09460v11/#bib.bibx289),     [408](https://arxiv.org/html/2207.09460v11/#bib.bibx408),     [409](https://arxiv.org/html/2207.09460v11/#bib.bibx409),     [410](https://arxiv.org/html/2207.09460v11/#bib.bibx410)\] into the     studio/shoot from the virtual set, allowing high value stakeholders     to be 'present' on set as virtual collaborants with spatial     descrimination allowing directional queues. This involved real time     human capture like moveAI or the expensive rigs with DSLRs.     Novel render pipeline for fast turnaround of final look and feel,     taking the rough scene and applying img2img ML with the kind of     interframe consistency we are starting to see from the video     projects     \[[411](https://arxiv.org/html/2207.09460v11/#bib.bibx411)\].     Text to model pipeline for interactively building key elements with     senior stakeholders, pushed from post ideation the the pre-shoot     Unreal content creation     \[[412](https://arxiv.org/html/2207.09460v11/#bib.bibx412)\].     All assets switch over to Unreal metaverse and become consistent     (optimised) digital set which can be visited by stakeholders,     funders, VIPs etc. Public can visit later for a fee? Digital assets     can be bought from the set.
- ##### VisionFlow: Ideate - Robotic Pre-Visualization
VisionFlow: Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.
Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.
The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.
By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.
- ##### VisionFlow: Connect - Telepresence System
VisionFlow: Connect is a breakthrough system in the film industry that brings remote directors to the heart of production using augmented reality technology. This is achieved through an innovative application of the Apple Vision Pro AR headset.
In the VisionFlow: Connect system, the director, located remotely, wears an AR headset and navigates along a marked line. This line mirrors the inward-facing edge of a large-scale, wrap-around LED virtual production facility. Within the LED volume, participants can view the director's avatar, providing a sense of spatial consistency and our work interaction, crucial for effective direction.
A novel technique, \"ghost frame\" by Helios, is employed to prevent the camera within the LED volume from capturing the director's remote avatar on the LED wall. This ensures the director's virtual presence doesn't interfere with the recorded footage.
The benefits of VisionFlow: Connect are multifold. It allows senior stakeholders to manage their time more efficiently as they can direct remotely without needing to be physically present on multiple sets. Directors can interact in real-time, giving instantaneous feedback and adjustments. It also enhances directors' spatial awareness of the scene, thereby improving the decision-making process.
  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------   [Slide 1: Title]                                                                                                                                                                                                                                                                                                                                                                                                                       [Slide 2: Problem]    \"VisionFlow: Revolutionizing Virtual Production with AI and Telecollaboration\"                                                                                                                                                                                                                                                                                                                                                                                       \"The current ICVFX workflow is time-consuming, costly, and requires specialized software knowledge. Remote collaboration in virtual production is challenging, often breaking the flow of communication and limiting the ability to convey spatial intent.\"   [Slide 3: Solution]                                                                                                                                                                                                                                                                                                                                                                                                                    [Slide 4: Market Size]    \"VisionFlow aims to streamline the virtual production process by integrating open-source machine learning tools and robot control software. This innovative approach inverts the existing ICVFX workflow, allowing rapid ideation, horizontal scaling, and expanded access to content creators. Furthermore, our ghost frame technology enables seamless remote collaboration, allowing remote stakeholders to interact with the set in a spatially coherent way.\"   \"The virtual production market is rapidly growing, driven by the increasing demand for high-quality visual effects and the rise of remote work. Our solution targets film studios, independent content creators, and remote collaborators.\"   [Slide 5: Business Model]                                                                                                                                                                                                                                                                                                                                                                                                              [Slide 6: Go-to-Market Strategy]    \"We will generate revenue through software licensing, cloud-based services, and professional services for setup and training, and our own in house motion control robotics offering\"                                                                                                                                                                                                                                                                                 \"Our initial focus will be on early adopters in the film industry who are already using virtual production techniques. We will also leverage the open-source Flossverse telecollaboration stack to expand our reach.\"   [Slide 7: Competitive Landscape]                                                                                                                                                                                                                                                                                                                                                                                                       [Slide 8: Team]    \"While there are other virtual production solutions on the market, none offer the unique combination of AI-driven scene generation, inverted ICVFX workflow, and seamless remote collaboration that VisionFlow does.\"                                                                                                                                                                                                                                                \"Our team combines expertise in AI, virtual production, and telecollaboration, positioning us uniquely to execute on this vision.\"   [Slide 9: Financial Projections]                                                                                                                                                                                                                                                                                                                                                                                                       [Slide 10: Current Status and Milestones]    \"We project rapid growth as we capture a significant share of the expanding virtual production market.\"                                                                                                                                                                                                                                                                                                                                                              \"We have already developed an MVP using the Flossverse stack and are now focused on refining the integration and licensing elements of our software.\"   [Slide 11: Ask]                                                                                                                                                                                                                                                                                                                                                                                                                      [Slide 12: Closing Remarks]    \"We are seeking investment to accelerate our development, expand our team, and bring our innovative solution to market.\"                                                                                                                                                                                                                                                                                                                                             \"In essence, VisionFlow is poised to revolutionize the virtual production industry by leveraging AI to streamline workflows and enable seamless remote collaboration. With your investment, we can bring this vision to life.\"   ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- #### 10.9.1 visionflow:knoWhere
The ultimate goal is to create a seamless, highly personalized visitor experience that evolves and continues before, during, and after a visit to a digital exhibition. This level of personalization is only made possible through the integration of advanced AI technology, biometrics, and a deep inferred understanding of individual preferences and behaviours.
- ##### Key Ideas
1.  [1.]      [Leveraging AI and Contextual Data:]  The     venue will use AI and contextual data to create dynamic narratives     and activities tailored to each visitor in real-time. This will     revolutionize the resort experience, making it highly personalized     and immersive. However, the implementation of AI must be mindful of     privacy concerns and be done in a way that respects the data     sovereignty of the guests. 2.  [2.]      [Tailored Personalization:]  Visitors should     have the ability to opt into different levels of personalization.     Some may want a fully immersive, personalized experience, while     others may prefer a more 'hands off' experience. This is an     important aspect of respecting individual preferences and ensuring     that all visitors feel comfortable and catered for. 3.  [3.]      [Communication Devices:]  Various     communication devices could be utilized within the resort to     facilitate interactions between visitors and the AI system. These     could include badges, wands, glasses, headphones, etc. Each of these     devices would contribute to the immersion and thematic consistency     of the resort while serving a practical purpose. 4.  [4.]      [Biometrics:]  The use of biometrics such as     gaze tracking and gesture recognition could allow the AI to     understand visitor preferences passively. This technology could be     incorporated in a non-intrusive way to augment the guest experience     without breaching privacy. 5.  [5.]      [Data Extraction:]  Visitors should have the     ability to extract their distilled data or creations, enabling them     to continue their vistor experience at home. This could also open up     new possibilities for visitors to create and share their own     narratives based on their visit experiences. To be clear this should     not be the raw data supplied to the venue inferencing engines (which     should be destroyed soon after use), but rather a distilled     narrative of the inference from the system. 6.  [6.]      [Data Privacy:]  Data sharing should be     underpinned by robust privacy controls to ensure guest data     sovereignty. It's crucial to maintain the trust of the visitors by     demonstrating a strong commitment to privacy. This should be     externally audited on a regular cadence. 7.  [7.]      [Continuous Experience:]  The visitor     experience should feel continuous before, during, and after the     visit. However, it's important to manage guest expectations and     avoid over promising pre-visit AI interactions. Ensuring a smooth     transition between these stages will enhance the overall guest     experience. 8.  [8.]      [Hyper-Personalization:]      Hyper-personalization should span the venue. This level of detail     will ensure each guest has a unique and highly personalized     experience. 9.  [9.]      [Adaptive and Immersive Experiences:]  The     core aim should be to craft continuously adaptive and immersive     experiences based on visitor needs and implied preferences. By doing     so, the venue can ensure each visitor has a unique, enjoyable, and     highly memorable experience, supportive of return visits.
The integration of these concepts will require careful planning and execution, but the result could be a venue experience like no other, one that caters to each individual guests and provides an experience that extends beyond the confines of the experience itself.
- ##### Multiview barrier lenticular
- ##### Background
Ubiquitous display technology, which allows different personalized views for multiple people on the same screen, has the potential to disrupt the way visitors interact and experience venues and exhibits. The displays can use techniques like lenticular lenses, or other steerable light, to send different light to viewers' eyes, allowing for discrete, customized views.
- ##### Technical Overview
The following display technologies have been identified as suitable for implementation:
    Lenticular lens arrays: By placing an array of magnifying lenses     over the screen, these displays direct light from alternating     columns of pixels toward the left and right eyes to create a     stereoscopic 3D image without glasses. There are several suppliers     of this technology, mainly for the events market. It seems that     churn of these companies is relatively high, with few demonstrating     longevity.     Parallax barriers: These displays have a layer of opaque and     transparent slits over the LCD matrix that directs different pixel     columns to each eye, creating a stereoscopic 3D image without     glasses. Alioscopy is known to use this approach, along with eye     tracking technology. They have been in business for decades and are     a good case study, but engaging with a research partner in China is     likely the best medium terms approach.     These display consists of a large lenticular lens sheet or array of     smaller tiled lenticular lenses mounted in front of a     high-resolution LED. The lenticular lenses are cylindrical and     arranged vertically, with each lens covering multiple pixel columns     of the display.     Behind the lens array, the display content is formatted into     vertical interleaved channels, with each channel containing a     slightly different perspective view of the 3D stereoscopic image.     The different perspective views are calculated in real-time based on     the tracked head positions of multiple viewers in front of the     display.     As light from the display pixels passes through the cylindrical     lenses, it is refracted into multiple viewing zones in front of the     screen. Each viewing zone contains a specific view channel, so each     eye of each viewer sees the perspective that matches their position.     This creates a glasses-free 3D effect with motion parallax as     viewers move their heads.     The viewer head tracking system uses camera and computer vision     techniques to determine the 3D positions of each viewer's eyes in     the space in front of the display. The changing viewer positions are     fed to the display rendering system to compute the proper     perspective views and adjust the lenticular flaps as needed.     This lenticular 3D display with dynamic view steering provides     illusion of depth for multiple viewers simultaneously, creating an     immersive large-screen 3D experience without the need for special     glasses. The real-time tracking and rendering system updates the     content smoothly as the viewers move around, maintaining the stereo     3D perspectives tailored individually to each viewer's changing     position.
- ##### Tracking Technologies
For personalization, tracking viewers' eyes, face, gestures, etc., is necessary. This can be done with cameras and computer vision algorithms, employing techniques like mesh abstraction for body tracking, facial landmark recognition, gaze estimation, micro expression recognition, and gross gesture detection.
- ##### AI Integration
AI can be integrated to steer personalized narratives and experiences subtly in the background or provide interactive moments. The AI backend can use game engines like Unreal Engine or Unity to render personalized content dynamically, allowing for real-time adaptation to the viewer's reactions.
- ##### Privacy and Security
The tracking data provides extremely valuable insights for personalizing experiences but raises significant privacy concerns. Thoughtful design around privacy and security, including data segmentation, auditing, and transparency, is critical to protect user data and ensure compliance with privacy regulations.
- ##### Technical Challenges
There are technical challenges in achieving dense personalized displays, especially for a large number of viewers. As of now, creating a personalized display for up to 5 people is feasible, but scaling up requires a substantial budget and careful planning. Fortunately both of these seem available and it seems timely to look at this option.
- ##### Proof of Concept
Starting with a small-scale proof of concept for up to 5 people would allow for demonstration of the capabilities and building stakeholder confidence. This would also provide valuable insights into the technical and logistical challenges that may arise during larger-scale implementation.
- ##### Future Developments
The display technology is rapidly evolving, with new advancements in resolution, refresh rates, brightness, and tracking accuracy. As the technology matures, there will be more opportunities to enhance the personalized experiences. This system would allow multiple viewers to see different images or perspectives from the same display, enhancing the interactive and educational value of the exhibit. Mollick et al. have done some lovely actionable work on the pedagogical implications of chatbots \[[413](https://arxiv.org/html/2207.09460v11/#bib.bibx413), [414](https://arxiv.org/html/2207.09460v11/#bib.bibx414), [415](https://arxiv.org/html/2207.09460v11/#bib.bibx415)\]. This could transform the way visitors engage with exhibits, providing a more immersive and personalized experience.
- ##### Alioscopy
Alioscopy uses a different approach than lenticular lenses for their glasses-free 3D displays. Their screens contain a parallax barrier - a layer of opaque and transparent slits - over the LCD matrix. This directs different pixel columns to each eye, creating a stereoscopic 3D image without glasses.
Their displays also incorporate proprietary eye tracking technology. An infrared camera follows the viewer's head position, automatically adjusting the angle of the projected 3D image for optimal viewing. This compensates for display viewing angle limitations.
Alioscopy's recent prototypes feature very high resolution like 4K and 8K to improve 3D image quality. Their barriers and tracking algorithms are precisely tuned to the display characteristics and desired viewing parameters.
- ##### Pitch section
Personalised emergent narratives for our visitors. What problem does the user, business or industry have that you want to solve?
For today's digital experience venue managers navigating the complexities of providing unique experiences, our AI solution, KnoWhere, offers a unique approach which will result in the capability to enhance visitor experiences. By utilising images from on-premise cameras, we enable to leverage data on visitors attention. Our solution's unique value propositions include spatial and attention tracking through AI, because of our ability to understand the needs of experience designers.
It works like this: Combine personal data, with visitor gaze Provide location and attention data stream Venue provides this to experience designers Designers build incredible emergent journeys
We believe this solution will impact our business/industry by: Elevating interactions through personalisation Making attention in physical spaces quantifiable Providing feedback data to experience designers
We will measure our impact by: Performing A/B testing on visitors engagement This can be a KPI that changes, ex: a productivity score - or it can be an amount saved because of the soluion
Describe what data is behind this AI model? Alphapose (2) Insightface (5)
Rate the quality/quantity of each point of data from 1-5 (1 being little data / low quality -- 5 being lots of data / high quality)
What will be the biggest challenge in implementing the AI model? Real time pose engine is noncommercial Occlusion can be tricky with space constraints The rich dataset is a privacy concern
Here are some areas to think about in terms of challenges:
Data: How much data exists? How representative is it of what we're trying to model? Are there issues in how it is collected which could impact the model? Is it likely to contain any missing values? Adoptance from users/customers Will it be easy to get people to use the AI in their business?
Governance Is the data accessible and are you allowed to use it? Who is responsible once the AI model is in use? How will make the final decisions?
Impact of solution What do we know about the need for this type of solution - is it nice to have or need to have? Can we find out if we don't know? Feasibility What will be the biggest challenge in implementing an AI solution to solve this problem? Can the issue / problem we're solving actually be measured / forecasted?
\"Our goal is to empower venue owners to provide an advanced platform that allows world class exhibition creators to tailor unique experiences for each visitor. This enables the crafting of rich, interconnected stories for groups of people, all while ensuring unforgettable, safe experiences for individuals and families.
- ### 10.10 Accessible metaverse for pre-viz
Pre-visualization (or \"pre-viz\") is a process in which a rough simulation of a visual effect or scene is created prior to its actual production. In the context of LED wall virtual production, pre-viz refers to the creation of a 3D representation of a virtual environment, including the placement of cameras, actors, and other elements, that is then used to plan and test the visual effects and lighting for a live-action scene that will eventually be shot in front of an LED wall.
The pre-viz process allows filmmakers and visual effects artists to experiment with different camera angles, lighting, and visual effects before committing to a final version. This helps to save time and resources during actual production by reducing the need for multiple takes or re-shoots. Additionally, it allows the filmmakers to see how the final product will look before committing to it, which can help to avoid costly mistakes or changes down the line.
The LED wall virtual production process typically involves using a combination of 3D animation software, motion capture technology, and real-time rendering to create a virtual environment that accurately reflects the physical environment in which the scene will be shot. The pre-viz process is then used to plan and test the various visual effects, lighting, and camera angles that will be used in the final production.
Our collaborative software stack is potentially ideally suited to some of this pre-viz work, especially when combined with the power of machine learning, and live linked into Unreal so that changes by stakeholders enter the pre-production pipeline in a seamless way.
- ### 10.11 Novel VP render pipeline
Putting the ML image generation on the end of a real-time tracked camera render pipeline might remove the need for detail in set building. To describe how this might work, the set designer, DP, director, etc will be able to ideate in a headset based metaverse of the set design, dropping very basic chairs, windows, light sources whatever. There is -no need- then to create a scene in detail. If the interframe consistency (img2img) can deliver then the output on the VP screen can simply inherit the artistic style from the text prompts, and render production quality from the basic building blocks. Everyone in the set (or just DP/director) could then switch in headset to the final output and ideate (verbally) to create the look and feel (lens, bokeh, light, artistic style etc). This isn't ready yet as the frames need to generate much faster (100x), but it's very likely coming in months not years. This "next level pre-vis" is being trailed in the Vircadia collaborative environment described in this book, and can be seen illustrated in Figure [10.5](https://arxiv.org/html/2207.09460v11/#Ch10.F5 "Figure 10.5 ‣ 10.11 Novel VP render pipeline ‣ Chapter 10 Our proposition ‣ Part I State of the art").
![Figure 10.5: Top panel is a screen grab from Vircadia and the bottom panel is a quick pass through img2img from Stable Diffusion.](../assets/vircadiaSD.jpg) 
This can be done now through the use of camera robots. A scene can be built in basic outline, the camera tracks can be encoded into the robot, and the scene can be rapidly post rendered by Stability with high inter frame consistency.
With the help of AI projects such as [LION](https://nv-tlabs.github.io/LION/) it may be possible to pass simple geometry and instructions to ML systems which can create complex textured geometry back into the scene.
![Figure 10.6: Robot VP](../assets/robotVP.jpg) 
- ### 10.12 Money in metaverses
- ##### Global collaboration and remuneration
In the book "Ghosts of my life" \[[416](https://arxiv.org/html/2207.09460v11/#bib.bibx416)\] Fisher asserts that there has been a slowing, even a 'cancellation' of creative progress in developed societies, their art, and their media. His contention is that neoliberalism itself is to blame. He says\ ["It is the contention of this book that 21st-century culture is marked by the same anachronism and inertia which afflicted Sapphire and Steel in their final adventure. But this status has been buried, interred behind a superficial frenzy of 'newness', of perpetual movement. The 'jumbling up of time', the montaging of earlier eras, has ceased to be worthy of comment; it is now so prevalent that it is no longer even noticed."] 
It is the feeling of the authors of this book that the creative and inspirational efforts of the whole world may be needed to heal these deep wounds. It is possible that by connecting creatives with very different global perspectives, directly into 'Western' production pipelines, that we will be able to see the shape of this potential.
- #### 10.12.1 ML actors and blockchain based bots
Stablity AI is an open source imitative to bring ML/AL capabilities to the world. This is a hugely exciting emergent area and much more will be developed here.
- #### 10.12.2 AI economic actors in mixed reality
AI actors can now be trusted visually \[[417](https://arxiv.org/html/2207.09460v11/#bib.bibx417)\]. We have some thinking on this which links the external web to our proposed metaverse. There is work in the community working on economically empowered bots which leverage Nostr and RGB to perform functions within our metaverse, and outside in the WWW, as well as interacting economically through trusted cryptography with other bots, anywhere, and human participants, anywhere. This is incredibly powerful and is assured by the Bitcoin security model. Imagine being able to interact with a bot flower seller representing all the real world florists it had found. In the metaverse you could handle the flowers and take advice and guidance from the bot agent, then it would be able to take your money to buy you flowers to send to a real world address, and later find you to tell you when it's delivered. These possibilities are endless. The AI chat element, the AI translation of images on websites to 3D assets in the Metaverse are difficult but possible challenges, but the secure movement of money from the local context in the metaverse to the real world is within reach using these bots, and they are completely autonomous and distributed.
- ### 10.13 Our socialisation best practice
- ##### Identity
We will base our identity and object management on Nostr public/private key pairs. The public key of these enable lightning based exchange of value globally. we plan to operate Nostr in multiple modes. Linking flossverse "rooms" will be a Nostr bot to bot system within the private relay mesh. This can also synchronise large amounts of data by leveraging torrents [negotiated by Nostr](https://iris.to/#/settings). Human to human text chat across and within instances is two 'types' kind of private nostr tag within the private relays mesh. External connectivity to web and nostr apps is just the public relay tags outbound. We don't need to store data external to the flossverse system, though access is obviously possible through the same torrent network.
- ##### Webs of trust
Webs of trust will be built within worlds using economically costly (but private) social rating systems, between any actor, human or AI. It should be too costly to attack an individual aggressively. This implies an increased weighting for scores issued in short time periods. Poorly behaving AI's will eventually be excluded through lack of funds.
- ##### Integration of 'good' actor AI entities
Gratitude practice should be encouraged between AI actors to foster trust and wellbeing in human observers. "It's nice to be nice" should be incentivised between all parties". This could include tipping and trust nudging through the social rating system. Great AI behaviour would result in economically powerful entities.
- #### 10.13.1 Emulation of important social cues
- ##### Behaviour incentives, arbitration, and penalties
Collapses of trust and abuse will trigger flags from ML based oversight, which will create situational records and payloads of involved parties to unlock with their nostr private keys. ML red flagged actors will be finacially penalised but have access to human arbitration using their copy of the data blob. Nothing will be stored except by the end users.
- #### 10.13.2 Federations of webs of trust and economics
Nostr is developing fast enough to provide global bridges between metaverse instances.
- ### 10.14 Security evaluation
As part of developing our stack we will penetration test the deployment as detailed using [Hexway](https://hexway.io/)
- ### 10.15 Potential applications
    Art / NFT galleries with instant sales
This application allows artists and content creator communities to display and sell NFT and fungible art to global consumer audiences, instantly.
    Large scale conference center
    -   [[--] ]      -   [[--] ]      -   [[--] ] 
In a hypothetical virtual conference centre a true marketplace of ideas could be enacted, with participants being paid directly by their audience based on the proximity to the presentation.
    -   [[--] ]          Global social puzzle gaming with prizes     -   [[--] ]          Music festivals and gigs - Pay live artists and DJs in real time         depending on location within the extended landscape of the         venue. Split to music producer a portion of the value     -   [[--] ]      -   [[--] ]      -   [[--] ]          Mixed reality live immersive MMORG games     -   [[--] ]          Bingo and mass participation gameshows     -   [[--] ]          Immersive brand storytelling metaverses     -   [[--] ]      Debating townhall meetings (with voting etc)     Mixed reality information metaverse
    -   [[--] ]          AR based city tours with collectibles     -   [[--] ]          AR based collectibles for trails and heritage (museums,         libraries) with location specific donations.     -   [[--] ]          Proxy for physical market     -   [[--] ]          AR home delivery market interface within physical marketplaces     Global course / Education provision
    -   [[--] ]          Explore the universe as a group of spaceship or planet         characters     -   [[--] ]          Explore biology and physics at a microscopic and nanoscopic         level     Micro tasking marketplace     Micro remittance role sharing (business PA / reception etc)     Careers fair with credential passing     Auctions in mixed reality     Gambling, betting markets, and financial leverage markets
- #### 10.15.1 Global cybersec course delivery
Isolating and building out one example here:
    Elements for the infrastructure: Economic layer, asset layer,     content interface, user management, data storage, microsites loaded     in Wolvin and webm, accessibility schema, network security, backups,     secure messaging. Deployable framework with high modularity. Some     more ossified elements for surity, some less so for malleability and     open opportunity. Figure     [10.9](https://arxiv.org/html/2207.09460v11/#Ch10.F9 "Figure 10.9 ‣ 10.16 notes for later ‣ Chapter 10 Our proposition ‣ Part I State of the art").     Course delivery in XR, how to we develop a platform, marketplace,     framework for open contribution.     WebXR, Vircadia, any snap in metaverse middleware that is free and     open source (action to compare the two).     Define an interface schema for bolting in any commercial or FOSS     metaverse engine.     VR marketplace (outside the scope of the VR engine) without a     trusted third party.     Cryptographically managed learning deliverables (coursework as NFT).     Secure messaging and group messaging using cryptographic keys. Check     this stuff with the distributed computing science people in the     group (action on John)     work toward an exemplar MVP which is then \"in the wild\"     Define scheme, documentation, best practice, interfaces, functional     objects, pedagogy, accessibility, multi-language.     Define user management system for educators and client learners.     Identify the pain points which current FOSS elements which need     development time/money     separate the UI/engine from the graphical assets, and the     educational / pedagogical components, accessibility, and the value     and asset transfer layers.     Desktop systems are the primary target (low end system)     define schema for accessibility. Colour, subtitles, immersion     concerns which can be applied to metaverse rooms through API?     Start to define the hybrid presentation model we favour. Avatars?     Micro sites? A combination of the two? Balance of guided vs unguided     experience. Do we need to test the correct way to do delivery? Is     there prior art we can draw on? I feel I should know. Is this part     of the research that's being done here?     Big work package on schema vs key and user management to enforce     rules in spaces. Only participants who have provably paid should     have access to learning material, the ability to input into the     assessment system, and the tokenised learning outcome 'NFT' or     proof.     Proof that XR system improve learning outcomes. Also that the     proposed systems for micro-transactions and user and schema     management give additional headroom for teaching.
- #### 10.15.2 Subvocalisation with LLMs
- #### 10.15.3 AI 3D objects in metaverse
Industry insiders I have talked to say they thing the Nvidia Omniverse initiative is failing, and this has been my impression too. The results can be seen in Figure [10.7](https://arxiv.org/html/2207.09460v11/#Ch10.F7 "Figure 10.7 ‣ 10.15.3 AI 3D objects in metaverse ‣ 10.15 Potential applications ‣ Chapter 10 Our proposition ‣ Part I State of the art").
![Figure 10.7: Omniverse allows voice created 3D models.](../assets/nvidiavoice.jpg) 
Their voice to facial animation stack Audio2Face is outstanding, but it's unclear how this might challenge Unreal's Metahuman. More promisingly Nvidia have a new video to NeRF to mesh technique called Neuralangelo \[[418](https://arxiv.org/html/2207.09460v11/#bib.bibx418)\] which knocks everything else out of the park.
This area of 3D generation from various high level inputs is moving [very]  fast and much of the high end stuff is coming as software that can be built on (like NeRF). An open source text to model pipeline for interactively building key assets for pre-viz, or project planning has appeal \[[412](https://arxiv.org/html/2207.09460v11/#bib.bibx412)\]. There's a lot here but you'd need to be sure that you're not competing directly with Omniverse, by differentiating the product.
- #### 10.15.4 Digital collectibles for events
I've been following the NFT and digital objects space since it started, but I have no interest in Ethereum or other NFT technologies because I have been waiting for what I regard as better technology. By luck my preferred option is going live as I write this document, and I am a tester on the programme. I have been talking to those teams since early 2021 and I think I understand it fairly well. There is a first mover advantage here. [Digital objects on Bitcoin](https://coinmarketcap.com/alexandria/article/bitcoin-is-now-the-second-most-popular-blockchain-for-nfts) are the second most popular route after Ethereum now, and might overtake. They split into a bunch of catagories:
    Ordinals inscriptions, which I have been following since they were     posited. I have minted one successfully, but I consider them     wasteful and expensive for commercial projects with large audiences.     BRC-20 tokens. These are comparatively new, again leveraging the     ordinal technique. All this is written up in detail in my book and     it's all over the internet. BRC-20 creates "batches" of fungible     tokens and can be approximated to event tickets, unique but     interchangable within their context. They are trivial to make but I     am unsure of the business case as they don't     [trigger]  that unique thing that people seem     to like. They're the new meme pump and dump token of choice, with     associated negative press in tow.     Taproot Assets. This is working on testnet still I think, and is a     naked copy of RGB backed by shifty stablecoin types. Might win     because of money, worth watching.     Pear credits. I think these are failing to get traction but are an     honourable mention.     Tokens on Liquid sidechain. Fringe, complex, but very heavily     invested in and developing quietly makes this one to watch longer     term.
My use case for these things has always been the cheap creation of a universal provenance for any and all objects and actions in a persistent digital collaborative space. Unlike Ethereum they're basically free, which opens up huge opportunity space. I can unpack this but it really needs to be application led.
- ##### Preferred options: LNP/BP and RGB
[LNP/BP](https://giacomozucco.com/layers-before-bitcoin) is a non profit standards organisation in Switzerland which contributes to open source development of Bitcoin layer 3 solutions into the Lightning protocol, and Bitcoin protocol (LNP/BP). One of the core product developments within their work is the ['RGB' protocol](https://www.rgb.tech/), which is somewhat of a meaningless name, evolved from "coloured coins" which were an early tokenised asset system on the Bitcoin network. RGB represents red, green, and blue. The proposal is built upon research by [Todd](https://petertodd.org/2016/commitments-and-single-use-seals) and [Zucco](https://giacomozucco.com/#intro). RGB is regarded as arcane Bitcoin technology, even within the already rarefied Bitcoin developer communities. Zucco provides the [following explanation](https://bitcoinmagazine.com/culture/video-interview-giacomo-zucco-rgb-tokens-built-bitcoin):
["When I want to send you a bitcoin, I will sign the transaction, I will give the transaction only to you, you will be the only one verifying, and then we'll take a commitment to this transaction and that I will give only the commitment to miners. Miners will basically build a blockchain of commitments, but without the actual validation part. That will be only left to you. And when you want to send the assets to somebody else, you will pass your signature, plus my signature, plus the previous signature, and so on."] 
This is non-intuitive explanation of Todds 'single-use-seals', applied to Bitcoin, with the purpose of underpinning arbitrary asset transfer secured by the Bitcoin network. In this model the transacting parties are the exclusive holders of the information about what the object they are transferring actually represents. This primitive can (and has) been expanded by the LNP/BP group into a concept called 'client side validation'. It's appropriate to explain this concept several times from different perspectives, because this is potentially a profoundly useful technology for metaverse applications.
    A promise is made to spend a multi output transaction in the future.     This establishes the RGB relationships between the parties.     One of the pubkeys to be spent to is known by both parties.     The second output is unknown and is a combination of the hash of the     state, and schema, from the operation which has been performed.     When the UTXO is spent the second spends pubkey can be processed     against the shared data blob to validate the shared state in a two     party consensus (sort this out, it's nonsense).     This is now tethered to the main chain. Some tokens from the     issuance have gone to the recipiant, and the remainder have gone     back to the issuer. More tokens can be issued in the same way from     this pool.     A token schema in the blob will show the agreed issuance and the     history back to the genesis for the token holder.     The data blob contains the schema which is the key to RGB functions     and the bulk of the work and innovation.     Each issuance must be verified on chain by the receiving party.
This leverages the single-use-seal concept to add in smart contracts, and more advanced concepts to Bitcoin. Crucially, this is not conceptually the same as the highly expressive 'layer one' chains which offer this functionality within their chain logic. In those systems there is a globally available shared consensus of 'state'. In the LNP/BP technologies the state data is owned, controlled, and stored by the transacting parties. Bitcoin provides the crytographic external proof of a state change in the event of a proof being required. This is an elegant solution in that it takes up virtually no space on the blockchain, is private by design, and is extensible to layer 2 protocols like Lightning.
This expanding ecosystem of client side verified proposals is as follows:
    RGB assets are fungible tokens on Bitcoin L1 and L2, and non     fungible Bitcoin L1 (and somewhat on L2).     Bifrost is an     [extension](https://github.com/LNP-BP/presentations/blob/master/Presentation%20slides/Bifrost.pdf)     to the Lightning protocol, with it's own Rust based node     implementation, and backwards compatibility with other nodes in the     network. This means it can transparently participate in normal     Lightning routing behaviour with other peers. Crucially however it     can also negotiate passing the additional data for token transfer     between two or more contiguous Bifrost enabled parties. This can be     considered an additional network liquidity problem on top of     Lightning, and is the essence of the "Layer 3" moniker associated     with LNP/BP. It will require a great number of such nodes to     successfully launch token transfer on Lightning. As a byproduct of     it's more 'protocol' minded design decisions Bifrost can also act as     a generic peer-to-peer data network, enabling features like Storm     file storage and Prometheus.     [AluVM](https://www.aluvm.org/) is a RISC based virtual machine     (programmable strictly in assembly) which can execute Turing     complete complex logic, but only outputs a boolean result which is     compliant with the rest of the client side validation system. In     this way a true or false can be returned into Bitcoin based logic,     but be arbitrarily complex within the execution by the contract     parties.     Contractum is the proposed smart contract language which will     compile the RGB20 contracts within AluVM (or other client side VMs)     to provide accessible layer 3 smart contracts on Bitcoin. It is a     very early proposal at this stage.     Internet2: "Tor/noise-protocol Internet apps based on Lightning     secure messaging     Storm is a lightly specified escrow-based bitcoin data storage layer     compliant with Lightning through Bifrost.     Prometheus is a lightly specified multiparty high-load computing     framework.
Really, any compute problem can be considered applicable to client side validation. In simplest terms a conventional computational problem is solved, and the cryptographically verifiable proof of this action, is made available to the stakeholders, on the Bitcoin ledger.
Less prosaically, at this stage of the project the more imminent proposed affordances of LNP/BP are described in 'schema' [on the project github](https://github.com/LNP-BP/LNPBPs). The most interesting to the technically minded layperson are:
    [RGB20](https://github.com/LNP-BP/LNPBPs/blob/master/lnpbp-0020.md)     fungible assets. This could be stablecoins like dollar or pounds     representation. Bitfinex exchange [have     code](https://github.com/RGB-Tools/rgb-lightning-sample) which     already works with RGB to transmit Tether stablecoins on testnet.     This is a huge application area for Bitcoin, and similar to Omni,     which will also be covered next.     [RGB21](https://github.com/LNP-BP/LNPBPs/blob/master/lnpbp-0021.md)     for nonfungible tokens and ownership rights. In principle BiFrost     allows these to be transferred over a the Lightning network,     significantly lowering the barrier to entry for this whole     technology. DIBA [have this technology working](https://diba.io/) on     testnet.     [RGB22](https://github.com/LNP-BP/LNPBPs/issues/29) may provide a     route to identity proofs. This is covered in detail later.
Federico Tenga is CEO of 'Chainside' and an educator and consultant in the space. He has written an up-to-date ["primer"](https://medium.com/@FedericoTenga/understanding-rgb-protocol-7dc7819d3059), which is still extremely complex for the uninitiated, but does capture how the RGB token transfer system works. That medium article also touches on Taro, which is next.
- ##### DIBA and Unique Digital Assets
[DIBA]  is a pioneering digital asset marketplace, powered by the [RGB Smart Contract Protocol](https://www.rgb.tech/). It permits the creation and direct transaction of Unique Digital Assets (UDAs), akin to Non-Fungible Tokens (NFTs), on Bitcoin without the necessity of other tokens. UDAs are special digital assets linked to a Bitcoin UTXO (unspent transaction output). These assets embody distinctive attributes like ownership, transferability, and divisibility, and remain under the full control and ownership of their creators.
Through DIBA, users can explore, purchase, and sell a vast array of UDAs, taking advantage of the robustness and permanence of the Bitcoin blockchain. DIBA's innovation extends to the integration of a Lightning layer 2 solution, aiming to facilitate faster and more affordable transactions.
Assets minted via DIBA are bound to Bitcoin's base layer with an on-chain UTXO. They are subsequently stored on the Arweave permaweb alongside a cryptographic hash, which, combined with a digital signature, can validate their authenticity. The RGB Smart Contract Protocol executes UDA transactions via BitMask, a wallet engineered by the DIBA Team.
- ##### BitMask
[BitMask]  is a browser extension wallet birthed by DIBA, intended for decentralized applications on Bitcoin. It grants access to Bitcoin Finance, UDAs, and more, utilizing the RGB protocol. It delivers comprehensive financial autonomy with its taproot-enabled Bitcoin and Lightning Network wallet, establishing it as a gateway to DeepWeb3 on Bitcoin. More details can be found on Bitmask.app.
- ##### DIBA's Launch and Marketplace Timelines
DIBA has initiated an Open Marketplace for Beta testing as of April 2022, available on Bitcoin testnet. The full launch on Bitcoin mainnet is expected to occur in the second or third quarter of 2022. Submissions for the DIBA Curated Marketplace are currently open, allowing interested individuals to apply as an Artist or a Curator.
These developments represent a substantial expansion of the capabilities within the Bitcoin network and further attest to the potential of the LNP/BP's work. Notably, DIBA [has this technology working](https://diba.io/) on testnet. The extensive application areas for Bitcoin, such as the transmission of Tether stablecoins on testnet by Bitfinex exchange [via RGB](https://github.com/RGB-Tools/rgb-lightning-sample), emphasize the potential impact of these advancements.
- #### 10.15.5 Product Design
- ##### User Journey
    [Discovery & Onboarding] : User downloads and     opens the app on their phone or accesses the web application on     their computer.     [Interaction with LLM] : User starts     conversing with the open-source AI large language model (LLM).     [Storyboard Creation] : User co-creates the     story arc with the LLM.     [Feedback & Iteration] : User can provide     feedback on the story subdivisions and the balance between visual     narrative, explanatory text, and dialogue.     [Art Style Selection] : User selects an     artistic style for the storyboard and provides additional contextual     information.     [Preliminary Image Generation] : The image AI     server generates five visual styles based on the chosen art style     and additional context.     [User Interaction with Panels] : The app     generates ten blank panels with a small contextual synopsis each.     [Initial Image Rendering] : The Image AI uses     the user-provided data to render ten panels into a single 1024x1024     image.     [Final Image Rendering & Text Placement] :     The AI subdivides and upscales the chosen image into ten     high-quality panels.     [Final Review & Confirmation] : After the     user has arranged the text to their liking, the AI performs a final     rendering pass to create a multi-page comic book-style PDF.     [Purchase/Download] : The user has the     option to purchase the PDF as a DIBA-based NFT (UDA), or simply save     it to their phone or computer.
- ##### Technical Overview
    [Frontend] : A web and 'progressive web app'     mobile interface with a clean, intuitive style for user interaction.     [Backend] : The application leverages an     open-source LLM trained through Lora and aligned with new [CFG     techniques](https://arxiv.org/abs/2306.17806), for the narrative     portion of the comic. This will run on a Lamda cloud compute system,     initially an A100 and as the user base scales an H100. It also uses     a highly capable Nvidia H100 based server system with open-source     generative art capabilities ([ComfyUI     API](https://github.com/comfyanonymous/ComfyUI/blob/master/script_examples/basic_api_example.py)     for image rendering. The rental cost per year of these systems is     around \$30k, without administration costs.
- ##### User Stories
    As a user, I want to interact with an AI to create a storyboard so     that I can craft a narrative that matches my vision.     As a user, I want to provide feedback on the AI-generated     subdivisions of my story to ensure they align with my creative     goals.     As a user, I want to choose an art style for my storyboard to suit     my aesthetic preferences.     As a user, I want to draw and add descriptive text to my panels to     give more context to the AI.     As a user, I want to choose my favourite among multiple AI-generated     renderings to have a say in the final visual output.     As a user, I want to arrange text assets on my storyboard to ensure     the narrative flow and visual balance is to my liking.     As a user, I want the option to buy my storyboard as a NFT or save     it to my device for maximum flexibility in how I use and share my     creation.
 