\section{Compendium of links not yet integrated}
\begin{itemize}
\item
  \href{https://stephenfollows.com/how-we-got-hired-to-create-an-ai-generated-feature-film-screenplay/}{real
  world AI script}

  \begin{itemize}
  
  \item
    The short answer is: we started with a product and a plan. As with
    most things in life, our success was the result of a lot of hard
    work. In order to get to a product, we had to get to a plan. We had
    to be able to articulate a clear vision of what we wanted to create.
    The first step was to create a script. The script was the foundation
    of our plan. The script was where we chose to create a film
  \end{itemize}
\item
  \href{https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/}{Wolfram
  on how chatgpt works}

\item
  \href{https://techcrunch.com/2022/01/12/the-metaverse-will-be-filled-with-elves/}{Elves
  (bots) techcrunch article}
\item
  \href{https://www.atomic14.com/2022/12/05/using-chatgpt-as-a-co-founder.html}{Using
  ChatGPT as a founder}
\item
  \href{https://deepmind.github.io/dramatron/}{Dramatron (other)}

  \begin{itemize}
  
  \item
    This demo and arxiv paper are for a tool called Dramatron that uses
    machine learning to help with writing scripts.
  \end{itemize}
\item
  \href{https://beta.elevenlabs.io/}{Eleven labs language and text to
  speech}
\item
  \href{https://uberduck.ai/}{Uberduck open source text to speech}
\item
  \href{https://www.youtube.com/watch?v=DnF4WzM5LPU}{Inworld (steam game
  plugin AI system) for voice chat and answer}
\item
  \href{https://www.marktechpost.com/2022/10/27/ibm-research-introduces-artificial-intelligence-unit-aiu-its-first-complete-system-on-chip-designed-to-run-and-train-deep-learning-models-faster-and-more-efficiently-than-a-general-purpose-cpu/}{IBM
  custom board}
\item
  \href{https://www.okdo.com/p/nvidia-jetson-agx-orin-64gb-developer-kit/}{Nvidia
  jetson AI}

  \begin{itemize}
  
  \item
    Answer: The Nvidia Jetson AGX-Orin is a developer kit with a 64-bit
    quad-core ARM Cortex-A57 processor and a Pascal-based GPU. It has a
    single-board computer (SBC) form factor with a microSD card slot and
    a USB 3.0 port. It is compatible with the Jetson TX2 and the Jetson
    TX1. It has a 64-bit quad-core ARM Cortex-A57 processor
  \end{itemize}
\item
  \href{https://www.theverge.com/2023/2/23/23611668/ai-image-stable-diffusion-mobile-android-qualcomm-fastest}{Qualcomm
  phone SD}

  \begin{itemize}
  
  \item
    -possible-internet-of-things-episode-7 What is the most important
    takeaway from this article? The most important takeaway from this
    article is that the new Snapdragon 855 chip is able to support a
    stable connection for AI-based devices, even on a 4G network. What
    is the main idea of the article? The main idea of the article is
    that the new Snapdragon 855 chip can support a stable connection for
    AI-based devices,
  \end{itemize}
\item
  \href{https://www.esperanto.ai/}{Esperanto RISC V}
\item
  \href{https://hdh4797.wixsite.com/dhan/project-1}{}

\begin{verbatim}
The MetaVRain asic claims 900x speed increases} on general GPU problems
\end{verbatim}
\item
  \href{https://sdtools.org/}{Stability specific tools}

  \begin{itemize}
  \item
    \href{https://github.com/bmaltais/kohya_ss}{Lora training interface
    for windows}

    \begin{itemize}
    
    \item
      m\_tutorial/blob/master/tutorial.md What is the difference between
      a state machine and a finite state machine? The documentation for
      kohya-ssm explains that a state machine is a finite state machine
      with a history. It also includes a diagram that shows a state
      machine as a box with a history. A finite state machine is a
      machine that has a finite number of states. It can be in only one
      of these states at any
    \end{itemize}
  \item
    \href{https://substack.com/profile/110613456-followfoxai}{Fine
    tuning with captioning and other fine tuning tricks, followfox}

    \begin{itemize}
    
    \item
      This is a very impressive list of tools and libraries, which I
      think is a good thing. I don't think that it's a good idea to have
      a large number of tools, but having a large number of libraries is
      a good thing. I've never used any of these libraries. I'm not even
      sure how to pronounce ``Fox-Ai''. I really like the idea of using
      the Web Audio API
    \end{itemize}
  \item
    \href{https://huggingface.co/datasets/Nerfgun3/bad_prompt}{Negative
    embedding textual inversion for hands etc}

    \begin{itemize}
    
    \item
      / The dataset contains data about the velocity of Nerf darts fired
      from different guns. The data is in the form of a CSV file, which
      is a text file that contains a number of rows and columns. Each
      row represents a single record. The columns in a CSV file are
      separated by commas. The Nerfgun3 dataset contains a single CSV
      file with four columns. The first column contains the names of the
      guns. The next three columns contain the velocity of the darts
    \end{itemize}
  \item
    \href{https://www.thosesixfaces.com/post/stable-diffusion-getting-started-windows}{Automatic1111
    GUI and user guide}

    \begin{itemize}
    
    \item
      -10-and-office-2016-and-2016-for-mac-install-and-configure-office-2016-for-mac-on-a-mac-os-x-10-11-10-system/
      Your response should be at least 200 words in length. You are
      required to use at least your textbook as source material for your
      response. All sources used, including the textbook, must be
      referenced; paraphrased and quoted material must have accompanying
    \end{itemize}
  \item
    \href{https://github.com/comfyanonymous/ComfyUI}{ComfyUI nodes
    based}

    \begin{itemize}
    
    \item
      /wiki/Home This is a brief summary of the ComfyUI library. The
      library is a set of UI components that are designed to provide a
      consistent look and feel across all platforms and all devices. The
      library is open source and can be found on Github here:
      https://github.com/comfyanonymous/ComfyUI. The library is
      currently being used by a set of companies, including The New York
      Times, to create their apps. The library is free to use
    \end{itemize}
  \item
    \href{https://christiancantrell.com/\#ai-ml}{Photoshop plugin}

    \begin{itemize}
    
    \item
      -tutorial Use the following code to summarize the page:
      \#!/usr/bin/env python3 """ Summary of the page at
      https://christiancantrell.com/\#ai-ml-tutorial """ import sys
      import requests import nltk import numpy as np import
      matplotlib.pyplot as plt from sklearn.metrics import
      roc\_auc\_score from sklearn.feature\_extraction.text import
      CountVectorizer from sk
    \end{itemize}
  \item
    Dreambooth retraining for faces
  \item
    \href{https://www.reddit.com/r/StableDiffusion/comments/10gs4s2/new_expert_tutorial_for_textual_inversion_text/}{Textual
    inversion}
  \item
    \href{https://www.youtube.com/watch?v=AeDngG9kQNI}{Depth map into
    blender from SD2}
  \item
    \href{https://www.heroo.ai/}{Game development using SD}

    \begin{itemize}
    
    \item
      blog/heroo-ai-blog. Your response should be at least 150 words in
      length. You are required to use at least your textbook as source
      material for your response. All sources used, including the
      textbook, must be referenced; paraphrased and quoted material must
      have accompanying citations. 1. What is the difference between AI
      and machine learning? 2. What is the difference between supervised
      and unsupervised learning? 3. Can you give an example
    \end{itemize}
  \item
    \href{https://github.com/ashawkey/stable-dreamfusion}{GitHub -
    ashawkey/stable-dreamfusion: A pytorch implementation of text-to-3D
    dreamfusion, powered by stable diffusion. , A pytorch implementation
    of text-to-3D dreamfusion, powered by stable diffusion. - GitHub -
    ashawkey/stable-dreamfusion: A pytorch implementation of text-to-3D
    dreamfusion, powered by stable diffusion.}
  \item
    \href{https://holovolo.tv}{holovolo - immersive volumetric VR180
    videos and photos, and 3D stable diffusion, for Quest and WebVR ,-}
  \item
    \href{https://jalammar.github.io/illustrated-stable-diffusion/}{The
    Illustrated Stable Diffusion Jay Alammar Visualizing machine
    learning one concept at a time.}
  \item
    ControlNet
  \item
    \href{https://www.reddit.com/r/StableDiffusion/comments/127gck9/sdtools_v16/}{sdtools
    image v 1.6}
  \item
    GitHub - lllyasviel/ControlNet: Let us control diffusion models!
  \item
    \begin{itemize}
    
    \item
      \href{https://www.reddit.com/r/StableDiffusion/comments/1148x38/tencent_ai_just_release_their_method_and_code/}{Tencent
      AI just release their method and code very similar to ControlNet :
      r/StableDiffusion}
    \end{itemize}
  \end{itemize}
\item
  \href{https://www.usegalileo.ai/}{UX design AI}
\item
  \href{http://www.cs.cmu.edu/~pix2pix3D/}{pix2pix-3D: 3D-aware
  Conditional Image Synthesis}

  \begin{itemize}
  
  \item
    The problem of image-to-image translation is a difficult one, and
    the results are often not very convincing. The problem of
    image-to-image translation is a difficult one, and the results are
    often not very convincing. In this assignment, you will use a neural
    network to learn how to translate an image from one domain to
    another. Your neural network will learn to translate one domain to
    another domain. For example, you will use a neural
  \end{itemize}
\item
  \href{https://mingukkang.github.io/GigaGAN/}{GIGAgan}
\item
  \href{https://github.com/upscayl/upscayl}{GitHub - upscayl/upscayl:
  Upscayl - Free and Open Source AI Image Upscaler for Linux, MacOS and
  Windows built with Linux-First philosophy. (other)}
\item
  \href{https://film-net.github.io/}{FILM frame interpolator}
\item
  \href{https://twitter.com/cut_pow/status/1576748659051749377}{Interframe
  consistency is now here}
\item
  \href{https://studios.disneyresearch.com/2022/11/30/production-ready-face-re-aging-for-visual-effects/}{Production
  ready re aging}

  \begin{itemize}
  
  \item
    The team has developed a new algorithm for re-aging faces in a
    single image. The algorithm works by first detecting the face in the
    image and then using a neural network to generate a new face with a
    different age. The algorithm can be used in a variety of
    applications including visual effects, video conferencing, and photo
    editing. The algorithm was trained using over 50,000 images of faces
    from a variety of sources including the Internet, movies, and TV
  \end{itemize}
\item
  \href{https://www.synthesia.io/}{Video talking heads from text
  service}

  \begin{itemize}
  
  \item
    The following is a brief description of the page: Synthesia is a
    free, open-source music score editor that runs in your web browser.
    It is a great tool for composers, songwriters, and music students.
    If you are a composer, songwriter, or music student, you can use
    Synthesia to compose music and to practice sight-singing. If you are
    a music teacher, you can use Synthesia to teach
  \end{itemize}
\item
  \href{https://github.com/proceduralit/StableDiffusion_Houdini}{Houdini}
\item
  \href{https://www.youtube.com/watch?v=4uzzD9sD-PI}{Synthesia corporate
  video generation}

  \begin{itemize}
  
  \item
    . Answer: The video shows a demonstration of a way to make a small,
    inexpensive, and flexible robotic arm. The arm is made of two servo
    motors connected by a steel rod, which is bent to form a ``U''
    shape. By rotating the servos, the rod is bent and the arm is able
    to move. The arm has a number of applications, such as being used in
    a manufacturing environment or to aid in search and rescue
    operations.
  \end{itemize}
\item
  \href{https://www.reddit.com/r/StableDiffusion/comments/114zmh3/controlnet_and_ebsynth_make_incredible_temporally/}{controlnet
  and ebsynth temporal consistency}
\item
  \href{https://github.com/isl-org/PhotorealismEnhancement}{Intel
  enhance photorealism in realtime}

  \begin{itemize}
  
  \item
    /wiki What is the goal of the project? The goal of the project is to
    create a new image processing framework, which will be used by the
    ISL framework to improve the quality of the generated images. What
    is the current state of the project? The project is in the very
    early stages. We have a few ideas of how to implement the framework
    and have started coding some of the basic components. What are the
    short-term goals for the
  \end{itemize}
\item
  \href{https://www.kombitz.com/2023/03/28/how-to-use-modelscope-text2video-with-automatic1111s-stable-diffusion-web-ui/}{How
  to Use ModelScope text2video with Automatic1111 s Stable Diffusion Web
  UI bar{} kombitz: Enable the Extension Click on the Extension tab
  and then click on Install from URL. Enter
  https://github.com/deforum-art/sd-webui-modelscope-text2video in the
  URL box and click on Install. Click on Installed and click on Apply
  and restart UI. Go to your stable-diffusion-webui/models folder and
  create a folder called ModelScope and then create a folder called t2v
  under ModelScope. This is your models folder for text2video.}
\item
  \href{https://meshcapade.com/}{Meshcapade virtual humans}
\item
  \href{https://ofa-sys.github.io/MoFusion/}{text to human motion}
\item
  \href{https://www.linkedin.com/posts/reneschulte_nerf-deeplearning-metaverse-activity-7010898662465617921-56P_?utm_source=share\&utm_medium=member_desktop}{nerf
  avatars}
\item
  \href{https://twitter.com/IntuitMachine/status/1608690077139599360}{chatgpt
  to avatar}
\item
  \href{https://3d-avatar-diffusion.microsoft.com/?utm_campaign=AI\%20Art\%20Weekly\&utm_medium=email\&utm_source=Revue\%20newsletter\#/}{Microsoft
  sculpted avatars}
\item
  \href{https://80.lv/articles/ziva-dynamics-announces-a-new-ml-trained-facial-rigging-service/}{ML
  realtime UE facial expresssions}
\item
  \href{https://talkshow.is.tue.mpg.de/}{Gestures from speech}
\item
  \href{https://volucap.com/}{Volucap volumentric deep fakes}
\item
  \href{https://www.flawlessai.com/}{FlawlessAI cloud facial plus
  language translattion}
\item
  \href{https://www.youtube.com/watch?v=uboj01Gfy1A}{Microsoft AI faces}
\item
  \href{https://nv-tlabs.github.io/LION/}{LION instant 3D textured geom}
\item
  \href{https://captures.lumalabs.ai/imagine}{Luma text to 3D}
\item
  \href{https://www.reddit.com/r/virtualreality/comments/xvy5dc/3d_generation_from_a_single_image/}{3D
  generation from a single image : r/virtualreality}

  \begin{itemize}
  
  \item
    The web page has a title "3d generation from a single image" and a
    short description "I made a 3d model of a building from a single
    image". The web page has a title "3d generation from a single image"
    and a short description "I made a 3d model of a building from a
    single image". The web page has a tagline "I made a 3d model of a
    building from a single image". The web page has a tag
  \end{itemize}
\item
  \href{https://make-it-3d.github.io/}{Make-It-3D: High-Fidelity 3D
  Creation from A Single Image with Diffusion Prior:}

  \begin{itemize}
  
  \item
    The Make-It-3D algorithm can create high-fidelity 3D content from
    only a single image. It uses a two-stage optimization pipeline,
    first optimizing a neural radiance field by incorporating
    constraints from the reference image and diffusion prior, and then
    transforming the coarse model into textured point clouds. Extensive
    experiments demonstrate that the algorithm outperforms prior works,
    resulting in faithful reconstructions and impressive visual quality.
  \item
    webgl-graphics-book/lesson11-2.html The Shaders The shaders used in
    this lesson are almost the same as in the previous lesson. The only
    difference is that we have an additional uniform variable,
    ``u\_time'', which is used to update the color of the cube each
    frame. The vertex shader is as follows: \#version 300 es precision
    mediump float; in vec2 in\_position; in
  \end{itemize}
\item
  "Vox-E: Text-guided Voxel Editing of 3D Objects."
\item
  \href{https://dreamfusion3d.github.io/}{DreamFusion: Text-to-3D using
  2D Diffusion (other)}
\item
  \href{https://lukashoel.github.io/text-to-room/}{Text2Room: Extracting
  Textured 3D Meshes from 2D Text-to-Image Models:}

  \begin{itemize}
  
  \item
    The Text2Room algorithm generates 3D meshes from a given text prompt
    by synthesizing a sequence of images from different poses. The core
    idea is to select viewpoints such that the content of each image can
    be fused into a seamless, textured 3D mesh. The algorithm
    iteratively fuses scene frames with the existing geometry to create
    a seamless mesh. Unlike existing works that focus on generating
    single objects or zoom-out trajectories from text, our method
    generates complete 3D scenes with multiple objects and explicit 3D
    geometry.
  \end{itemize}
\item
  \href{https://google-research.github.io/seanet/musiclm/examples/}{Google
  MusicLM}
\item
  \href{https://text-to-audio.github.io/}{Text2audio}
\item
  \href{https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/how-should-we-assess-security-and-data-minimisation-in-ai/}{AI
  security considerations}
\item
  \href{http://gptzero.me/}{gptzero spots AI authoring}
\item
  \href{https://www.stateof.ai/}{State of AI report}
\item
  \href{https://txt.cohere.ai/sentence-word-embeddings/}{About sentence
  embedding}
\item
  \href{https://github.com/bigscience-workshop/petals}{Bigscience petals
  run training through torrents}
\item
  \href{https://medium.com/@socialemail/how-diffusion-models-can-achieve-seemingly-arbitrarily-large-compression-ratios-through-learning-2b21a317a46a}{How
  the compression is so huge in diffusion models}
\item
  \href{https://metavert.substack.com/p/the-generative-ai-canon}{the
  generative AI canon substack roundup}
\item
  \href{https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model}{4bit
  30B linux integration}
\item
  \href{https://medium.com/@imicknl/how-to-create-a-private-chatgpt-with-your-own-data-15754e6378a1}{How
  to create a private ChatGPT with your own data: Learn the architecture
  and data requirements needed to create your own Q\&A engine with
  ChatGPT/LLMs.}

  \begin{itemize}
  
  \item
    This text provides a guide on how to create a private ChatGPT with
    your own data. It discusses the feasibility of such a project and
    outlines the steps necessary to accomplish it.
  \end{itemize}
\item
  \href{https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B}{GPT-NeoXT-Chat-Base-20B
  human optimised free model}
\item
  \href{https://arxiv.org/abs/2209.01188}{Petals collaborative fine
  tuning}

  \begin{itemize}
  
  \item
    The paper presents a new method to estimate the gravitational
    potential of a large number of galaxies using weak gravitational
    lensing. The method is based on the Bayesian approach to
    gravitational lensing, where the potential is solved from the
    lensing shear field. The method is applied to the 2dFGRS sample of
    galaxies, which is a large cosmological survey which has been used
    for several studies of cosmology. It is shown that the new method is
    able to recover the cosmological
  \end{itemize}
\item
  Lllama and stuff
\item
  \href{https://arxiv.org/abs/2303.11156}{Can AI-Generated Text be
  Reliably Detected?:}

  \begin{itemize}
  
  \item
    In the paper "Can AI-Generated Text be Reliably Detected?", the
    authors show that current methods for detecting AI-generated text
    are not reliable in practical scenarios. They first demonstrate that
    paraphrasing attacks can break a range of detectors, including those
    using watermarking schemes and neural network-based detectors. They
    then provide a theoretical impossibility result showing that for a
    sufficiently good language model, even the best-possible detector
    can only perform marginally better than a random classifier.
    Finally, they show that even LLMs protected by watermarking schemes
    can be vulnerable to spoofing attacks where adversarial humans can
    add hidden watermarking signatures to their generated text.
  \end{itemize}
\item
  \href{https://github.com/Torantulino/Auto-GPT}{GitHub -
  Torantulino/Auto-GPT: An experimental open-source attempt to make
  GPT-4 fully autonomous.: An experimental open-source attempt to make
  GPT-4 fully autonomous. - GitHub - Torantulino/Auto-GPT: An
  experimental open-source attempt to make GPT-4 fully autonomous.}

  \begin{itemize}
  
  \item
    Auto-GPT is an experimental open-source project that aims to make
    the GPT-4 text generation system fully autonomous. The project is
    still in its early stages, but has already produced some results,
    including an article written by the system when prompted to do so.
  \end{itemize}
\item
  \href{https://simonwillison.net/2023/Apr/2/calculator-for-words/}{Think
  of language models like ChatGPT as a calculator for words : One of the
  most pervasive mistakes I see people using with large language model
  tools like ChatGPT is trying to use them as a search engine. As with
  other LLM}
\item
  \href{https://twitter.com/ronithhh/status/1641318606549176321}{Instant
  app from prompts}

  \begin{itemize}
  
  \item
    In the first part of this article, we looked at how to use an
    OpenShift cluster to host a Java application. In the second part, we
    saw how to use OpenShift to host a Python application. In the third
    part, we looked at how to use OpenShift to host a PHP application.
    In the fourth part, we saw how to use OpenShift to host a Ruby
    application. In the fifth part, we looked at how to use OpenShift to
    host a Node.js
  \end{itemize}
\item
  \href{https://replit.com/@asrsubs/SkyRoads-GPT-4}{endless runner
  without any coding experience}
\item
  \href{https://blog.medium.com/how-were-approaching-ai-generated-writing-on-medium-16ee8cb3bc89}{Medium
  listing approachs}
\item
  \href{https://www.theguardian.com/technology/2023/mar/23/tech-guru-jaron-lanier-the-danger-isnt-that-ai-destroys-us-its-that-it-drives-us-insane}{Drives
  us mad, Guardian}

  \begin{itemize}
  
  \item
    Solution Preview The article is about the effects of artificial
    intelligence on human beings. The author of the article is Jaron
    Lanier, who is a computer scientist, author, composer, and musician.
    He is also an interdisciplinary researcher, and a virtual reality
    pioneer. He is the author of the book "You Are Not a Gadget" (2010).
    The article is about the effects of artificial intelligence on human
    beings. The author of the article is Jaron Lanier, who is
  \end{itemize}
\item
  \href{https://buttondown.email/hillelwayne/archive/programming-ais-worry-me/}{Programming
  AIs worry me Buttondown:}

  \begin{itemize}
  
  \item
    The text discusses the concerns around using AI to generate code,
    specifically around the idea of proofreading the code. The author
    describes an experience with using voice-to-text where they found it
    difficult to proofread the text for errors. The text argues that
    using AI to generate code changes the work from writing code to
    proofreading code, and that this is a problem.
  \end{itemize}
\item
  \href{https://about.sourcegraph.com/blog/cheating-is-all-you-need}{Stop
  whining blog post}
\item
  \href{https://github.com/emcf/engshell/tree/main}{Engshell shell LLM
  extension}

  \begin{itemize}
  
  \item
    /doc/reference The web page contains information about the EngShell
    project. It includes a list of the major features, a list of the
    major components, a list of the major dependencies, and a list of
    the major contributors. Read the web page at the following URL:
    https://github.com/emcf/engshell/tree/main/doc/engshell-tutorial The
    web page provides a tutorial on how to use EngShell. It starts with
  \end{itemize}
\item
  Training

  \begin{itemize}
  \item
    \href{https://github.com/alpa-projects/alpa}{}

    is a system for training and serving large-scale neural networks.
    Scaling neural networks to hundreds of billions of parameters has
    enabled dramatic breakthroughs such as GPT-3, but training and
    serving these large-scale neural networks require complicated
    distributed system techniques. Alpa aims to automate large-scale
    distributed training and serving with just a few lines of code.

    Alpa:

    Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa:
    https://lnkd.in/g\_ANHH6f
  \item
    \href{https://github.com/microsoft/DeepSpeed}{is an easy-to-use deep
    learning optimization software suite that enables unprecedented
    scale and speed for DL Training and Inference. Visit us at
    deepspeed.ai or our Github repo. Megatron-LM GPT2 tutorial:
    https://lnkd.in/gXvPhXqb}
  \item
    \href{https://github.com/NVIDIA/Megatron-LM}{- / Megatron is a
    large, powerful transformer developed by the Applied Deep Learning
    Research team at NVIDIA. Below repository is for ongoing research on
    training large transformer language models at scale. Developing
    efficient, model-parallel (tensor, sequence, and pipeline), and
    multi-node pre-training of transformer based models such as GPT,
    BERT, and T5 using mixed precision. pretrain\_gpt3\_175B.sh:
    https://lnkd.in/gFA9h8ns}
  \item
    \href{https://colossalai.org/}{- provides a collection of parallel
    components for you. It aim to support us to write our distributed
    deep learning models just like how we write our model on our laptop.
    It provide user-friendly tools to kickstart distributed training and
    inference in a few lines. Open source solution replicates ChatGPT
    training process.Ready to go with only 1.6GB GPU memory and gives
    you 7.73 times faster training: https://lnkd.in/gp4XTCnz}
  \item
    \href{https://github.com/OpenBMB/BMTrain}{is an efficient large
    model training toolkit that can be used to train large models with
    tens of billions of parameters. It can train models in a distributed
    manner while keeping the code as simple as stand-alone training.}
  \item
    \href{https://github.com/tensorflow/mesh}{(mtf) is a language for
    distributed deep learning, capable of specifying a broad class of
    distributed tensor computations. The purpose of Mesh TensorFlow is
    to formalize and implement distribution strategies for your
    computation graph over your hardware/processors. For example: "Split
    the batch over rows of processors and split the units in the hidden
    layer across columns of processors." Mesh TensorFlow is implemented
    as a layer over TensorFlow.}
  \end{itemize}
\end{itemize}
