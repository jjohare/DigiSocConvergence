%\input{09_LLMs}
\input{09_GenArt}
\subsection{Explore and document enterprise AI}
For this micro project we briefly explore and document setting up a `proof of concept' project work-flow assistant for Immersive. What we'd like to explore here is the current state of workarounds for the token limit problem. In a business context an LLM should ideally provide accurate, suitable, specific and/or actionable responses when prompted about things to do with our work's processes and work-flows. It is fair to say that LLMs are pretty awful at this. That is likely to change pretty soon. \par
\textbf{There are several options to look at, with different trade-offs. Keeping an eye on the right tool for the job in the medium term will give a competitive advantage for a while}, and prevent the replication of effort many companies will have to go through, as they pivot between approaches.
\subsubsection{Some ways of thinking}
Poulter \href{https://twitter.com/jamespoulter}{CEO} of Vixen Labs has come up with a somewhat strained analogy he calls ``The Central Intelligence Agent''. I'm going to include it until I find something better because I think he's struck on something by dividing up the company needs with his taxonomy (Figure \ref{fig:vixenAnthro}).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{vixenanthro}
    \caption{Vixen Labs anthropomorphic taxonomy of business functions}
    \label{fig:vixenAnthro}
\end{figure}
\href{https://observer.com/2023/06/ceo-ai-survey-yale-professor/}{``Renowned Yale Professor Jeffrey Sonnenfeld Discusses CEOs’ Fear and Confusion of A.I''} in a recent survey and presentation, and this is worth a quick look.\par
He sees five groups (per the article):
\begin{itemize}
\item ``Curious creators'' argue everything you can do, you should do. (Venture capitalist Marc Andreessen recently expressed a similar view in a blog post about A.I.)
\item ``Euphoric true believers'' only see the good in technology.
\item \textbf{``Commercial profiteers'' don’t necessarily understand the new technology but are enthusiastically seeking to cash in on the hype.}
\item ``Alarmist activists'' advocate for restricting A.I.
\item ``Global governance advocates'' support regulation and necessary crackdown.
\end{itemize}
This seems a pretty simplistic set of buckets, but he's got a decent dataset, and he's -very- eminent so perhaps we should realistically see ourselves in the emboldened commercial profiteer category. I think this kind of self reflection is important when dealing with things this potentially transformational. Sonnenfeld said: \textit{``As Robert Oppenheimer warned us, it can be very dangerous to think that technology only takes us to the best of the world.''}
\subsubsection{Trusted enterprise AI}
Enterprise AI, specifically designed or retrofitted for professional use cases, is becoming a significant theme. This trend is driven by leading companies such as Google, Microsoft, and latterly, curiously, Salesforce. Trust has become a primary consideration. At this early stage in the technology it is important that corporate and private users alike bear in mind that the LLMs are `leaky' and are using the data fed into them to train themselves. They are \href{https://help.openai.com/en/articles/6783457-chatgpt-general-faq}{explicit about this}. Anything that goes into ChatGPT can resurface, as Samsung have found out \href{https://cybernews.com/news/chatgpt-samsung-data-leak/}{to their cost}. At this time the following companies have responded by banning the use of the technology internally. 
\begin{itemize}
\item Apple
\item Samsung
\item Verizon
\item Bank of America Corp.
\item Citigroup Inc.
\item Deutsche Bank AG
\item Goldman Sachs Group Inc.
\item Wells Fargo \& Co.
\item JPMorgan Chase \& Co.
\item Amazon's lawyers recommended caution, though a recently leaked document suggests that managers are recommending it's use.
\end{itemize}
There are likely \textit{many} more who have done this less publicly, and indeed I am aware of examples. In response to this \href{https://openai.com/blog/new-ways-to-manage-your-data-in-chatgpt}{OpenAI announced} a business version of its tool, ChatGPT Business. Clearly this is a premium subscription service designed to be private by default. The service manages multiple users and does not train its future models on any conversations that flow through the business. This approach is a significant step towards establishing trust in AI tools, as it ensures that sensitive business conversations are not used to train AI models.\par
As mentioned, Salesforce has been partially AI-powered for years. They recently announced a series of AI-related developments, including the pilot of `Einstein GPT', dubbed ``the world's first generative AI for CRM''. This tool builds on an existing underlying intelligence layer called Einstein, which has been running in Salesforce since 2016. The new generative Einstein GPT is more content-oriented, helping businesses auto-generate text, pictures, and code. This tool is designed to help sales teams find the most likely next customer to buy. More interestingly they are leveraging their expertise in `Salesforce Ventures' a \$500 million fund focused on funding generative AI startups. They have already invested in major projects like Humane, Triple, Anthropic, and Cohere.\par
They have \textit{also} announced an AI Cloud suite: `Salesforce's AI Cloud'. It includes nine GPT-powered applications designed to automate and enhance various business processes. These applications include Sales GPT, Service GPT, Marketing GPT, Commerce GPT, \textbf{Slack GPT}, Tableau GPT, Flow GPT, and Apex GPT. \par
Each of these applications is designed to cater to specific business needs, such as personalizing text generation for emails, automating mundane tasks, customizing messages for different audience segments, and creating workflows from natural language prompts. You can see that our work is already a customer here and this could be built upon.\par 
This suite emphasizes the `Einstein GPT Trust Layer', designed to ensure no potential data leakage, allowing enterprises to use AI for their most sensitive needs. They say this trust layer sits between customer data and the AI models, ensuring that the AI capability can provide predictions without actually looking inside the data. This approach would allow our work to leverage the power of enterprise AI without sacrificing data privacy and/or security.\par
Elsewhere in enterprise AI:
\begin{itemize}
\item Accenture announced a \$3 billion investment into their data and AI practice. This investment includes doubling their talent to 80,000 people, launching an AI navigator for Enterprise platform, and starting accelerators for data and AI readiness across 19 different industries. 
\item Contextual, an enterprise-focused AI startup, recently launched out of stealth with \$20 million in seed funding. 
\item Glean announced a \$100 million round and introduced a workplace chatbot called the Glean Chat Business. 
\item Olive: This healthcare automation startup has raised \$450 million in fresh capital to build out its enterprise AI for hospitals 1.
\item Welltok: This company provides a cloud-based employee wellness platform and has raised \$355 million 1.
\item Outreach: This sales engagement platform has raised \$114 million in series E funding 1.
\item Stackpath: This cybersecurity startup has raised \$396 million in funding 1.
\item Cohere, which is another business-focused AI startup, recently announced a \$270 million series C funding round 2, and are partnered by \href{https://www.oracle.com/news/announcement/oracle-to-deliver-powerful-and-secure-generative-ai-service-for-business-2023-06-13/}{Oracle}.
\item Tunisian enterprise AI startup InstaDeep has also raised \$100 million in Series B financing led by Alpha 3.
\item There's a raft of tools like \href{https://customgpt.ai/}{CustomGPT}, or day planner \href{https://www.beforesunset.ai/}{Before Sunset} that promise to take your data and make it smart by leveraging their deals with the big cloud providers. The prime example of this approach is \href{https://blog.dropbox.com/topics/product/introducing-AI-powered-tools}{Dropbox AI}, which claims to bring the Apple spotlight experience, with ChatGPT smarts, to clouds data. I don't have confidence that I know enough about this, and it seems to be the purview of the AI-Club. If you have a use case there's likely a product, but we'd have to project plan it in specifically and find the right fit and cost/benefit.
\end{itemize}
Taken overall these investments indicate a strong belief in the transformative potential of AI in the enterprise sector.\par
With all this said it's possible the technology is over-hyped. While some believe that AI will disrupt industries in unimaginable ways, others argue that the technology still has a long way to go. Some even argue that the distracting nature of the platforms may be net negative in the short term. Regardless, the current state of Enterprise AI represents a pivotal moment, with companies trying to productise AI and change workflows within large corporations. The impact of these developments could range from a promise of transformation with AI being every bit as disruptive as everyone says it is, to an overhyped flop, as often happens with new technologies. Some industry analysts argue that we're still in the early stages of AI's potential impact. Tech analyst Dan Ives likens the current state of AI to a ``Gold Rush'' moment, suggesting that we're closer to 1995 than 1999 in terms of AI's evolution and impact on industries. This perspective suggests that while AI has made significant strides, there's still a long way to go before it fully transforms the business landscape. I would tentatively agree with this analysis, and avoid over investing in low confidence FOMO plays.
\subsubsection{Brute forcing ChatGPT4 with contexts}
While we await `ChatGPT Business' it's still possible to explore using the tooling. The ChatGPT4 system is so far out ahead of everyone else it's sometimes useful to consider using it for business by adding in carefully crafted chunks of context data to refine how it answers. This is prompt engineered. A fine example of this everyday use of the technology can be found in \href{https://clarifycapital.com/the-future-of-investment-pitching}{this clarify capital report} which finds that ChatGPT created pitch decks are \textbf{far} more compelling than human created ones. It can be developed onward for more complex corporate proposals like this through the API, which is a subscription service, with additional tiers for heavy corporate use cases (\ref{sec:microsoft}).
\begin{itemize}
\item \textbf{Advantages}
\begin{itemize}
\item Can be trialled in the web interface, spending a few hours or perhaps days building a custom context that solves a specific use case for the business, then simply copy/pasting.
\item \textbf{OpenAI GPT is incredibly cost effective} (\$1 for around 700 pages for GPT3.5 performance), or \$20 a month for the web subscription.
\item GPT4 is \textbf{way} ahead in terms of performance. Possibly 18 months ahead of open source.
\item Incredible team and partnership. Plug-ins are arriving very fast to solve real business problems. They have scale and velocity. 
\end{itemize}
\item \textbf{Disadvantages}
\begin{itemize}
\item In terms of future project planning 18 months isn't that long, open source is worth investing in for the sake of differentiation in those time-scales.
\item It's a very general model, refining this through the API means programming work. This is a known unknown with staffing costs.
\item You're necessarily giving your commercial data to a cloud service.
\item Their ``corporate'' private package has trust implications, and cost implications (more in the next section).
\item \textbf{It's reliant on a reliable internet connection, so it's suitable for the office but perhaps not `site'. Using it might therefore mean ending up investing time in two development tracks}.
\end{itemize}
\end{itemize}
One of the incredibly frustrating things about the GPT series is that OpenAI are changing the code behind the models all the time as seen in figure \ref{fig:GPTchanges}. This makes it hard to build upon in a trustable way \cite{chen2023chatgpts}.
The team built a dataset with 50 easy problems from LeetCode and measured how many GPT-4 answers ran without any changes. The March version succeeded in 52\% of the problems, but this dropped to a pale 10\% using the model from June. I assume that OpenAI pushes changes continuously, but we don't know how the process works and how they evaluate whether the models are improving or regressing.
In my opinion, this is a red flag for anyone building applications that rely on GPT-4. Having the behavior of an LLM change over time is not acceptable.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{gptchanges}
    \caption{Changes to GPT models performance over time - denied by OpenAI (Chen et al) \cite{chen2023chatgpts}}
    \label{fig:GPTchanges}
\end{figure}

 
\subsubsection{ChatGPT - Code Interpreter}
The ChatGPT Code Interpreter Plugin, introduced in March 2023, offers a sandboxed environment featuring a working Python interpreter. This environment, which is isolated from other users and the Internet, supports an impressive array of functionalities. It comes pre-loaded with over 330 libraries, including popular ones such as pandas, matplotlib, seaborn, and TensorFlow, among others.

As illustrated in Figure \ref{fig:chatGPTdata}, the Code Interpreter Plugin is capable of performing a myriad of tasks. For example, it can visualize any data inputted by the user, generate GIFs of the visualizations, and perform file uploads and downloads. It can extract colors from an image to create a color palette, and autonomously compress large images when memory is running low. Moreover, the plugin can clean and process data, generate insightful visualizations, and convert files to different formats quickly and efficiently.

The Code Interpreter Plugin can be installed by ChatGPT Plus users in a few simple steps. However, it is worth noting that while this plugin is powerful, it does have certain limitations, such as frequent environment resets, limited Optical Character Recognition (OCR) capabilities, and an inability to access the web. Despite these limitations, OpenAI continues to work on improving the capabilities of the Code Interpreter Plugin, promising a future with substantial impacts on the world of programming.

\begin{itemize}
\item The Code Interpreter Plugin introduces a sandbox and an advanced language model, both of which are critical to its functionality.
\item The emphasis of the plugin is on the quality of the model, which can generate code, debug it, and even decide when not to proceed without human input.
\item The plugin offers substantial model autonomy, enabling it to work through multiple steps of code generation autonomously.
\item Despite its powerful capabilities, the plugin does have limitations, such as frequent environment resets and limited OCR capabilities.
\item The plugin is only available to ChatGPT Plus users, and requires a few simple steps for installation.
\item The Code Interpreter Plugin represents a significant advancement in the realm of programming, changing the way programmers interact with AI systems.
\end{itemize}
\subsubsection{Go all in with Microsoft}
\label{sec:microsoft}
%Much like the above option, this is the ``corporate offering'' where they can train what you need, hand hold you through it, give you their servers. It's a `whatever you want and can afford'' option, which was rumoured to be in 6 figures before they will even onboard you. That has just changed with \href{https://azure.microsoft.com/en-us/blog/build-next-generation-ai-powered-applications-on-microsoft-azure/}{Azure AI studio}, which I haven't dug into yet. 
Microsoft have -just- released GPT4 which privately works on your own data. \href{https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-azure-openai-service-on-your-data-in-public-preview/ba-p/3847000}{This is likely the best option on the market right now}.
\subsubsection{Anthropic - Claude 2}
Claude-2, Anthropic's ChatGPT competitor was just released. It's cheaper, stronger, faster, can handle PDFs, and supports longer conversations.
\begin{itemize}
\item Claude is 5x cheaper than GPT-4.

\item It has more recent data. A a mix of websites, licensed data sets from third parties and voluntarily-supplied user data from early 2023.

\item It outperforms GPT4 on the GRE writing and HumanEval coding benchmarks.

\item It features a context window of 100,000 tokens, the largest of any commercially available model.

\item It can analyze roughly 75,000 words, about the length of “The Great Gatsby".

\item It can easily handle any code related tasks.
\end{itemize}

\subsubsection{Llama 2}
The new Llama 2 model from Meta looks initially exciting but is pretty mired in legal detail compared to the emerging open source community efforts.
\paragraph{License Grant}

You are granted a non-exclusive, worldwide, non-transferable, royalty-free license to use, reproduce, distribute, modify, and create derivative works of Llama 2.
\paragraph{Attribution and Acceptable Use}

You must retain the attribution notice in all copies of Llama 2.
Your use must comply with Meta's Acceptable Use Policy, which prohibits illegal, deceptive, dangerous, or harmful uses.
\paragraph{Commercial Use and Model Improvement}

You cannot use Llama 2 to improve any other large language model besides Llama 2.
If your products or services have over 700 million monthly active users, you must obtain a separate license from Meta.
\paragraph{Disclaimer, Liability, and Ownership}

Llama 2 is provided "as is" with no warranties. You assume all risks from use.
Meta has no liability for damages arising from use of Llama 2.
You own any derivative works and modifications you create, subject to Meta's ownership of Llama 2.
\paragraph{Termination and Risks}

Meta can terminate the license if you breach it. You must delete Llama 2 on termination.
Be aware of regulations like Article 28b of the AI Act in the EU. Do appropriate diligence to comply with laws and address risks around bias, fairness, transparency, and safety.
\paragraph{Key Takeaways}

Understand attribution requirements, acceptable use policy, commercial use limits, disclaimer, risks, and ownership provisions.
Seek legal counsel given complexities.

\subsubsection{Roll your own trained LLM}
This costs around \$500k to train something up from a trillion tokens that you bring to the party. This gets to `last years' GPT3 level. It's too much, but it's worth being aware of. It's worth noting that Cerebras are offering access to their \href{https://www.cerebras.net/press-release/cerebras-unveils-andromeda-a-13.5-million-core-ai-supercomputer-that-delivers-near-perfect-linear-scaling-for-large-language-models}{Andromeda cluster} which can train a significant model in the vein of Llama in around 11 days. 
\subsubsection{Wait for the Google integrations}
I very strongly suspect that corporate level ML assistance is coming in force to the Google stack already employed at our work. This is \textbf{by far the most likely and pragmatic solution for the `project planning assistant' business case}. \href{}{Vertex AI} cloud based generative art support shows the direction of travel in this regard.  \begin{tcolorbox}[enhanced, frame style={fill=lightgray}, interior style={fill=lightgray}]
With this update, developers can access our text model powered by PaLM 2, Embeddings API for text, and other foundation models in Model Garden, as well as leverage user-friendly tools in Generative AI Studio for model tuning and deployment. Backed by enterprise-grade data governance, security, and safety features, Vertex AI can make it easier than ever for customers to access foundation models, customize them with their own data, and quickly build generative AI applications.
\end{tcolorbox}
\begin{itemize}
\item Advantages
\begin{itemize}
\item our work already trusts Google with it's business data
\item Single repository potential to leverage that fact
\item Will likely be very cheap as a customer incentive. Currently it's around 700 pages per dollar.
\end{itemize}
\item Disadvantages
\begin{itemize}
\item GCHQ have \href{https://www.ncsc.gov.uk/blog-post/chatgpt-and-large-language-models-whats-the-risk}{taken the unusual step} of warning that the data put into these systems goes into their training and can thereby resurface in competitors searches later
\item The likes of Apple and Samsung have banned the use of these cloud tools as a result. There's anecdotal evidence of commercially sensitive details surfacing, though it's hard to validate these
\item The products can change over time, in ways that are outside of your control
\end{itemize}
\end{itemize}
With all that said there is potentially business advantage to learning how these systems work through doing.
\subsubsection{Build something custom self hosted}
Building something custom here means taking an open source model, with a permissive license. There's a lot of these now and they are `decent'. It's possible to add in some code engineering around the edges to give it access to private datasets through a chat interface.
\begin{itemize}
\item \textbf{Advantages}
\begin{itemize}
\item It's private, local, under your control, and so you can trust your data will be within the company walled garden
\item It's building toward IP and knowledge, in the likely scenario where GPT4 level models are less than 2 years away. This is real internal investment
\item There are NO legal repercussions to using it in a purely off-line way. You don't even need to tell people you're doing it. `Probably' no GPDR, data governance, compliance overheads if designed right.
\end{itemize}
\item \textbf{Disadvantages}
\begin{itemize}
\item It will still lie, and the company and individuals are still responsible for the legal repercussions of acting on what the model says.
\item In a live deployment to the public it will occasionally say ``bad things''. It's just impossible to control edge cases. Even without the issues incurred by being online, the exposure around that in terms of waivers is uncertain.
\item Making it fit for purpose by adding memory is hard. The token limits on these models are really small. They're just not that smart yet.
\end{itemize}
\end{itemize}
Below is output from the top two models from the \href{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}{global leader board}. The second highest rated worked better (supercot). Into it I uploaded the `International Code of Practice for Entertainment Rigging' which is a \href{https://www.plasa.org/wp-content/uploads/2017/11/ICOPER_V1.0.pdf}{meagre 35 page document}. I have done 500 pages in the past but the quality starts to break down with scale. Section 3.5 of the document says
\begin{tcolorbox}[enhanced, frame style={fill=lightgray}, interior style={fill=lightgray}]``All custom built equipment selected for a project must be reviewed and approved by a qualified person. Custom built equipment must be provided with necessary markings and documentation to ensure safe use.''
\end{tcolorbox}
You can see the output from the model running at home (without internet) in Figure \ref{fig:ICOPERLLM}. It has royally embellished the facts, but it's actually delivering up good advice, and the memory injection from the uploaded document means it's specific enough to the context of the question.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{ICOPERLLM}
    \caption{Results from home hosted LLM}
    \label{fig:ICOPERLLM}
\end{figure}
I have very much seen this when playing with both my own "landmark attention" models, and Claude. I expect they will find a way round it at some point. Because humans write with a focus on the beginning and end of documents, so large language models pay far more attention at the beginning and end of their context input \cite{liu2023lost}. 

That's tricky because if  you load in a stack of PDFs and they get translated by the system into one big chunk (which may or may not be in the expected order of the PDFs), then the PDFs in the middle of the submission are subject to more hallucination, garbling, forgetting etc.

The "other" system, vector databases is more even handed, and so commensurately more predictable. Still got huge problems though. 

I have seen both effects. They're currently both bad enough that I wouldn't trust document interrogation to these things. There are workarounds obviously:
Ask for references, ask if to tell you if it thinks it's making it up, as for quotes, ask for locations, check the working, ask in many sperate ways etc.

These "tips" can be encoded into the preamble that goes into every query, as a standing command, so you don't have to, except for the checking bit of course.
You can do that with either self host models or the web ones, but it's something you need to do often with the web interfaces. There's no silver bullet yet but it feels like months away, so I am not advocating learning these tricks. Might be better to wait for the fixes. Just don't trust them, these are the reasons, and they are all basically seeded in the way humans write.

There's a lot that can be done here, but the cost benefit is unclear. If this were sensitive internal planning documents, with a lot of complexity, then there's potentially a strong case, but we'd need tight procedures to check on it's homework. This tool, as usual, is best as a way to rapidly construct a framework.\par 
As a side note there's a lot of open source tools like \href{https://github.com/TransformerOptimus/SuperAGI}{SuperAGI} for `Agents' and \href{https://github.com/Mintplex-Labs/anything-llm}{AnythingLLM} for document analysis which straddle the line between using cloud vendors and local hosting. Building on one of these gives optionality, but they are new and `flakey'.
\subsubsection{LoRA training}
\label{sec:LoRA}
LoRA stands for Low-Rank Adaptation and it's a cheap way to optimise models. A few hundred dollars of rented GPU time can nuance a model to be more performant for a specific task. It sounds great but basically you're optimising for a task that you need to understand very well, possibly/probably to the detriment of the generality of the tool. If there's something you know you want, then this is an option that's achievable and affordable.\par
Goat, a 7B LLaMA model finetuned for arithmetic tasks notably outperformed the ~75x larger 540B PaLM model and GPT-4. Goat's success can be attributed to two primary factors: task-specific finetuning and LLaMA's unique digit tokenization. The problem here is Llama is arguably derived from data with a non-commercial license. OpenLlama gets around this with their Apache2 (do what you like) copy of the model. The situation is rapidly evolving. Another example of task-specific finetuning is the Gorilla project, where the LLM was trained to generate API calls. This is a really important area and we might be able to get ahead in this niche. This controlling complex whole site systems with voice control. The model was finetuned using 1,645 API calls from various sources and demonstrated superior performance compared to non-finetuned models. We can easily repeat that.\par
Recent findings suggest that less LoRA training gets better results, so this is increasingly being adopted as a way to improve business fit \cite{xue2023repeat}.
\subsubsection{Enormous token limits}
\label{sec:llmoptions}
There's a few models now boasting staggering token input limits. With 1 million token windows it's possible that each query to the LLM can be `the whole corporate database and filesystem' and then the question you want an answer to. This is a beguiling option, probably the most performant (though I have not played with one), and also pretty much useless as nobody yet offers a legally trustable way to upload your whole business into a chat window.
\begin{itemize}
\item \textbf{Advantages}
\begin{itemize}
\item Likely excellent at it's job
\item Will `hallucinate' less
\item Probably stay on target
\item Knows all about the business
\end{itemize}
\item \textbf{Disadvantages}
\begin{itemize}
\item Web based right now, and knows all about the business
\item Not actually available, I'm on wait lists. 
\item Longer latency as everything will need uploading in the token window (assuming you don't buy a service)
\end{itemize}
\end{itemize}
Very recently this in beginning to shift with the \href{https://github.com/eugenepentland/landmark-attention-qlora}{emergence} of a locally run \href{}{`landmark attention' model}. There are some issues with it at time of writing but this could be run on an our work cloud instance. I have 4000 tokens running at home. I am exploring tree of thought which needs these bigger windows and elevates reasoning by 70 percent \cite{yao2023tree}.
\subsubsection{Memory and other systems}
There's many ways to use a database (in this case a `vector' DB) to prime the context window with appropriate information. It's also potentially useful to look at combining older and well understood ML techniques like knowledge graphs in combination \cite{pan2023unifying}. I've not personally had much luck with these yet.
\subsubsection{Cost implication for self build}
It's less than \$20k to build a system that can do anything you need, on site, rack mount in a ship, whatever. You can do it for less in the cloud, see the next section. You can also spend much, much more. Use-case dependent.\par
Here's the output from the `local' open source model for clarity. Note it drifts off into nonsense toward the end then stops. This can be dealt with but I chose not to:
\begin{tcolorbox}[enhanced, frame style={fill=lightgray}, interior style={fill=lightgray}]
ASSISTANT:Based on the provided information, here are some steps you can take regarding custom built equipment: 
1. Ensure that the equipment is properly marked and traceable to the manufacturer. 
2. Keep up-to-date with maintenance schedules and ensure that maintenance records are readily available. 
3. Follow safety procedures and guidelines for rigging work, including visual inspections, PPE usage, and clear communication. 
4. Prevent any malfunctioning or faulty equipment from being used and quarantine it if necessary. 
5. Use equipment with a capacity that meets or exceeds the design specifications in the lifting plan. 
6. Attach loads in a manner that does not damage the equipment or the load, and consider sling angle factors. 
7. Inspect electrical connections and repair or replace equipment as necessary. 
8. Maintain good housekeeping and prepare a post
\end{tcolorbox}

\subsubsection{Build something custom in a private cloud}
This is exactly the same as the previous section but you hire a private cloud system `on demand' to do the work. This is \href{https://lambdalabs.com/service/gpu-cloud/pricing}{currently priced at} \$1.10/hr and only costs you money when you're using it (though you have to shut it down yourself). This is both secure, and fairly cost effective. Also, it scales in that if you find a real serious application you can just get bigger rental GPUs and open a private/public interface. \textbf{It's my preferred path of all the private use cases except for mission critical on site stuff}, and edge cases like boats at sea etc. For that you need to buy GPUs. AMD have \href{https://www.amd.com/en/newsroom/press-releases/2023-6-13-amd-expands-leadership-data-center-portfolio-with-.html}{recently announced} a partnership with opensource behemoth Huggingface to allow access to large and capable models like Falcon in their enormous new memory architecture. Falcon is from the UAE and has odd views on human rights. This is one to watch.
\subsection{Whistlestop tour of terms}
\label{sec:aiterms}
\subsubsection{Transformers}
Machine learning transformers are a groundbreaking architecture in the field of natural language processing (NLP) that have redefined tasks such as text generation, translation, and sentiment analysis. \href{https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g13dd67c5ab8_0_2648}{This link} is a very technical but excellent overview of how they work. Transformers have gained popularity due to their ability to efficiently capture long-range dependencies and model complex relationships between words in a sentence.\par 
At the core of the transformer architecture lies the self-attention mechanism. This mechanism allows the model to weigh the importance of each word in a sentence relative to the others, effectively capturing context and dependencies. In contrast to traditional neural networks, like recurrent neural networks (RNNs), transformers process input sequences in parallel, rather than sequentially. This parallel processing enables transformers to efficiently understand and remember long-range dependencies, which is particularly important in NLP tasks.\par
RNNs, on the other hand, process input sequences one element at a time, making it difficult for them to capture relationships between words that are far apart in a sentence. As a result, RNNs can struggle with tasks that involve complex sentences or require a deep understanding of context.\par
Transformer-based models, such as GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), have become the go-to models for many NLP tasks.\par
The layers contain weight matrices that are responsible for encoding the model's knowledge and language understanding. \par 

\subsubsection{GANs}
Generative adversarial networks are used for generating synthetic data, and are incredibly useful for our fine tuning use cases. GANs consist of two neural networks that are trained to compete with each other, with one network generating synthetic data and the other trying to distinguish between the synthetic data and real data. This process allows GANs to learn the underlying distribution of the data and generate samples that are highly realistic.\par
Reinforcement learning is a type of machine learning that involves an agent learning through trial and error in order to maximize a reward. 
\subsubsection{LoRA}
LoRA, or Low-Rank Adaptation \cite{hu2021lora}, is a technique that enables efficient adaptation of large language models to specific tasks or domains while maintaining their expressive power. It does so by introducing a small modification to the pre-trained model's weight matrices, enabling the fine-tuning process to be more computationally efficient without sacrificing performance. Visually you can think of this as slipping modification layers in between the transformer layers, which are far more interdependent and thereby expensive to retrain. We are already experimenting with these systems for our use cases.
\subsubsection{Embeddings and Latent Space}
Embeddings play a crucial role in both generative AI art and large language models, as they provide a way to represent complex data types, such as text or images, in a continuous vector space. In both contexts, embeddings capture the underlying structure and semantics of the input data, enabling AI models to learn and generate new content based on these representations. This is what the user sees happening with both AI generative art, and LLMs.\par 
In the context of generative AI art, embeddings are often used to represent visual elements, such as images or shapes. Latent space is a continuous vector space where each point corresponds to an embedding that encodes the features and semantics of an image. Once the AI model is trained, new images can be generated by sampling points from this latent space and decoding them back into the image domain. Embeddings can similarly represent styles or artistic techniques. Style transfer techniques, for example, utilize embeddings to extract and apply the style of one image to the content of another.\par 
In the context of large language models, embeddings are used to represent words, phrases, or sentences in a continuous vector space. These models are trained on vast amounts of text data, learning to generate contextually relevant and semantically meaningful embeddings for language. Similarly to the visual use case in generative art these embeddings capture various aspects of language, such as syntax, semantics, and relationships between words or phrases. Once trained, the large language models can generate new text by sampling from the distribution of embeddings and decoding them back into the text domain.\par 
Embeddings are also used in tasks like sentiment analysis, machine translation, and text classification, where the AI model must understand the meaning and context of the input text.
\subsubsection{Vector databases}
One of the highest points of human `friction' when dealing with and AI model, and especially LLMs is the lack of a persistent and/or contextual memory within the systems. This is beginning to be addressed using vector databases.
A vector database is designed to efficiently store, manage, and query the high-dimensional vectors, often used in the context of machine learning and artificial intelligence. These high-dimensional vectors are the embeddings previously discussed.\par 
Using a vector database with embeddings for AI data retrieval and processing can significantly improve efficiency, scalability, and performance. In the context of a stored item of data, embeddings allow the storage of complex `concepts' as fixed-length vector, which interacts with the enormous latent space in the trained model. This makes storage and retrieval more efficient. \par 
Vector databases allow and optimise for efficient nearest neighbor search, which is crucial for data retrieval tasks in AI applications.
To do this, given a query input, the AI system first converts the input into an embedding using the same technique as for the stored data. The vector database then performs a nearest neighbour search to find the most similar embeddings in the database. At scale this can result in more consistency when using models, but crucially it doesn't train the models on events that have happened. It is not a `memory'.
\subsubsection{Memory Streams}
In the paper `Generative Agents: Interactive Simulacra of Human Behavior' Park et al present a solution and working example for the problem of contextual memory in AI systems \cite{park2023generative}. This is a pretty stunning paper for our purposes in collaborative XR where we would hope to interact with virtual agents. \par
As they point out in the paper virtual agents should be able to manage constantly-growing memories and handle cascading social dynamics that unfold between multiple agents. Their architecture uses a large language model to generate a memory stream, reflection, and planning. The memory stream contains a comprehensive list of the agent's experiences (written as a kind of internal monologue), and the planning module synthesizes higher-level inferences over time. These memory transcriptions are highly compressible and would be excellent as RGB style private data blobs between our federated virtual worlds. It will therefore me possible to `meet' virtual agent friends across instances of virtual spaces \textbf{and} through nostr social media. This is a key technology for our uses now.
\subsubsection{Gradient descent}
Gradient descent is an optimization algorithm widely used in machine learning and deep learning, including large language models, to minimize a loss function. The loss function measures the difference between the predicted output and the actual output (also known as the target) for a given input. The goal of the training process is to \href{https://societyofai.medium.com/gradient-descent-basics-and-application-1cef98179ee6}{minimize this loss} to improve the model's performance.\par
In the context of large language models, gradient descent helps to adjust the model's parameters (weights and biases) so that it can generate more accurate predictions. These models consist of multiple layers of neurons with a large number of parameters that need to be fine-tuned. It is this training process that takes so much time and energy.
\begin{itemize}
\item Initialize parameters: The model's parameters are initially set to random values. These parameters are then iteratively adjusted using gradient descent.
\item Calculate loss: For a given input and target, the model generates a prediction, and the loss function calculates the difference between the prediction and the target.
\item Compute gradients: The gradients of the loss function with respect to each parameter are computed. A gradient is a vector that points in the direction of the greatest increase of the function, and its components are the partial derivatives of the function with respect to each parameter. The gradients indicate how much each parameter contributes to the loss.
\item Update parameters: The model's parameters are updated using the gradients. This is done by subtracting a fraction of the gradient from the current parameter value. The fraction is determined by a hyperparameter called the learning rate. A smaller learning rate results in smaller updates and slower convergence, while a larger learning rate can result in faster convergence but might overshoot the optimal values.
\item Iterate: Steps 2-4 are repeated for a certain number of iterations, a specified tolerance, or until convergence is reached (i.e., when the change in the loss function becomes negligible).
\item Gradient descent has several variations, such as Stochastic Gradient Descent (SGD) and mini-batch gradient descent. These methods differ in how they use the training data to compute the gradients and update the parameters. In SGD, the gradients are computed and the parameters are updated using only one data point at a time, while in mini-batch gradient descent, a small batch of data points is used to compute the gradients and update the parameters. LLMs like GPT can use meta optimisers to train as they operate \cite{dai2022can}.
\end{itemize}
\subsubsection{TPUs}
Tensor Processing Units (TPUs) are specialized hardware accelerators for machine learning workloads, developed by Google. TPUs are designed to speed up the training and inference of machine learning models, particularly large deep neural networks. They are highly parallel and optimized for low-precision arithmetic, which allows them to perform computations much faster than traditional CPUs or GPUs. TPUs can be used in a variety of machine learning applications, such as natural language processing, computer vision, and speech recognition. Google has integrated TPUs into its cloud platform, allowing developers to easily use them for their machine learning workloads. Overall, TPUs provide a powerful and efficient platform for machine learning.
The top of the line Nvidia tensorflow unit at this time is the v4, and it is comparable if more generalised hardware.
\subsubsection{Tensorflow}
TensorFlow is a popular open-source machine learning framework developed by Google and was instrumental in kicking off a lot of this research area. It is still widely used for training and deploying machine learning models in a variety of applications, such as natural language processing, computer vision, and speech recognition, but is being somewhat superceded by JAX. The consensus seems to be that JAX itself is more specialised and harder to use, but works well with Googles hardware cloud systems. Time will tell if this upgrade gets community traction. TensorFlow provides a flexible and high-performance platform for building and deploying machine learning models. It allows users to define, train, and evaluate models using a variety of deep learning algorithms, such as convolutional neural networks and recurrent neural networks. TensorFlow also has a strong emphasis on scalability and performance, with support for distributed training and deployment on a variety of platforms, including GPUs and TPUs. Overall, TensorFlow is a powerful tool for building and deploying machine learning models.
\subsubsection{PyTorch}
PyTorch is a popular open-source machine learning framework developed by Facebook's AI research group. It is primarily used for applications such as natural language processing and computer vision. PyTorch is based on the Torch library and provides two high-level features: tensor computations with strong GPU acceleration and deep neural networks built on a tape-based autograd system. PyTorch offers a variety of tools and libraries for machine learning, including support for computer vision, natural language processing, and generative models. It also allows for easy and seamless interaction with the rest of the Python ecosystem, including popular data science and machine learning libraries such as NumPy and scikit-learn. 
\subsubsection{NumPy}
NumPy is a popular open-source library for scientific computing in Python. It provides a high-performance multidimensional array object, as well as tools for working with these arrays. NumPy's array class is called ndarray, which is a flexible container for large datasets that can be processed efficiently. The library provides a wide range of mathematical functions that can operate on these arrays, including linear algebra operations, Fourier transforms, and random number generation. NumPy also has a powerful mechanism for integrating C, C++, and Fortran code, which allows it to be used for high-performance scientific computing in a variety of applications. Overall, NumPy is an essential library for working with numerical data in Python.
\subsubsection{Latent space}
In the context of generative artificial intelligence (AI), a latent space is a high-dimensional space in which the model represents data as points. This space is "latent" because it is not directly observed, but is inferred by the model based on the data it is trained on. In the case of a generative model, the latent space is often used to encode the underlying structure of the data, such that samples can be generated by sampling from the latent space and then decoding them into the data space.

For example, in a generative model for images, the latent space may encode the features or characteristics of the image, such as the shape, color, and texture. By sampling from this latent space and decoding the sample, the model can generate new images that are similar to the training data, but are not exact copies. This allows the model to generate novel and diverse samples that capture the essence of the training data.

The latent space is an important aspect of generative models because it allows the model to capture the underlying structure of the data in a compact and efficient way. It also provides a way to control the generation process, such as by interpolating between latent space points to generate smooth transitions between samples. At this time the navigation through that mathematical space is steered by vectors into the space, which come from a separate and parallel integration of a natural language model. This crucial bridge came from research at OpenAI, and has been instrumental in the current explosion of usability of the systems \cite{radford2021learning}.
\subsubsection{Edge AI compute and APUs}
\begin{itemize}
\item \href{https://www.theverge.com/2023/2/23/23611668/ai-image-stable-diffusion-mobile-android-qualcomm-fastest}{Qualcomm phone chip} offers low power and high speed Stable Diffusion on mobiles
\item IBM have introduced the \href{https://research.ibm.com/blog/ibm-artificial-intelligence-unit-aiu}{concept of the AIU}, for high speed and low power training
\item Nvidia's \href{https://www.okdo.com/p/nvidia-jetson-agx-orin-64gb-developer-kit/}{latest in the Jetson} Edge AGX line is a high performance general AI unit for industrial applications
\item Esperanto Risc V chip \href{https://www.esperanto.ai/News/risc-v-startup-esperanto-technologies-samples-first-ai-silicon/}{claims incredible performance} gains
\item The MetaVRain asic \href{https://hdh4797.wixsite.com/dhan/project-1}{claims 900x speed increases} on general GPU problems
\item Microsoft are rumoured to be looking to mitigate the staggering costs of running ChatGPT (\$1M/day) using forthcoming \href{https://www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge?}{hardware of their own design}
\item \href{https://www.cerebras.net/}{Cerebras systems} have built an AI architecture from the ground up and claim incredible numbers.
\end{itemize}
 These systems will drive the compute to less `constrained' but somewhat less capable AI systems, distributing the access but increasing risks.
\subsubsection{Prompt engineering}
The art of prompting constrains the generation space into documents that contain correct answers.

A better performance in few-shot prompting can be achieved by constructing fictional scenarios.

Using flattery or painting a clear fictional narrative, such as referring to the LLM as a "masterful French translator," can help to guide the model towards producing the desired output.

Much of the initial prompt engineering involves constructing scenarios that can only be completed correctly.

A good strategy often involves imagining what kind of document might contain the correct answer.

Fine-tuning models for tasks helps make the model more capable and more able to do as it's told.

Fine-tuning involves giving many examples of a task being done correctly, resulting in a model that acts almost as though it had been prompted by thousands of correct examples.
Use of assisted learning: Working with assistants or partners on the project can help identify problems and improve the model.

Correct absurdity instead of playing along: Even if the question seems absurd, the model should provide an answer grounded in reality instead of going along with the absurdity.

Satisfaction of preference model: The model should aim to fulfill a preference model emulating what a human would want.

Consciousness of self: The model should be conscious in a sense of what it is, what it's doing, and where it's situated in the world.

Avoidance of interpolation: Instead of just interpolating what humans might do, the model should do something other than that.

Use of instruction tuning: This makes the desired outcome of the model clearer.

Reinforcement learning with a reward model: This technique takes the results to another level and provides feedback to improve the model.

Avoidance of mode collapse: The model should avoid fixating on a particular way of answering.
