\section{Augmented intelligence and ML}
\subsection{Novel VP render pipeline}
Putting the ML image generation on the end of a real-time tracked camera render pipeline might remove the need for detail in set building. To describe how this might work, the set designer, DP, director, etc will be able to ideate in a headset based metaverse of the set design, dropping very basic chairs, windows, light sources whatever. There is -no need- then to create a scene in detail. If the interframe consistency (img2img) can deliver then the output on the VP screen can simply inherit the artistic style from the text prompts, and render production quality from the basic building blocks. Everyone in the set (or just DP/director) could then switch in headset to the final output and ideate (verbally) to create the look and feel (lens, bokeh, light, artistic style etc). This isn’t ready yet as the frames need to generate much faster (100x), but it’s very likely coming in months not years. This is being trailed by Pathway in the Vircadia collaborative environment described in this book.\par
This can be done now through the use of camera robots. A scene can be built in basic outline, the camera tracks can be encoded into the robot, and the scene can be rapidly post rendered by Stability with high inter frame consistency.
\begin{figure}[ht]\centering 	\includegraphics[width=0.5\linewidth]{robotvp}
	\caption{Robot VP}
	\label{fig:robotvp}
\end{figure}
\subsection{Accessibility}
\subsubsection{Real time transcription}
\subsubsection{Real time translation}
\href{https://openai.com/blog/whisper/}{OpenAI whisper}
\subsubsection{Real time description}
\subsubsection{Interfaces}
\href{https://tech.fb.com/ar-vr/2021/03/inside-facebook-reality-labs-wrist-based-interaction-for-the-next-computing-platform/}{emg}
\subsection{Virtual humans}
\subsubsection{Real time human to avatar mapping}
\subsection{AI actors}
\subsubsection{Faces}
\subsubsection{Voices}
\subsubsection{Autonomous tasks}
\subsection{Governance and safeguarding}


